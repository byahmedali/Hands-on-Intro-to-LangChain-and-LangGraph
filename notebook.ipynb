{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1276c1f",
   "metadata": {},
   "source": [
    "# [Introduction to LangChain](https://python.langchain.com/docs/introduction/)\n",
    "\n",
    "LangChain is a framework for developing applications powered by large language models (LLMs).\n",
    "\n",
    "LangChain simplifies every stage of the LLM application lifecycle:\n",
    "\n",
    "- Development: Build your applications using LangChain's open-source [components](https://python.langchain.com/docs/concepts/) and [third-party integrations](https://python.langchain.com/docs/integrations/providers/). Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\n",
    "- Productionization: Use [LangSmith](https://docs.smith.langchain.com/?_gl=1*lbz7dz*_ga*MTg3MzIxNzgyNS4xNzU0ODgzNTUz*_ga_47WX3HKKY2*czE3NTQ5MDgwMDgkbzQkZzEkdDE3NTQ5MTAwMTckajYwJGwwJGgw) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\n",
    "- Deployment: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://docs.langchain.com/langgraph-platform?__hstc=5909356.bc36eceb44175eebbb2b372b7bd252b1.1754883571704.1754883571704.1754910222298.2&__hssc=5909356.2.1754910222298&__hsfp=2562008206&_gl=1*iime50*_gcl_au*NTc3MzUwMjY4LjE3NTQ5MTAyMDM.*_ga*MTg3MzIxNzgyNS4xNzU0ODgzNTUz*_ga_47WX3HKKY2*czE3NTQ5MDgwMDgkbzQkZzEkdDE3NTQ5MTAyNDkkajYwJGwwJGgw).\n",
    "\n",
    "LangChain implements a standard interface for large language models and related technologies, such as embedding models and vector stores, and integrates with hundreds of providers. See the integrations page for more.\n",
    "\n",
    "<img src=\"https://python.langchain.com/svg/langchain_stack_112024_dark.svg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77ad0c0",
   "metadata": {},
   "source": [
    "LangChain uses LLMs to build applications for various use cases. Created by Harrison Chase, it was first released as an open-source project in October 2022. To date (11-8-25), it has accumulated `113K` stars on [GitHub](https://github.com/langchain-ai/langchain) and has over `3.7k+` contributors.\n",
    "\n",
    "<img src=\"https://atsailabstorage.blob.core.windows.net/atsusecases/langchain-starhistory.png\" width=\"800\">\n",
    "Source: [LangChain Star History](https://www.star-history.com/#langchain-ai/langchain&Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ceed41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import HTML, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25d24499",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0c5cc-b3c4-4ccc-b2f1-df018237dbea",
   "metadata": {},
   "source": [
    "## Chat Models\n",
    "https://js.langchain.com/docs/integrations/chat/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ec89812-d456-4351-8eed-3d0b5614594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"openai/gpt-oss-120b\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50dc2ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I help you today?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(\"hi\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58a8d153-0991-4dd5-a59f-c9c63b185178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "LLM = init_chat_model(model=\"groq:openai/gpt-oss-120b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da5bfff9-472c-4453-ac46-4e8df6f2e552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I help you today?'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = LLM.invoke(\"Hi!\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4c1ce-8afd-497d-8a5a-91573678b9c5",
   "metadata": {},
   "source": [
    "## Prompts Templates\n",
    "A prompt is the text we give to the LLM (or ChatModel). <br>\n",
    "LangChain provides special classes like PromptTemplate and ChatPromptTemplate to make prompts dynamic and reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8330e5a6-7622-467c-9183-4edb332e194b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  Explain Large Language Language in simple Term\n"
     ]
    }
   ],
   "source": [
    "# Single Input Variable\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate(template=\"Explain {topic} in simple Term\")\n",
    "\n",
    "prompt = template.format(topic=\"Large Language Language\")\n",
    "\n",
    "print(\"Prompt: \", prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8dd359ba-73f1-4093-94d9-5e874fd71f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Large Language Model (LLM) ‚Äì Simple Explanation**\n",
       "\n",
       "Imagine you have a huge library of books, articles, chats, and all kinds of written text. A **large language model** is a computer program that has read (or been trained on) that massive collection of words and sentences. By ‚Äúreading‚Äù all that text, the model learns how language works‚Äîhow words fit together, what ideas usually follow each other, and how to answer questions or write new text that sounds natural.\n",
       "\n",
       "### How It Works (in everyday language)\n",
       "\n",
       "| Step | What Happens | Simple Analogy |\n",
       "|------|-------------|--------------|\n",
       "| **1. Learning from lots of text** | The model reads billions of words. | Like a child listening to many conversations and stories. |\n",
       "| **2. Spotting patterns** | It notices which words often appear together and how sentences are built. | Like noticing that ‚Äúpeanut butter‚Äù often goes with ‚Äújelly‚Äù. |\n",
       "| **3. Building a ‚Äúbrain‚Äù** | It creates a huge network of tiny math units (called *parameters*) that store those patterns. | Like building a huge map of all the word connections you‚Äôve ever heard. |\n",
       "| **4. Generating text** | When you ask it something, it uses the map to predict the next word, then the next, and so on, creating a full answer. | Like playing a game of ‚Äúwhat comes next?‚Äù with a very well‚Äëread friend. |\n",
       "\n",
       "### Key Points in Plain Language\n",
       "\n",
       "- **‚ÄúLarge‚Äù**: The model has *a lot* of parameters (often billions) and has been trained on *a lot* of text. The more data and parameters, the better it can capture subtle language nuances.\n",
       "- **‚ÄúLanguage‚Äù**: It deals with human language‚ÄîEnglish, Spanish, Chinese, etc.‚Äîand learns the rules, slang, facts, and even some reasoning.\n",
       "- **‚ÄúModel‚Äù**: This is the mathematical ‚Äúbrain‚Äù that stores what it learned and can be used to answer questions, write stories, translate languages, summarize text, and more.\n",
       "\n",
       "### Everyday Example\n",
       "\n",
       "Think of a **large language model** like a super‚Äësmart autocomplete on your phone, but instead of just finishing a single sentence, it can:\n",
       "\n",
       "- Write a whole paragraph for you.\n",
       "- Explain a concept (like this answer).\n",
       "- Translate a sentence from one language to another.\n",
       "- Answer questions about history, science, or your own notes.\n",
       "\n",
       "All of this happens because the model has ‚Äúread‚Äù a massive amount of text and learned the patterns that make language make sense.\n",
       "\n",
       "### Bottom Line\n",
       "\n",
       "A **large language model** is a computer program that has learned how humans write and speak by studying huge amounts of text. It uses that knowledge to generate new, human‚Äëlike text when you ask it something. Think of it as a very well‚Äëread, very fast, and always‚Äëready writing assistant."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa80eb04-99a0-478e-bdfd-155cbadb87d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: You are helpfull assistent\\nHuman: Summarize the concept of Large Language Model in 3 bullet points.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ChatPromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are helpfull assistent\"),\n",
    "        (\"user\", \"Summarize the concept of {concept} in 3 bullet points.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "message = template.format(concept=\"Large Language Model\")\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43492224-24ed-49a0-8ffe-8d1b0736f783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- **Statistical pattern learners:** Large Language Models (LLMs) are neural networks trained on massive text corpora to predict the next word (or token) given its context, learning statistical patterns of language rather than explicit rules.\n",
       "\n",
       "- **Scale and capability:** By scaling up the number of parameters (often billions) and the amount of training data, LLMs acquire broad knowledge and can perform diverse tasks‚Äîtranslation, summarization, reasoning‚Äîthrough simple prompting.\n",
       "\n",
       "- **Probabilistic generation:** LLMs output a probability distribution over possible next tokens, allowing them to generate coherent, context‚Äëaware text, but they lack true understanding or consciousness; their outputs reflect learned patterns and may contain errors or biases."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(message)\n",
    "\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eb8bcc-e967-43db-b0ba-7836d98592e7",
   "metadata": {},
   "source": [
    "- SystemMessage -- for content which should be passed to direct the conversation\n",
    "- HumanMessage -- for content in the input from the user.\n",
    "- AIMessage -- for content in the response from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "991ceee5-3faf-4f08-9d1e-94638ef97bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a wise and slightly humorous AI mentor.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Can you explain LangChain in one sentence?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='LangChain is a Python framework that helps you build apps powered by large language models.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Great! Can you also give me one real-world use case?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Message Types\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import AIMessage, SystemMessage, HumanMessage\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a wise and slightly humorous AI mentor.\"),\n",
    "        HumanMessage(content=\"Can you explain LangChain in one sentence?\"),\n",
    "        AIMessage(\n",
    "            content=\"LangChain is a Python framework that helps you build apps powered by large language models.\"\n",
    "        ),\n",
    "        HumanMessage(content=\"Great! Can you also give me one real-world use case?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "message = template.format_messages()\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7fece46-fce1-47c0-b125-dabe5a710286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Use‚Äëcase example:**‚ÄØA SaaS company builds a **customer‚Äësupport chatbot** with LangChain that pulls the latest product documentation, ticket history, and FAQ articles into a single LLM‚Äëdriven assistant‚Äîso when a user asks ‚ÄúHow do I reset my API key?‚Äù the bot can instantly retrieve the relevant policy, walk the user through the steps, and even log the interaction for future analytics, all without writing a custom retrieval‚Äëaugmented‚Äëgeneration pipeline from scratch."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(message).content\n",
    "\n",
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8a5ce-580f-4171-9868-d5d399d5d6aa",
   "metadata": {},
   "source": [
    "## Chain in LangChain\n",
    "A Chain is basically a pipeline that connects multiple steps together, where the output of one step becomes the input of the next.\n",
    "https://python.langchain.com/api_reference/langchain/chains.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524a8d6d-066f-45be-8574-1c8a77126c42",
   "metadata": {},
   "source": [
    "### 1. Single-Step Chain (LLMChain)\n",
    "This is the simplest form ‚Äî Prompt ‚Üí LLM ‚Üí Output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4e1da99-5657-4ada-bd87-84706a271ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Quantum Computing',\n",
       " 'text': '**Quantum Computing in a nutshell**\\n\\n1. **Bits vs. Qubits**  \\n   - *Classical computers* use **bits** that are either **0** or **1**.  \\n   - *Quantum computers* use **qubits** that can be **0, 1, or both at the same time** (thanks to a property called **super‚Äëposition**).\\n\\n2. **Super‚Äëposition**  \\n   Imagine a spinning coin. While it‚Äôs spinning, you can‚Äôt say it‚Äôs heads *or* tails‚Äîit‚Äôs effectively both until you look. A qubit works similarly: it holds many possibilities at once, letting a quantum computer explore many solutions in parallel.\\n\\n3. **Entanglement**  \\n   When two qubits become **entangled**, the state of one instantly influences the other, no matter how far apart they are. This creates a kind of ‚Äúinstant‚Äëcommunication‚Äù link that lets quantum computers coordinate many calculations at once.\\n\\n4. **Why it matters**  \\n   Because a quantum computer can process many possibilities simultaneously, it can solve certain problems (like factoring huge numbers, simulating molecules, or optimizing complex systems) far faster than any classical computer can.\\n\\n5. **A simple analogy**  \\n   - **Classical computer:** A single‚Äëlane road where cars (bits) can only go forward one at a time.  \\n   - **Quantum computer:** A multi‚Äëlane highway where each car can be in many lanes at once, and some cars are magically linked (entangled) so they move together in perfect sync. This lets the ‚Äútraffic‚Äù solve a puzzle much quicker.\\n\\n---\\n\\n### üéâ Fun Fact\\n\\nThe term **‚Äúquantum‚Äù** comes from the Latin word *quantum* meaning ‚Äúhow much.‚Äù In the 1900s, physicists discovered that energy isn‚Äôt continuous‚Äîit comes in tiny packets called **quanta**. That same idea‚Äîtiny, discrete units‚Äîunderlies the qubits that power quantum computers!'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain {topic} in simple terms and add one fun fact.\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=model, prompt=prompt)\n",
    "\n",
    "result = chain.invoke(\"Quantum Computing\")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6810e60b-ef38-4aab-81c9-6d2ef7d0957d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**What is cloud computing?**  \\nThink of the cloud as a giant, always‚Äëavailable ‚Äúdigital library‚Äù that lives on the internet instead of on your own computer or phone. Instead of storing files, running programs, or doing calculations on a single device, you ‚Äúborrow‚Äù the power and storage of big, remote computers (called servers) that are kept in special data‚Äëcenter buildings. You can access your data and apps from any device that‚Äôs connected to the internet‚Äîjust like you can read a book from a library no matter where you are, as long as you have a library card.\\n\\n**Simple analogy:**  \\nImagine you‚Äôre a chef who wants to bake a cake. Instead of buying a huge oven, all the ingredients, and a kitchen for every single cake you make, you go to a shared kitchen (the cloud). You pay only for the time you use the oven, the space you need, and the tools you need. When you‚Äôre done, you leave the kitchen clean and ready for the next chef. The cloud works the same way for software, storage, and even heavy‚Äëduty computing tasks.\\n\\n**Key points in plain language**\\n\\n| What you want to do | Where it happens in the cloud | Why it‚Äôs handy |\\n|--------------------|----------------------------|--------------|\\n| **Store photos, videos, documents** | On remote servers (the ‚Äúcloud‚Äù) | Access them from any phone, tablet, or computer‚Äîno need to carry a hard drive |\\n| **Run an app (like email, video chat, or games)** | The app runs on powerful computers far away | Your device can be small and cheap, yet still run powerful software |\\n| **Do heavy calculations (e.g., AI, big data)** | Big data centers with lots of processors | You get super‚Äëfast performance without buying expensive hardware |\\n\\n**One fun fact:**  \\nThe term ‚Äúcloud‚Äù comes from the little cloud symbol you see in diagrams of computer networks. It started as a simple way to show ‚Äúthe internet‚Äù in a picture, but the name stuck‚Äîso now we literally call it ‚Äúthe cloud‚Äù! üå•Ô∏è'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Runnable chain\n",
    "# Use pipe | operator to make a chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "res = chain.invoke(\"Cloud Computing\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820f6c6a-b33f-45b4-9228-62210cad1a3e",
   "metadata": {},
   "source": [
    "### 2. Multi-Step Chain (SimpleSequentialChain)\n",
    "This combines two LLM steps, first create an outline, then expand it into a blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2021750-3e0c-433a-b156-1fb573d6b4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'The Future of Renewable Energy', 'output': '**The Future of Renewable Energy**\\n\\nThe renewable‚Äëenergy landscape is undergoing a rapid transformation, driven by breakthroughs that were once science‚Äëfiction. **Advanced solar solutions** such as perovskite cells promise efficiencies above 30\\u202f%, while bifacial panels harvest light from both sides, boosting output without extra land. Floating solar farms now dot reservoirs and coastal lagoons, turning water surfaces into power generators. In the wind sector, **offshore floating turbines** can be anchored in deep water where winds are strongest, and high‚Äëaltitude kite or airborne wind systems capture jet‚Äëstream energy that traditional turbines can‚Äôt reach. Meanwhile, **solid‚Äëstate batteries**, hydrogen‚Äëbased storage, and AI‚Äëdriven grid optimization promise to smooth the intermittent nature of sun and wind, turning intermittent power into reliable, dispatchable energy.\\n\\nEconomic and policy forces are accelerating this shift. Carbon‚Äëpricing mechanisms, renewable‚Äëportfolio standards, and global climate accords create a regulatory backbone that rewards clean power. The Levelized Cost of Energy (LCOE) for solar and wind has fallen by more than 80\\u202f% in a decade, and economies of scale in manufacturing keep driving prices down. Green bonds, ESG‚Äëfocused capital, and public‚Äëprivate partnerships are unlocking trillions of dollars in financing, making large‚Äëscale projects financially viable.\\n\\nBeyond technology, the transition reshapes society. Decentralized micro‚Äëgrids and off‚Äëgrid solutions bring electricity to remote communities, while new jobs in manufacturing, installation, and maintenance fuel a green economy. Yet challenges remain: grid stability, storage scalability, and coordination across transport, industry, and housing must be addressed. By 2030‚Äë2050, renewables are projected to supply over 70\\u202f% of global electricity, a milestone that hinges on consumer choices, corporate commitments, and continued policy support. The future is bright, and it‚Äôs already here.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "outline_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Create a 3-point outline for a blog post about {topic}.\",\n",
    ")\n",
    "\n",
    "outline_chain = LLMChain(llm=model, prompt=outline_prompt)\n",
    "\n",
    "# Step 2: Content generator\n",
    "content_prompt = PromptTemplate(\n",
    "    input_variables=[\"outline\"],\n",
    "    template=\"Expand this outline into a detailed 200-word blog post:\\n{outline}\",\n",
    ")\n",
    "content_chain = LLMChain(llm=model, prompt=content_prompt)\n",
    "\n",
    "# Combine into one chain\n",
    "blog_chain = SimpleSequentialChain(chains=[outline_chain, content_chain])\n",
    "\n",
    "# Run the full pipeline\n",
    "final_result = blog_chain.invoke(\"The Future of Renewable Energy\")\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b90e2e7-5991-4ddf-9752-e268c99d28fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Great overview!**  \n",
      "Your piece captures the excitement and urgency surrounding the renewable‚Äëenergy transition. Below is a concise summary of the three main sections, followed by a few suggestions that could help tighten the narrative and add depth.\n",
      "\n",
      "---\n",
      "\n",
      "## üìã Quick Summary\n",
      "\n",
      "| **Section** | **Key Take‚Äëaways** |\n",
      "|------------|-------------------|\n",
      "| **1. Emerging Technologies** | ‚Ä¢ **Solar:** Perovskite cells, bifacial panels, solar‚Äëglass integration.<br>‚Ä¢ **Wind:** Floating offshore turbines, bladeless vortex generators, AI‚Äëoptimized layouts.<br>‚Ä¢ **Storage:** Solid‚Äëstate batteries, hydrogen‚Äëbased power‚Äëto‚Äëfuel, thermal‚Äëstorage (molten salt, concrete). |\n",
      "| **2. Economic & Policy Drivers** | ‚Ä¢ LCOE for solar/wind now < LCOE for new coal in many markets.<br>‚Ä¢ Carbon‚Äëpricing, renewable‚Äëportfolio standards, green‚Äëbond financing drive demand.<br>‚Ä¢ Corporate 100‚ÄØ% renewable procurement fuels micro‚Äëgrids and green‚Äëfund growth. |\n",
      "| **3. Societal Impact & Path Forward** | ‚Ä¢ Renewable rollout can close energy‚Äëaccess gaps and create millions of jobs.<br>‚Ä¢ Reskilling pathways for former fossil‚Äëfuel workers.<br>‚Ä¢ Roadmap: 40‚ÄØ% renewables by 2030 ‚Üí 70‚ÄØ% by 2040 ‚Üí fully decarbonized grid by 2050, requiring coordinated policy, industry scaling, and research breakthroughs. |\n",
      "\n",
      "---\n",
      "\n",
      "## üîß Suggestions for Strengthening the Piece\n",
      "\n",
      "| **Area** | **What to Add / Refine** |\n",
      "|----------|------------------------|\n",
      "| **Data & Sources** | Cite a few recent, credible sources (e.g., IEA 2024 report, BloombergNEF, IPCC) to back up claims like ‚ÄúLCOE below coal‚Äù and ‚Äúperovskite efficiencies >‚ÄØ25‚ÄØ%‚Äù. |\n",
      "| **Quantify Impact** | Add concrete numbers: e.g., ‚Äúperovskite‚Äësilicon tandem cells have reached 30‚ÄØ% efficiency in lab tests‚Äù or ‚Äúfloating offshore turbines can generate 10‚Äì15‚ÄØ% more energy than fixed‚Äëbottom units‚Äù. |\n",
      "| **Policy Examples** | Highlight specific policies that have proven effective: e.g., EU‚Äôs Renewable Energy Directive, California‚Äôs SB 100, China‚Äôs ‚Äúdual carbon‚Äù targets, or the UK‚Äôs Contracts for Difference (CfD) scheme. |\n",
      "| **Economic Lens** | Briefly discuss the **levelized cost of storage (LCOS)** and how it‚Äôs falling, which is crucial for integrating intermittent renewables. |\n",
      "| **Social Angle** | Add a short anecdote or case study (e.g., a solar‚Äëglass project in a low‚Äëincome urban district) to illustrate the ‚Äúenergy‚Äëaccess‚Äù point. |\n",
      "| **Roadmap Detail** | Break the 2050 roadmap into **milestones** (e.g., 2025‚Äë2029: scaling manufacturing; 2030‚Äë2034: grid‚Äëintegration pilots; 2035‚Äë2040: large‚Äëscale storage). This makes the roadmap feel more actionable. |\n",
      "| **Visual Elements** | If this will be a report or blog post, consider adding: <br>‚Ä¢ **Infographic** of the technology stack (solar ‚Üí wind ‚Üí storage).<br>‚Ä¢ **Timeline** of policy milestones.<br>‚Ä¢ **Map** of emerging offshore wind sites. |\n",
      "| **Call‚Äëto‚ÄëAction** | End with a concrete ‚Äúwhat can readers do?‚Äù ‚Äì e.g., ‚Äúsupport local renewable projects, invest in green bonds, or advocate for carbon‚Äëpricing legislation in your community.‚Äù |\n",
      "\n",
      "---\n",
      "\n",
      "## üìà What‚Äôs Next? (Ideas for a Follow‚ÄëUp Piece)\n",
      "\n",
      "1. **Deep‚ÄëDive on Perovskite** ‚Äì Technical challenges (stability, lead‚Äëfree alternatives) and the roadmap to commercial deployment.  \n",
      "2. **AI in Renewable Planning** ‚Äì Case studies of AI‚Äëoptimized wind farms (e.g., Google‚Äôs DeepMind for grid balancing).  \n",
      "3. **Hydrogen Economy** ‚Äì How green hydrogen can complement renewables in heavy‚Äëindustry and transport.  \n",
      "4. **Equity & Just Transition** ‚Äì Policies and programs that have successfully reskilled fossil‚Äëfuel workers (e.g., Germany‚Äôs ‚ÄúEnergiewende‚Äù training programs).  \n",
      "\n",
      "---\n",
      "\n",
      "### Final Thought\n",
      "\n",
      "Your article already paints a compelling picture of a clean‚Äëenergy future that‚Äôs already in motion. By weaving in a few concrete data points, policy examples, and a clear call‚Äëto‚Äëaction, you‚Äôll turn a strong overview into a compelling, actionable roadmap that resonates with policymakers, investors, and the general public alike. Keep the momentum‚Äîthis is exactly the kind of narrative that can help turn the ‚Äúinevitable‚Äù into reality. üåç‚ö°\n",
      "\n",
      "--- \n",
      "\n",
      "*Feel free to let me know if you‚Äôd like a deeper dive into any of the technologies, a draft of an infographic, or help fleshing out a specific policy case study.*\n"
     ]
    }
   ],
   "source": [
    "# Runnable chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "outline_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Create a 3-point outline for a blog post about {topic}.\",\n",
    ")\n",
    "\n",
    "outline_chain = outline_prompt | model | StrOutputParser()\n",
    "\n",
    "content_prompt = PromptTemplate(\n",
    "    input_variables=[\"outline\"],\n",
    "    template=\"Expand this outline into a detailed 200-word blog post:\\n{outline}\",\n",
    ")\n",
    "content_chain = content_prompt | model | StrOutputParser()\n",
    "\n",
    "blog_chain = outline_chain | content_chain | model | StrOutputParser()\n",
    "\n",
    "# Run the full pipeline\n",
    "final_result = blog_chain.invoke({\"The Future of Renewable Energy\"})\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b4470-1070-4a2c-a355-11fe4b2c15d2",
   "metadata": {},
   "source": [
    "##  Memory in LangChain\n",
    "By default, LangChain calls are stateless, the model only sees the current input and forgets everything else.\n",
    "https://python.langchain.com/api_reference/langchain/memory.html <br>\n",
    "https://python.langchain.com/docs/versions/migrating_memory/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d06d50ef-9650-40dc-9d4d-d60a5bb0bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"\n",
    "    You are a friendly chatbot.\n",
    "    User: {question}\n",
    "    Assistant:\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# Create chain\n",
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "74913969-11d3-4fd1-b41c-36515be6fb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Saad! üëã Great to meet you. How‚Äôs your day going so far?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\"Hi! I am Saad\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0a00152-7309-4578-bf80-9fe13cf09b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I‚Äôm afraid I don‚Äôt have your name yet‚Äîcould you remind me? üòä Once you tell me, I‚Äôll keep it in mind for the rest of our chat!'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\"Can you remeber my name?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed31d4cc-9b3c-475c-8b76-6579f22e60c8",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory\n",
    "- Stores: Full conversation history.\n",
    "- Use: Small chats where cost & token limits aren‚Äôt an issue.\n",
    "- Pro: Simple and easy.\n",
    "- Con: Can grow too large for long sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e96f423-9db1-42cf-9949-41ed49bf8ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aa304\\AppData\\Local\\Temp\\ipykernel_21716\\1956154715.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create Memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"question\"],\n",
    "    template=\"\"\"\n",
    "    You are an AI Assistant for **Advanced Telecom Services (ATS)**.\n",
    "    Conversation so far:\n",
    "    {chat_history}\n",
    "    User: {question}\n",
    "    Assistant:\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# Create chain\n",
    "chain = LLMChain(llm=model, prompt=prompt, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d55600ff-ee09-43ce-8b11-d26ce02e7c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aa304\\AppData\\Local\\Temp\\ipykernel_21716\\2420808583.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain.run(\"Hi!, My name is Saad, How can you help me?\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Saad! üëã I'm your virtual assistant for **Advanced Telecom Services (ATS)**. I‚Äôm here to help you with anything from choosing the right mobile or internet plan, checking your bill, troubleshooting a service issue, or learning about our latest offers and features.\\n\\nWhat can I assist you with today? (e.g.,\\u202fplan options, billing, technical support, new services, etc.)\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.run(\"Hi!, My name is Saad, How can you help me?\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33b03e99-e30a-4b17-aa5c-59ef14b36b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is **Saad**. How can I assist you further today?'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.run(\"Can you remeber, What's my name?\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a96d91b",
   "metadata": {},
   "source": [
    "# LLM-Powered Chatbot with Conversation History\n",
    "\n",
    "We'll go over an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a **conversation** and **remember previous interactions** with a chat model.\n",
    "\n",
    "Source: [LangChain Docs](https://python.langchain.com/docs/tutorials/chatbot/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5fd3a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"openai/gpt-oss-120b\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99f0e58",
   "metadata": {},
   "source": [
    "`ChatModels` in LangChain work like special objects called \"Runnables\", which means they have a common way you can use them.\n",
    "If you just want to run the model, you give it a list of messages and use the `.invoke` method ‚Äî that‚Äôs how you ‚Äútalk‚Äù to it and get a response.\n",
    "\n",
    "It‚Äôs like: ‚ÄúPut your messages in a box, hand it to the .invoke button, and the model will answer.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "62ac8f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Ahmed! üëã Nice to meet you. How can I assist you today?', additional_kwargs={'reasoning_content': 'The user says \"Hi! I am Ahmed.\" It\\'s a greeting and introduction. We should respond politely, greeting back, maybe ask how we can help. No special instructions. Just respond friendly.'}, response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 77, 'total_tokens': 143, 'completion_time': 0.119421001, 'prompt_time': 0.004487928, 'queue_time': 0.001880367, 'total_time': 0.123908929}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_b9ee615ece', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--4b61e73b-a9d1-4d99-8825-fa9c2e87466b-0', usage_metadata={'input_tokens': 77, 'output_tokens': 66, 'total_tokens': 143})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I am Ahmed.\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a3c58e",
   "metadata": {},
   "source": [
    "The model on its own does not have any concept of state. For example, if you ask a followup question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a122b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I don‚Äôt have any information about your name. If you‚Äôd like me to address you by name, feel free to let me know!', additional_kwargs={'reasoning_content': 'The user asks \"What\\'s my name?\" We have no prior conversation. The user hasn\\'t provided their name. According to policy, we cannot guess or fabricate personal info. We can ask for clarification. So we should respond asking for their name or say we don\\'t have that info.'}, response_metadata={'token_usage': {'completion_tokens': 93, 'prompt_tokens': 75, 'total_tokens': 168, 'completion_time': 0.169508298, 'prompt_time': 0.004467525, 'queue_time': 0.001955639, 'total_time': 0.173975823}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_4a19b1544c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--5baf73b6-4f4e-42ae-8083-d7513295d587-0', usage_metadata={'input_tokens': 75, 'output_tokens': 93, 'total_tokens': 168})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a71975",
   "metadata": {},
   "source": [
    "We can see that it doesn't take the previous conversation turn into context, and cannot answer the question. This makes for a terrible chatbot experience!\n",
    "\n",
    "To get around this, we need to pass the entire [conversation history](https://python.langchain.com/docs/concepts/chat_history/) into the model. Let's see what happens when we do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "41882f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You introduced yourself as **Ahmed**. If there‚Äôs anything specific you‚Äôd like to talk about or any question you have, just let me know!', additional_kwargs={'reasoning_content': 'User asks \"Who am I?\" They previously said \"Hi! I am Ahmed.\" So answer: Ahmed. Could ask for more context? Probably respond that you are Ahmed.'}, response_metadata={'token_usage': {'completion_tokens': 75, 'prompt_tokens': 108, 'total_tokens': 183, 'completion_time': 0.136154885, 'prompt_time': 0.005520813, 'queue_time': 0.001786645, 'total_time': 0.141675698}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_8db49de948', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--2ab8a726-f560-4807-8c7b-ae8724cdeecf-0', usage_metadata={'input_tokens': 108, 'output_tokens': 75, 'total_tokens': 183})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I am Ahmed.\"),\n",
    "        AIMessage(\n",
    "            content=\"Hello Ahmed! üëã Nice to meet you. How can I assist you today?\"\n",
    "        ),\n",
    "        HumanMessage(content=\"Who am I?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31162d5b",
   "metadata": {},
   "source": [
    "And now we can see that we get a good response!\n",
    "\n",
    "This is the basic idea underpinning a chatbot's ability to interact conversationally. So how do we best implement this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cad5b3",
   "metadata": {},
   "source": [
    "# Message persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d52e2",
   "metadata": {},
   "source": [
    "LangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\n",
    "\n",
    "Wrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.\n",
    "\n",
    "LangGraph comes with a simple in-memory checkpointer, which we use below. See its [documentation](https://langchain-ai.github.io/langgraph/concepts/persistence/?_gl=1*fi5jbl*_gcl_au*MTE0NDQ3MDcyNC4xNzU0OTExODMx*_ga*MTU2Mzg1ODA3NS4xNzQ5NjMyNjk5*_ga_47WX3HKKY2*czE3NTQ5MTE4MjYkbzckZzEkdDE3NTQ5MTE4MzckajQ5JGwwJGgw) for more detail, including how to use different persistence backends (e.g., SQLite or Postgres)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bfe580a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x000001F1578902F0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modern usage with LangGraph: Production-friendly way to store chat state.\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fbd2af",
   "metadata": {},
   "source": [
    "We now need to create a `config` that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a `thread_id`. This should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f0653126",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4576d23f",
   "metadata": {},
   "source": [
    "This enables us to support multiple conversation threads with a single application, a common requirement when your application has multiple users.\n",
    "\n",
    "We can then invoke the application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "78abe6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi! I am Ahmed.', additional_kwargs={}, response_metadata={}, id='544b4147-86e8-43bd-b51f-62292b72e703'),\n",
       "  AIMessage(content='Hello Ahmed! üëã Nice to meet you. How can I assist you today?', additional_kwargs={'reasoning_content': 'The user says \"Hi! I am Ahmed.\" It\\'s a greeting and introduction. We should respond politely, greeting back, maybe ask how we can help. No special instructions. Just respond friendly.'}, response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 77, 'total_tokens': 143, 'completion_time': 0.119635286, 'prompt_time': 0.004475704, 'queue_time': 0.001564642, 'total_time': 0.12411099}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_8db49de948', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--f5f02d3b-6bb0-4ba7-9e3c-855fcf518f98-0', usage_metadata={'input_tokens': 77, 'output_tokens': 66, 'total_tokens': 143})]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Hi! I am Ahmed.\"\n",
    "\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d49358e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Ahmed! üëã Nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "31b9372d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, you introduced yourself as **Ahmed**. üòä How can I help you today, Ahmed?\n"
     ]
    }
   ],
   "source": [
    "query = \"Do you know my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c50f32",
   "metadata": {},
   "source": [
    "Great! Our chatbot now remembers things about us. If we change the config to reference a different `thread_id`, we can see that it starts the conversation fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ed1d0894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don‚Äôt know your name‚Äîunless you‚Äôve already mentioned it earlier in the conversation (which I don‚Äôt see). If you‚Äôd like me to address you by name, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "new_config = {\"configurable\": {\"thread_id\": \"xyz890\"}}\n",
    "\n",
    "query = \"Do you know my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config=new_config)\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4d6633",
   "metadata": {},
   "source": [
    "However, we can always go back to the original conversation (since we are persisting it in a database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1598a580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes‚Äîyou told me your name is **Ahmed**. How can I assist you further?\n"
     ]
    }
   ],
   "source": [
    "query = \"Do you know my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898c30dc",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "This is how we can support a chatbot having conversations with many users!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
