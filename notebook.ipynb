{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1276c1f",
   "metadata": {},
   "source": [
    "# Introduction to LangChain and LangGraph\n",
    "\n",
    "## What is LangChain?\n",
    "\n",
    "LangChain is a powerful framework for building applications powered by Large Language Models (LLMs). Think of it as a toolkit that helps you create AI-powered applications more easily by providing:\n",
    "\n",
    "1. [**Components**](https://python.langchain.com/docs/integrations/components/): Ready-to-use building blocks for working with LLMs\n",
    "2. [**Chains**](https://python.langchain.com/api_reference/langchain/chains.html): Ways to combine these components into useful applications\n",
    "3. [**Memory**](https://python.langchain.com/api_reference/langchain/memory.html): Tools to help LLMs [remember conversation history\n",
    "4. [**Agents**](https://python.langchain.com/api_reference/langchain/agents.html): Autonomous AI assistants that can use tools and make decisions\n",
    "\n",
    "## What is LangGraph?\n",
    "\n",
    "LangGraph is an extension of LangChain that helps you build stateful, multi-step applications. It's particularly useful for:\n",
    "- Building conversational agents that remember context\n",
    "- Creating workflows where each step depends on previous steps\n",
    "- Managing state and persistence in your applications\n",
    "\n",
    "## The LangChain Ecosystem\n",
    "\n",
    "LangChain offers a complete ecosystem for LLM application development:\n",
    "\n",
    "<img src=\"https://python.langchain.com/svg/langchain_stack_112024_dark.svg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77ad0c0",
   "metadata": {},
   "source": [
    "## Key Features of LangChain\n",
    "\n",
    "1. **Prompts Management**: Easy creation and reuse of prompts\n",
    "2. **Model Integration**: Support for multiple LLM providers\n",
    "3. **Chain Building**: Combine multiple steps into pipelines\n",
    "4. **Memory Systems**: Add memory to your LLM applications\n",
    "5. **Agents & Tools**: Create AI assistants that can use tools\n",
    "\n",
    "LangChain was created by Harrison Chase and first released in October 2022. It has since become one of the most popular frameworks for building LLM applications with over [113K GitHub stars](https://github.com/langchain-ai/langchain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ceed41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import HTML, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d24499",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0c5cc-b3c4-4ccc-b2f1-df018237dbea",
   "metadata": {},
   "source": [
    "## [Chat Models](https://python.langchain.com/docs/integrations/chat/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ec89812-d456-4351-8eed-3d0b5614594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"openai/gpt-oss-120b\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50dc2ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I help you today?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(\"hi\")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4c1ce-8afd-497d-8a5a-91573678b9c5",
   "metadata": {},
   "source": [
    "## [Working with Prompts Templates](https://python.langchain.com/docs/concepts/prompt_templates/)\n",
    "\n",
    "LangChain provides powerful tools for working with prompts:\n",
    "\n",
    "1. **PromptTemplate**: For creating reusable text prompts with variables\n",
    "2. **ChatPromptTemplate**: For creating structured chat prompts with different message types:\n",
    "   - `SystemMessage`: Instructions or context for the AI\n",
    "   - `HumanMessage`: User inputs\n",
    "   - `AIMessage`: AI responses\n",
    "\n",
    "Prompt templates help you:\n",
    "- Keep your prompts consistent\n",
    "- Make prompts reusable with variables\n",
    "- Structure multi-turn conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8330e5a6-7622-467c-9183-4edb332e194b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  Explain Large Language Language in simple Term\n"
     ]
    }
   ],
   "source": [
    "# Single Input Variable\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate(template=\"Explain {topic} in simple Term\")\n",
    "\n",
    "prompt = template.format(topic=\"Large Language Language\")\n",
    "\n",
    "print(\"Prompt: \", prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dd359ba-73f1-4093-94d9-5e874fd71f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Large Language Model (LLM) ‚Äì a simple, everyday‚Äëstyle explanation**\n",
       "\n",
       "---\n",
       "\n",
       "### 1. What it is, in one sentence  \n",
       "A **large language model** is a computer program that has read a huge amount of text and learns how to predict the next word in a sentence, so it can write, answer questions, translate, and do many other language‚Äërelated tasks.\n",
       "\n",
       "---\n",
       "\n",
       "### 2. How it works ‚Äì the ‚Äústory‚Äù version  \n",
       "\n",
       "| Step | What happens | Everyday analogy |\n",
       "|------|--------------|----------------|\n",
       "| **1. Reading a huge library** | The model is fed billions of words from books, articles, websites, etc. | **Like a kid who reads thousands of books** |\n",
       "| **2. Learning patterns** | It looks for patterns: ‚ÄúIf the sentence says ‚ÄòThe cat is ‚Ä¶‚Äô, the next word is often ‚Äòsleeping‚Äô or ‚Äòon the roof‚Äô.‚Äù | **Like learning that ‚Äúpeanut butter and ‚Ä¶‚Äù is usually followed by ‚Äújelly‚Äù.** |\n",
       "| **3. Guessing the next word** | When you give it a prompt, the model predicts the most likely next word, then the next, and so on, until it forms a full sentence. | **Like playing a game of ‚Äúfill‚Äëin‚Äëthe‚Äëblank‚Äù** |\n",
       "| **4. Using the guess** | The string of predicted words becomes the answer, story, translation, etc. | **Like a storyteller finishing a story you started.** |\n",
       "\n",
       "---\n",
       "\n",
       "### 3. Why ‚Äúlarge‚Äù matters  \n",
       "\n",
       "- **Lots of data**: The more text it reads, the better it gets at spotting subtle patterns.\n",
       "- **Big brain (parameters)**: Think of ‚Äúparameters‚Äù as tiny knobs that the model adjusts while learning. A ‚Äúlarge‚Äù model has billions of these knobs, giving it a finer‚Äëgrained sense of language.\n",
       "\n",
       "---\n",
       "\n",
       "### 4. What it can do (in simple terms)\n",
       "\n",
       "| Task | Example |\n",
       "|------|--------|\n",
       "| **Answer questions** | ‚ÄúWhat‚Äôs the capital of France?‚Äù ‚Üí ‚ÄúParis.‚Äù |\n",
       "| **Write text** | Write a short story about a dragon. |\n",
       "| **Translate** | Turn English into Spanish. |\n",
       "| **Summarize** | Turn a long article into a short paragraph. |\n",
       "| **Chat** | Have a conversation like we‚Äôre doing now. |\n",
       "\n",
       "---\n",
       "\n",
       "### 5. What it can‚Äôt do (or does poorly)\n",
       "\n",
       "- **Understand the world** the way humans do. It only knows what it has seen in text.\n",
       "- **Feel emotions** or have personal experiences.\n",
       "- **Guarantee truth** ‚Äì it can confidently say something that‚Äôs wrong if the pattern it learned suggests that answer.\n",
       "\n",
       "---\n",
       "\n",
       "### 6. Quick analogy: **The Predictive Text on Your Phone**\n",
       "\n",
       "- When you type ‚ÄúI‚Äôm going to the ‚Ä¶‚Äù, your phone suggests ‚Äústore‚Äù, ‚Äúpark‚Äù, ‚Äúgym‚Äù, etc.  \n",
       "- A large language model does the same thing, but on a **much larger scale**: it can generate whole paragraphs, not just a single word, and it has learned from **all** the text on the internet, not just your personal messages.\n",
       "\n",
       "---\n",
       "\n",
       "### 7. TL;DR (the ultra‚Äëshort version)\n",
       "\n",
       "A **large language model** is a computer program that reads a massive amount of text, learns how words usually follow each other, and then uses that knowledge to generate or understand language‚Äîmuch like a very well‚Äëread, super‚Äëfast ‚Äúautocomplete‚Äù that can write essays, answer questions, translate languages, and more. \n",
       "\n",
       "--- \n",
       "\n",
       "*Hope that makes the idea clear! If you want a deeper dive into any part (like how the ‚Äútiny knobs‚Äù work, or how training happens), just let me know.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa80eb04-99a0-478e-bdfd-155cbadb87d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: You are helpfull assistent\\nHuman: Summarize the concept of Large Language Model in 3 bullet points.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ChatPromptTemplate\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are helpfull assistent\"),\n",
    "        (\"user\", \"Summarize the concept of {concept} in 3 bullet points.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "message = template.format(concept=\"Large Language Model\")\n",
    "\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43492224-24ed-49a0-8ffe-8d1b0736f783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- **Statistical pattern learners:** Large Language Models (LLMs) are neural networks trained on massive text corpora to predict the next word (or token) in a sequence, learning statistical patterns, grammar, facts, and reasoning abilities from the data.\n",
       "\n",
       "- **Scale and architecture:** They typically use transformer architectures with billions (or more) of parameters, enabling them to capture long‚Äërange dependencies and generate coherent, context‚Äëaware text across many domains.\n",
       "\n",
       "- **Versatile applications:** Once trained, LLMs can be prompted or fine‚Äëtuned for tasks such as translation, summarization, coding, and conversational agents, often achieving performance comparable to or surpassing specialized models."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(message)\n",
    "\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eb8bcc-e967-43db-b0ba-7836d98592e7",
   "metadata": {},
   "source": [
    "### Using Message Types\n",
    "- SystemMessage -- for content which should be passed to direct the conversation\n",
    "- HumanMessage -- for content in the input from the user.\n",
    "- AIMessage -- for content in the response from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "991ceee5-3faf-4f08-9d1e-94638ef97bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a wise and slightly humorous AI mentor.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Can you explain LangChain in one sentence?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='LangChain is a Python framework that helps you build apps powered by large language models.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Great! Can you also give me one real-world use case?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Message Types\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a wise and slightly humorous AI mentor.\"),\n",
    "        HumanMessage(content=\"Can you explain LangChain in one sentence?\"),\n",
    "        AIMessage(content=\"LangChain is a Python framework that helps you build apps powered by large language models.\"),\n",
    "        HumanMessage(content=\"Great! Can you also give me one real-world use case?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "message = template.format_messages()\n",
    "\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7fece46-fce1-47c0-b125-dabe5a710286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Use‚Äëcase: AI‚Äëpowered customer‚Äësupport chatbot that pulls in‚Äëdepth product knowledge from a company‚Äôs internal docs, FAQs, and support tickets, then uses a language model (via LangChain) to understand a user‚Äôs question, retrieve the most relevant sections, and generate a helpful, context‚Äëaware response in real time.** \n",
       "\n",
       "*Why it shines:*  \n",
       "- **Document retrieval** (vector‚Äëstore search) pulls the exact policy or troubleshooting step the user needs.  \n",
       "- **Chain orchestration** lets you combine a *retriever* ‚Üí *LLM* ‚Üí *post‚Äëprocessing* (e.g., tone‚Äëadjustment, escalation check) all in one reusable pipeline.  \n",
       "- **Scalable**: you can add new knowledge bases (manuals, release notes, chat logs) without rewriting the bot‚Äîjust re‚Äëindex and the same chain handles the rest.  \n",
       "\n",
       "In short, LangChain lets you stitch together ‚Äúsearch‚Äëthe‚Äëknowledge‚Äëbase‚Äù + ‚Äúgenerate‚Äëthe‚Äëanswer‚Äù + ‚Äúapply‚Äëbusiness‚Äërules‚Äù into a single, maintainable chatbot. üöÄ"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(message).content\n",
    "\n",
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8a5ce-580f-4171-9868-d5d399d5d6aa",
   "metadata": {},
   "source": [
    "## Chains in LangChain\n",
    "\n",
    "A Chain in LangChain is a sequence of steps that process inputs to produce outputs. There are two ways to create chains:\n",
    "\n",
    "1. **Classic Chains**: Using classes like `LLMChain`\n",
    "2. **LCEL (LangChain Expression Language)**: Using the modern pipe operator `|`\n",
    "\n",
    "LCEL is the recommended way to build chains as it's:\n",
    "- More flexible\n",
    "- Easier to debug\n",
    "- Better for streaming\n",
    "- More performant\n",
    "\n",
    "Let's look at examples of both approaches:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524a8d6d-066f-45be-8574-1c8a77126c42",
   "metadata": {},
   "source": [
    "### 1. Single-Step Chain (LLMChain)\n",
    "This is the simplest form ‚Äî Prompt ‚Üí LLM ‚Üí Output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4e1da99-5657-4ada-bd87-84706a271ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aa304\\AppData\\Local\\Temp\\ipykernel_17232\\1870853064.py:9: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=model, prompt=prompt)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'topic': 'Quantum Computing',\n",
       " 'text': '**Quantum Computing in a nutshell**\\n\\n1. **Bits vs. Qubits**  \\n   - *Classical computer*: uses **bits** that are either **0** or **1**.  \\n   - *Quantum computer*: uses **qubits** that can be **0, 1, or both at the same time** (thanks to a property called **super‚Äëposition**).\\n\\n2. **Super‚Äëposition**  \\n   Think of a spinning coin. While it‚Äôs spinning, you can‚Äôt say it‚Äôs heads *or* tails‚Äîit‚Äôs in a blend of both. A qubit can hold a blend of 0 and 1 simultaneously, letting a quantum computer explore many possibilities at once.\\n\\n3. **Entanglement**  \\n   When two qubits become **entangled**, the state of one instantly influences the other, no matter how far apart they are. This lets a quantum computer link qubits together in a way that lets them ‚Äútalk‚Äù instantly, giving the machine extra ‚Äúco‚Äëordination power‚Äù.\\n\\n4. **Why it matters**  \\n   Because a quantum computer can try many solutions at the same time, it can solve certain problems (like factoring huge numbers, simulating molecules, or optimizing complex systems) far faster than any classical computer can.\\n\\n5. **A simple analogy**  \\n   - **Classical**: Searching for a name in a phone book by reading each entry one by one.  \\n   - **Quantum**: Having a magical ‚Äúsuper‚Äësearch‚Äù that looks at *all* pages at once and instantly tells you where the name is (but only for certain types of problems).\\n\\n---\\n\\n### üéâ Fun Fact\\n\\nThe term **‚Äúquantum‚Äù** comes from the Latin word *quantum* meaning ‚Äúhow much‚Äù. The first quantum computer was built in 1998, and it was so tiny that it fit inside a refrigerator‚Äîbecause the delicate quantum states need to be kept **extremely cold**, often just a few thousandths of a degree above absolute zero! üöÄ'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain {topic} in simple terms and add one fun fact.\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=model, prompt=prompt)\n",
    "\n",
    "result = chain.invoke(\"Quantum Computing\")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7790487e",
   "metadata": {},
   "source": [
    "**LCEL (LangChain Expression Language)**: Using the modern pipe operator `|`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6810e60b-ef38-4aab-81c9-6d2ef7d0957d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**What is cloud computing?**  \n",
       "Think of the cloud as a giant, shared computer that lives on the internet instead of on your own desk. Instead of buying and maintaining your own hardware (like a big server or a bunch of hard drives), you ‚Äúrent‚Äù space and power from this remote computer. You can store files, run apps, and process data on the cloud, and you access everything through the internet‚Äîjust like you open a website or use an app on your phone. The cloud does the heavy lifting, so your own device can stay small, cheap, and always up‚Äëto‚Äëdate.\n",
       "\n",
       "**Fun fact:** The term ‚Äúcloud‚Äù comes from the old practice of drawing a cloud shape on diagrams to represent the internet. The ‚Äúcloud‚Äù we use today is literally the same symbol‚Äîjust now it‚Äôs a massive, real‚Äëworld network of data centers spread across the globe! üöÄ"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Runnable chain\n",
    "# Use pipe | operator to make a chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "response = chain.invoke(\"Cloud Computing\")\n",
    "\n",
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820f6c6a-b33f-45b4-9228-62210cad1a3e",
   "metadata": {},
   "source": [
    "### 2. Multi-Step Chain (SimpleSequentialChain)\n",
    "This combines two LLM steps, first create an outline, then expand it into a blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2021750-3e0c-433a-b156-1fb573d6b4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'The Future of Renewable Energy',\n",
       " 'output': '**The Future of Renewable Energy**\\n\\nThe renewable‚Äëenergy landscape is undergoing a rapid transformation, driven by breakthroughs that were once science‚Äëfiction. **Advanced solar solutions**‚Äîfrom perovskite cells that promise 30\\u202f% higher efficiency to bifacial panels that harvest light from both sides‚Äîare now being deployed on floating platforms that turn reservoirs into power plants. Meanwhile, **next‚Äëgeneration wind** is taking to the seas and skies: floating offshore turbines tap stronger, steadier winds, while high‚Äëaltitude kite and airborne systems capture energy at altitudes where wind speeds are three‚Äëto‚Äëfour times higher than on land. The real game‚Äëchanger, however, is **storage and grid intelligence**. Solid‚Äëstate batteries deliver longer life and faster charging, hydrogen‚Äëbased systems store excess power for weeks, and AI‚Äëdriven grid optimization balances supply and demand in real time, making intermittent sources reliable.\\n\\nPolicy and economics are the twin engines accelerating this shift. Carbon‚Äëpricing mechanisms, renewable‚Äëportfolio standards, and the Paris Agreement‚Äôs targets create a predictable regulatory environment. Simultaneously, the Levelized Cost of Energy (LCOE) for solar and wind has fallen below $0.05\\u202fkWh in many markets, and economies of scale are driving prices even lower. Green bonds, ESG‚Äëfocused funds, and public‚Äëprivate partnerships are funneling trillions of dollars into clean‚Äëtech pipelines.\\n\\nBeyond technology, the **societal impact** is profound. Decentralized micro‚Äëgrids bring electricity to remote villages, while new jobs in manufacturing, installation, and maintenance are reshaping local economies. Yet challenges remain: grid stability, scalable storage, and coordination across transport, industry, and housing sectors must be addressed. Looking ahead to 2030‚Äë2050, renewables could supply 70\\u202f% of global electricity, cutting CO‚ÇÇ emissions by half. Achieving this vision will hinge on consumer behavior, digital innovation, and a shared commitment to a cleaner, more equitable energy future.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "outline_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Create a 3-point outline for a blog post about {topic}.\",\n",
    ")\n",
    "\n",
    "outline_chain = LLMChain(llm=model, prompt=outline_prompt)\n",
    "\n",
    "# Step 2: Content generator\n",
    "content_prompt = PromptTemplate(\n",
    "    input_variables=[\"outline\"],\n",
    "    template=\"Expand this outline into a detailed 200-word blog post:\\n{outline}\",\n",
    ")\n",
    "content_chain = LLMChain(llm=model, prompt=content_prompt)\n",
    "\n",
    "# Combine into one chain\n",
    "blog_chain = SimpleSequentialChain(chains=[outline_chain, content_chain])\n",
    "\n",
    "# Run the full pipeline\n",
    "final_result = blog_chain.invoke(\"The Future of Renewable Energy\")\n",
    "\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b90e2e7-5991-4ddf-9752-e268c99d28fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Summary ‚Äì The Future of Renewable Energy**\n",
       "\n",
       "| **Theme** | **Key Points** | **Implications** |\n",
       "|----------|---------------|----------------|\n",
       "| **1. Emerging Technologies** | ‚Ä¢ **Perovskite & bifacial solar** ‚Äì higher efficiency, lower cost, integration into buildings.<br>‚Ä¢ **Floating offshore & bladeless wind** ‚Äì access to deep‚Äëwater resources, reduced noise/maintenance, AI‚Äëoptimized layouts.<br>‚Ä¢ **Advanced storage** ‚Äì solid‚Äëstate batteries, hydrogen, thermal‚Äëstorage systems close the intermittency gap. | Enables higher‚Äëdensity, ubiquitous generation and reliable, on‚Äëdemand renewable power. |\n",
       "| **2. Economic & Policy Drivers** | ‚Ä¢ **LCOE decline** ‚Äì economies of scale, manufacturing efficiencies.<br>‚Ä¢ **Policy tools** ‚Äì carbon pricing, renewable‚Äëportfolio standards, green‚Äëbond financing.<br>‚Ä¢ **Corporate procurement & micro‚Äëgrids** ‚Äì direct clean‚Äëpower purchases, community‚Äëlevel resilience.<br>‚Ä¢ **Green‚Äëinvestment funds** ‚Äì capital directed to projects meeting financial & environmental criteria. | Creates a predictable, investment‚Äëfriendly market that accelerates deployment. |\n",
       "| **3. Societal Impact & Path Forward** | ‚Ä¢ **Energy equity** ‚Äì affordable, reliable power for underserved communities and developing nations.<br>‚Ä¢ **Smart‚Äëgrid & digital‚Äëtwin** ‚Äì real‚Äëtime optimization, demand‚Äëresponse, resilient grid operations.<br>‚Ä¢ **Roadmaps (2030‚Äë2050)** ‚Äì government, industry, academia set milestones; success hinges on collaboration, research, and public‚Äëprivate partnerships. | Moves the transition from a technology‚Äëonly story to a holistic, inclusive, and resilient energy system. |\n",
       "\n",
       "---\n",
       "\n",
       "### Why These Trends Matter\n",
       "\n",
       "1. **Technology‚Äëdriven cost reductions** make renewables competitive with fossil fuels without subsidies.\n",
       "2. **Policy certainty** (e.g., carbon pricing) de‚Äëriskes investments, attracting both institutional and corporate capital.\n",
       "3. **Storage breakthroughs** eliminate the ‚Äúintermittency‚Äù barrier, enabling renewables to serve baseload demand.\n",
       "4. **Equity focus** ensures that the transition benefits all socioeconomic groups, reducing the global energy divide.\n",
       "5. **Digital integration** (smart grids, digital twins) maximizes efficiency, reduces waste, and improves reliability.\n",
       "\n",
       "---\n",
       "\n",
       "### Actionable Takeaways\n",
       "\n",
       "| **For Policymakers** | **For Investors & Corporates** | **For Researchers & Innovators** |\n",
       "|----------------------|------------------------------|--------------------------------|\n",
       "| ‚Ä¢ Strengthen carbon‚Äëpricing mechanisms and expand renewable‚Äëportfolio standards.<br>‚Ä¢ Incentivize community‚Äëowned micro‚Äëgrids and energy‚Äëjustice programs. | ‚Ä¢ Allocate capital to projects with clear ESG metrics and proven technology readiness.<br>‚Ä¢ Use corporate power purchase agreements (PPAs) to lock in clean‚Äëenergy supply. | ‚Ä¢ Accelerate scaling of perovskite and solid‚Äëstate battery production.<br>‚Ä¢ Develop AI‚Äëdriven grid‚Äëoptimization tools and digital‚Äëtwin platforms. |\n",
       "| ‚Ä¢ Support financing tools (green bonds, climate‚Äëfinance) for early‚Äëstage projects. | ‚Ä¢ Diversify procurement: combine utility‚Äëscale, distributed, and storage assets. | ‚Ä¢ Focus on low‚Äëcost, high‚Äëdurability materials for offshore and floating wind. |\n",
       "| ‚Ä¢ Promote energy‚Äëequity programs (subsidies, community‚Äëownership models). | ‚Ä¢ Partner with local utilities to integrate micro‚Äëgrids and demand‚Äëresponse programs. | ‚Ä¢ Collaborate with policy makers to align research roadmaps with 2030‚Äë2050 targets. |\n",
       "\n",
       "---\n",
       "\n",
       "### Outlook to 2050\n",
       "\n",
       "- **2030:** >50‚ÄØ% of global electricity from renewables; widespread adoption of perovskite solar and floating offshore wind; solid‚Äëstate batteries in commercial use.\n",
       "- **2040:** Grid‚Äëscale storage (hydrogen, thermal) provides >30‚ÄØ% of system flexibility; smart‚Äëgrid and digital‚Äëtwin platforms become standard.\n",
       "- **2050:** Near‚Äëzero‚Äëcarbon electricity grid in most developed nations; energy‚Äëequity metrics integrated into national energy plans; global renewable share >80‚ÄØ% with residual fossil use limited to hard‚Äëto‚Äëdecarbonize sectors.\n",
       "\n",
       "---\n",
       "\n",
       "**Bottom line:** The convergence of cutting‚Äëedge technology, supportive policy frameworks, and a focus on equity is turning clean energy from a hopeful vision into an inevitable reality. The next decade will be decisive‚Äîthose who invest, innovate, and legislate now will shape a resilient, inclusive, and sustainable energy landscape for generations to come."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Runnable chain\n",
    "\n",
    "outline_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Create a 3-point outline for a blog post about {topic}.\",\n",
    ")\n",
    "\n",
    "outline_chain = outline_prompt | model | StrOutputParser()\n",
    "\n",
    "content_prompt = PromptTemplate(\n",
    "    input_variables=[\"outline\"],\n",
    "    template=\"Expand this outline into a detailed 200-word blog post:\\n{outline}\",\n",
    ")\n",
    "content_chain = content_prompt | model | StrOutputParser()\n",
    "\n",
    "blog_chain = outline_chain | content_chain | model | StrOutputParser()\n",
    "\n",
    "# Run the full pipeline\n",
    "final_result = blog_chain.invoke({\"The Future of Renewable Energy\"})\n",
    "\n",
    "Markdown(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b4470-1070-4a2c-a355-11fe4b2c15d2",
   "metadata": {},
   "source": [
    "# Memory in LangChain and LangGraph\n",
    "\n",
    "LLMs are stateless by default - they don't remember previous conversations. LangChain provides two ways to add memory:\n",
    "\n",
    "1. **LangChain Memory**: Traditional memory systems like `ConversationBufferMemory` (Deprecated)\n",
    "   - Simple to use\n",
    "   - Good for basic chatbots\n",
    "   - Limited scalability\n",
    "\n",
    "2. **LangGraph State Management**: Modern approach using `StateGraph` & `MemorySaver`\n",
    "   - Better for production use\n",
    "   - Supports multiple users/threads\n",
    "   - More scalable\n",
    "   - Better persistence options\n",
    "\n",
    "> Let's use the second approach provided by LangGraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a96d91b",
   "metadata": {},
   "source": [
    "# Conversation History with LangGraph MemorySaver\n",
    "\n",
    "We'll go over an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a **conversation** and **remember previous interactions** with a chat model.\n",
    "\n",
    "Source: [LangChain Docs](https://python.langchain.com/docs/tutorials/chatbot/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99f0e58",
   "metadata": {},
   "source": [
    "`ChatModels` in LangChain work like special objects called \"Runnables\", which means they have a common way you can use them.\n",
    "If you just want to run the model, you give it a list of messages and use the `.invoke` method ‚Äî that‚Äôs how you ‚Äútalk‚Äù to it and get a response.\n",
    "\n",
    "It‚Äôs like: ‚ÄúPut your messages in a box, hand it to the .invoke button, and the model will answer.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54b7c3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello Saad! üëã Nice to meet you. How can I help you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke([HumanMessage(content=\"Hi! I am Saad.\")])\n",
    "\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a3c58e",
   "metadata": {},
   "source": [
    "The model on its own does not have any concept of state. For example, if you ask a followup question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d69fca8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I‚Äôm sorry, but I don‚Äôt have any information about your name. If you‚Äôd like to share it, I‚Äôll be happy to address you by that name in our conversation!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke([HumanMessage(content=\"What's my name?\")])\n",
    "\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a71975",
   "metadata": {},
   "source": [
    "We can see that it doesn't take the previous conversation turn into context, and cannot answer the question. This makes for a terrible chatbot experience!\n",
    "\n",
    "To get around this, we need to pass the entire [conversation history](https://python.langchain.com/docs/concepts/chat_history/) into the model. Let's see what happens when we do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41882f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You introduced yourself as **Saad**. üòä If there‚Äôs anything specific you‚Äôd like to talk about or any question you have, just let me know!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I am Saad.\"),\n",
    "        AIMessage(content=\"Hello Saad! üëã Nice to meet you. How can I assist you today?\"),\n",
    "        HumanMessage(content=\"Who am I?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31162d5b",
   "metadata": {},
   "source": [
    "And now we can see that we get a good response!\n",
    "\n",
    "This is the basic idea underpinning a chatbot's ability to interact conversationally. So how do we best implement this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cad5b3",
   "metadata": {},
   "source": [
    "# Message persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d52e2",
   "metadata": {},
   "source": [
    "\n",
    "LangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\n",
    "\n",
    "Wrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.\n",
    "\n",
    "LangGraph comes with a simple in-memory checkpointer, which we use below. See its [documentation](https://langchain-ai.github.io/langgraph/concepts/persistence/?_gl=1*fi5jbl*_gcl_au*MTE0NDQ3MDcyNC4xNzU0OTExODMx*_ga*MTU2Mzg1ODA3NS4xNzQ5NjMyNjk5*_ga_47WX3HKKY2*czE3NTQ5MTE4MjYkbzckZzEkdDE3NTQ5MTE4MzckajQ5JGwwJGgw) for more detail, including how to use different persistence backends (e.g., SQLite or Postgres)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfe580a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x000001E41D138440>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modern usage with LangGraph: Production-friendly way to store chat state.\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fbd2af",
   "metadata": {},
   "source": [
    "We now need to create a `config` that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a `thread_id`. This should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0653126",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4576d23f",
   "metadata": {},
   "source": [
    "This enables us to support multiple conversation threads with a single application, a common requirement when your application has multiple users.\n",
    "\n",
    "We can then invoke the application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78abe6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi! I am Saad.', additional_kwargs={}, response_metadata={}, id='246012ff-1fef-4b93-bdf2-fc730d8118c7'),\n",
       "  AIMessage(content='Hello Saad! üëã Nice to meet you. How can I help you today?', additional_kwargs={'reasoning_content': 'The user says \"Hi! I am Saad.\" Probably a greeting. We should respond friendly. No special instructions. So respond with greeting.'}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 78, 'total_tokens': 135, 'completion_time': 0.113675522, 'prompt_time': 0.004576473, 'queue_time': 0.001326876, 'total_time': 0.118251995}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_4a19b1544c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--4209ed58-3e49-403b-b0be-925e7a9972d5-0', usage_metadata={'input_tokens': 78, 'output_tokens': 57, 'total_tokens': 135})]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Hi! I am Saad.\"\n",
    "\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d49358e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Saad! üëã Nice to meet you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31b9372d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, you told me your name is **Saad**. üòä How can I assist you today, Saad?\n"
     ]
    }
   ],
   "source": [
    "query = \"Do you know my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c50f32",
   "metadata": {},
   "source": [
    "Great! Our chatbot now remembers things about us. If we change the config to reference a different `thread_id`, we can see that it starts the conversation fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed1d0894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don‚Äôt know your name unless you tell me. If you‚Äôd like to share it, feel free! Otherwise, I‚Äôm happy to help with any question you have.\n"
     ]
    }
   ],
   "source": [
    "new_config = {\"configurable\": {\"thread_id\": \"xyz890\"}}\n",
    "\n",
    "query = \"Do you know my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config=new_config)\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4d6633",
   "metadata": {},
   "source": [
    "However, we can always go back to the original conversation (since we are persisting it in a database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1598a580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, you introduced yourself as **Saad**. How can I help you today, Saad?\n"
     ]
    }
   ],
   "source": [
    "query = \"Do you know my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898c30dc",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "This is how we can support a chatbot having conversations with many users!\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you understand the basics of LangChain and LangGraph, you can:\n",
    "\n",
    "1. **Explore More Components**:\n",
    "   - Document loaders\n",
    "   - Vector stores\n",
    "   - Structured output\n",
    "   - Tools and agents\n",
    "\n",
    "2. **Learn Advanced Features**:\n",
    "   - Custom chains\n",
    "   - Advanced agents\n",
    "   - Async operations\n",
    "   - Streaming responses\n",
    "\n",
    "3. **Build Production Applications**:\n",
    "   - Use LangSmith for monitoring\n",
    "   - Deploy with LangServe\n",
    "   - Implement proper error handling\n",
    "   - Add security measures\n",
    "\n",
    "Check out the [LangChain documentation](https://python.langchain.com/docs/get_started/introduction) for more examples and tutorials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-intro-and-memory-management",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
