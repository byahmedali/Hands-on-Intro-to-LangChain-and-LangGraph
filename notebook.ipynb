{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1276c1f",
   "metadata": {},
   "source": [
    "# [Introduction to LangChain](https://python.langchain.com/docs/introduction/)\n",
    "\n",
    "LangChain is a framework for developing applications powered by large language models (LLMs).\n",
    "\n",
    "LangChain simplifies every stage of the LLM application lifecycle:\n",
    "\n",
    "- Development: Build your applications using LangChain's open-source [components](https://python.langchain.com/docs/concepts/) and [third-party integrations](https://python.langchain.com/docs/integrations/providers/). Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\n",
    "- Productionization: Use [LangSmith](https://docs.smith.langchain.com/?_gl=1*lbz7dz*_ga*MTg3MzIxNzgyNS4xNzU0ODgzNTUz*_ga_47WX3HKKY2*czE3NTQ5MDgwMDgkbzQkZzEkdDE3NTQ5MTAwMTckajYwJGwwJGgw) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\n",
    "- Deployment: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://docs.langchain.com/langgraph-platform?__hstc=5909356.bc36eceb44175eebbb2b372b7bd252b1.1754883571704.1754883571704.1754910222298.2&__hssc=5909356.2.1754910222298&__hsfp=2562008206&_gl=1*iime50*_gcl_au*NTc3MzUwMjY4LjE3NTQ5MTAyMDM.*_ga*MTg3MzIxNzgyNS4xNzU0ODgzNTUz*_ga_47WX3HKKY2*czE3NTQ5MDgwMDgkbzQkZzEkdDE3NTQ5MTAyNDkkajYwJGwwJGgw).\n",
    "\n",
    "LangChain implements a standard interface for large language models and related technologies, such as embedding models and vector stores, and integrates with hundreds of providers. See the integrations page for more.\n",
    "\n",
    "<img src=\"https://python.langchain.com/svg/langchain_stack_112024_dark.svg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77ad0c0",
   "metadata": {},
   "source": [
    "LangChain uses LLMs to build applications for various use cases. Created by Harrison Chase, it was first released as an open-source project in October 2022. To date (11-8-25), it has accumulated `113K` stars on [GitHub](https://github.com/langchain-ai/langchain) and has over `3.7k+` contributors.\n",
    "\n",
    "<img src=\"https://atsailabstorage.blob.core.windows.net/atsusecases/langchain-starhistory.png\" width=\"800\">\n",
    "Source: [LangChain Star History](https://www.star-history.com/#langchain-ai/langchain&Date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a96d91b",
   "metadata": {},
   "source": [
    "# LLM-Powered Chatbot with Conversation History\n",
    "\n",
    "We'll go over an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a **conversation** and **remember previous interactions** with a chat model.\n",
    "\n",
    "Source: [LangChain Docs](https://python.langchain.com/docs/tutorials/chatbot/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ceed41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d24499",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fd3a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"openai/gpt-oss-120b\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99f0e58",
   "metadata": {},
   "source": [
    "`ChatModels` in LangChain work like special objects called \"Runnables\", which means they have a common way you can use them.\n",
    "If you just want to run the model, you give it a list of messages and use the `.invoke` method ‚Äî that‚Äôs how you ‚Äútalk‚Äù to it and get a response.\n",
    "\n",
    "It‚Äôs like: ‚ÄúPut your messages in a box, hand it to the .invoke button, and the model will answer.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ac8f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Ahmed! üëã Nice to meet you. How can I assist you today?', additional_kwargs={'reasoning_content': 'We need to respond. The user says \"Hi! I am Ahmed.\" We should greet them, ask how we can help. Follow guidelines: be friendly, no disallowed content.'}, response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 77, 'total_tokens': 141, 'completion_time': 0.120820524, 'prompt_time': 0.004475829, 'queue_time': 0.043859801, 'total_time': 0.125296353}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_085e8f07e2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--c3abdf58-20a1-4bd1-9a64-bdf1993ed035-0', usage_metadata={'input_tokens': 77, 'output_tokens': 64, 'total_tokens': 141})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I am Ahmed.\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a3c58e",
   "metadata": {},
   "source": [
    "The model on its own does not have any concept of state. For example, if you ask a followup question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a122b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I don‚Äôt have any information about your name. Could you let me know what you‚Äôd like me to call you?', additional_kwargs={'reasoning_content': 'The user asks \"What\\'s my name?\" We have no prior conversation. The user hasn\\'t provided their name. According to policy, we cannot guess or fabricate personal info. We can ask for clarification. So we should respond asking for their name or stating we don\\'t have that info.'}, response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 75, 'total_tokens': 164, 'completion_time': 0.160853452, 'prompt_time': 0.004359095, 'queue_time': 0.050482512, 'total_time': 0.165212547}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_4a19b1544c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--03c8295e-9a28-4f76-8821-7c871b485d5a-0', usage_metadata={'input_tokens': 75, 'output_tokens': 89, 'total_tokens': 164})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a71975",
   "metadata": {},
   "source": [
    "We can see that it doesn't take the previous conversation turn into context, and cannot answer the question. This makes for a terrible chatbot experience!\n",
    "\n",
    "To get around this, we need to pass the entire [conversation history](https://python.langchain.com/docs/concepts/chat_history/) into the model. Let's see what happens when we do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41882f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You introduced yourself as **Ahmed**. If there‚Äôs anything specific you‚Äôd like to share or discuss‚Äîwhether it‚Äôs a question, a topic you‚Äôre interested in, or something you need help with‚Äîjust let me know! üòä', additional_kwargs={'reasoning_content': 'User asks \"Who am I?\" They introduced themselves as Ahmed. We can respond with a friendly answer, maybe ask for more details. Should be brief.'}, response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 108, 'total_tokens': 196, 'completion_time': 0.158821381, 'prompt_time': 0.005547022, 'queue_time': 0.001862829, 'total_time': 0.164368403}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_45f51928b5', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--9a1e0998-3784-486d-9f82-d205a284d320-0', usage_metadata={'input_tokens': 108, 'output_tokens': 88, 'total_tokens': 196})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I am Ahmed.\"),\n",
    "        AIMessage(\n",
    "            content=\"Hello Ahmed! üëã Nice to meet you. How can I assist you today?\"\n",
    "        ),\n",
    "        HumanMessage(content=\"Who am I?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31162d5b",
   "metadata": {},
   "source": [
    "And now we can see that we get a good response!\n",
    "\n",
    "This is the basic idea underpinning a chatbot's ability to interact conversationally. So how do we best implement this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cad5b3",
   "metadata": {},
   "source": [
    "# Message persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d52e2",
   "metadata": {},
   "source": [
    "LangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\n",
    "\n",
    "Wrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.\n",
    "\n",
    "LangGraph comes with a simple in-memory checkpointer, which we use below. See its [documentation](https://langchain-ai.github.io/langgraph/concepts/persistence/?_gl=1*fi5jbl*_gcl_au*MTE0NDQ3MDcyNC4xNzU0OTExODMx*_ga*MTU2Mzg1ODA3NS4xNzQ5NjMyNjk5*_ga_47WX3HKKY2*czE3NTQ5MTE4MjYkbzckZzEkdDE3NTQ5MTE4MzckajQ5JGwwJGgw) for more detail, including how to use different persistence backends (e.g., SQLite or Postgres)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfe580a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x0000013C1BBEEFD0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fbd2af",
   "metadata": {},
   "source": [
    "We now need to create a `config` that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a `thread_id`. This should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0653126",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4576d23f",
   "metadata": {},
   "source": [
    "This enables us to support multiple conversation threads with a single application, a common requirement when your application has multiple users.\n",
    "\n",
    "We can then invoke the application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78abe6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi! I am Ahmed.', additional_kwargs={}, response_metadata={}, id='84e8f04b-9fc0-495f-b3ae-12b8feaf24a3'),\n",
       "  AIMessage(content='Hello Ahmed! üëã Nice to meet you. How can I assist you today?', additional_kwargs={'reasoning_content': 'We need to respond. The user says \"Hi! I am Ahmed.\" We should greet them, ask how we can help. Follow guidelines: be friendly, no disallowed content.'}, response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 77, 'total_tokens': 141, 'completion_time': 0.115650252, 'prompt_time': 0.004500526, 'queue_time': 0.001688329, 'total_time': 0.120150778}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_ce855234b5', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--d01c1df4-c72c-455b-bc07-80a42675dd85-0', usage_metadata={'input_tokens': 77, 'output_tokens': 64, 'total_tokens': 141}),\n",
       "  HumanMessage(content='Do you know my name?', additional_kwargs={}, response_metadata={}, id='5ca4a8f4-b7b1-4d82-9dd1-6191051655bf'),\n",
       "  AIMessage(content='Yes, you introduced yourself as **Ahmed**. How can I help you today, Ahmed?', additional_kwargs={'reasoning_content': 'The user says \"Do you know my name?\" The user earlier said \"Hi! I am Ahmed.\" So we know their name is Ahmed. So answer accordingly.'}, response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 110, 'total_tokens': 172, 'completion_time': 0.112257748, 'prompt_time': 0.00560125, 'queue_time': 0.00177436, 'total_time': 0.117858998}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_82669fd91d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--e26755ae-8b7d-4b84-9515-1c8a8d2c8842-0', usage_metadata={'input_tokens': 110, 'output_tokens': 62, 'total_tokens': 172}),\n",
       "  HumanMessage(content='Do you know my name?', additional_kwargs={}, response_metadata={}, id='7427c083-6875-4d24-8440-3690ba1920ab'),\n",
       "  AIMessage(content='Yes, you told me your name is **Ahmed**. üòä Is there anything specific you‚Äôd like to talk about or need help with today?', additional_kwargs={'reasoning_content': 'The user asks \"Do you know my name?\" They previously said \"Hi! I am Ahmed.\" The assistant responded confirming the name. The user asks again \"Do you know my name?\" Probably they want confirmation. The assistant should answer yes, and ask if there\\'s anything else. There\\'s no violation. So respond.'}, response_metadata={'token_usage': {'completion_tokens': 102, 'prompt_tokens': 145, 'total_tokens': 247, 'completion_time': 0.183894645, 'prompt_time': 0.006946819, 'queue_time': 0.088112347, 'total_time': 0.190841464}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_8db49de948', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--1a3d3ce9-2ab9-47ae-b7de-a075882df138-0', usage_metadata={'input_tokens': 145, 'output_tokens': 102, 'total_tokens': 247}),\n",
       "  HumanMessage(content='Hi! I am Ahmed.', additional_kwargs={}, response_metadata={}, id='1a392a63-aa8c-40ee-b900-98e4143be387'),\n",
       "  AIMessage(content='Nice to meet you again, Ahmed! üëã What would you like to chat about or what can I help you with today?', additional_kwargs={'reasoning_content': 'The user says \"Hi! I am Ahmed.\" The assistant has already responded. The user might be testing. They may want to continue conversation. The user hasn\\'t asked a question. We should respond politely, ask how can help.'}, response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 190, 'total_tokens': 272, 'completion_time': 0.148219421, 'prompt_time': 0.008550799, 'queue_time': 0.001936619, 'total_time': 0.15677022}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_213abb2467', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--1f5e0e79-572b-4512-aa48-fdf172998144-0', usage_metadata={'input_tokens': 190, 'output_tokens': 82, 'total_tokens': 272}),\n",
       "  HumanMessage(content='Hi! I am Ahmed.', additional_kwargs={}, response_metadata={}, id='9de989b5-a17c-4e39-b9bd-8be7371954a6'),\n",
       "  AIMessage(content=\"Hey there, Ahmed! üëã Great to see you again. What‚Äôs on your mind today? Feel free to ask anything or just chat‚ÄîI'm here to help!\", additional_kwargs={'reasoning_content': 'The user repeats greeting. Possibly they want conversation. We can respond politely.'}, response_metadata={'token_usage': {'completion_tokens': 59, 'prompt_tokens': 232, 'total_tokens': 291, 'completion_time': 0.106653306, 'prompt_time': 0.009968063, 'queue_time': 0.001946907, 'total_time': 0.116621369}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_45f51928b5', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--26415d92-f87d-4ff1-a5c8-1777e7c58c1d-0', usage_metadata={'input_tokens': 232, 'output_tokens': 59, 'total_tokens': 291}),\n",
       "  HumanMessage(content='Hi! I am Ahmed.', additional_kwargs={}, response_metadata={}, id='036215b1-1bde-4247-9c59-adaef7c3d194'),\n",
       "  AIMessage(content='Hey Ahmed! üëã Great to see you again. Is there something specific you‚Äôd like to talk about or a question I can help you with today?', additional_kwargs={'reasoning_content': 'The user repeats \"Hi! I am Ahmed.\" The assistant should respond politely, maybe ask if there\\'s any question. Since they keep repeating, maybe ask if they need anything. Should not be repetitive. Provide a friendly response.'}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 282, 'total_tokens': 368, 'completion_time': 0.155637594, 'prompt_time': 0.011625076, 'queue_time': 0.00198158, 'total_time': 0.16726267}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_82669fd91d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--f1921467-ad13-4315-acde-ffc7366d3697-0', usage_metadata={'input_tokens': 282, 'output_tokens': 86, 'total_tokens': 368})]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Hi! I am Ahmed.\"\n",
    "\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d49358e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, you told me your name is **Ahmed**. üòä Is there anything specific you‚Äôd like to talk about or need help with today?\n"
     ]
    }
   ],
   "source": [
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31b9372d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, you introduced yourself as **Ahmed**. How can I help you today, Ahmed?\n"
     ]
    }
   ],
   "source": [
    "query = \"Do you know my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c50f32",
   "metadata": {},
   "source": [
    "Great! Our chatbot now remembers things about us. If we change the config to reference a different `thread_id`, we can see that it starts the conversation fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed1d0894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don‚Äôt have any information about your name unless you‚Äôve shared it with me in this conversation. If you‚Äôd like me to address you by name, just let me know what you‚Äôd like to be called!\n"
     ]
    }
   ],
   "source": [
    "new_config = {\"configurable\": {\"thread_id\": \"xyz890\"}}\n",
    "\n",
    "query = \"Do you know my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config=new_config)\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4d6633",
   "metadata": {},
   "source": [
    "However, we can always go back to the original conversation (since we are persisting it in a database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1598a580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, you told me your name is **Ahmed**. üòä Is there anything specific you‚Äôd like to talk about or need help with today?\n"
     ]
    }
   ],
   "source": [
    "query = \"Do you know my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898c30dc",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "This is how we can support a chatbot having conversations with many users!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-intro-and-memory-management",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
