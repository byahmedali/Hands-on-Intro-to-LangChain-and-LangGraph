{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1276c1f",
   "metadata": {},
   "source": [
    "# [Introduction to LangChain](https://python.langchain.com/docs/introduction/)\n",
    "\n",
    "LangChain is a framework for developing applications powered by large language models (LLMs).\n",
    "\n",
    "LangChain simplifies every stage of the LLM application lifecycle:\n",
    "\n",
    "- Development: Build your applications using LangChain's open-source [components](https://python.langchain.com/docs/concepts/) and [third-party integrations](https://python.langchain.com/docs/integrations/providers/). Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\n",
    "- Productionization: Use [LangSmith](https://docs.smith.langchain.com/?_gl=1*lbz7dz*_ga*MTg3MzIxNzgyNS4xNzU0ODgzNTUz*_ga_47WX3HKKY2*czE3NTQ5MDgwMDgkbzQkZzEkdDE3NTQ5MTAwMTckajYwJGwwJGgw) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\n",
    "- Deployment: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://docs.langchain.com/langgraph-platform?__hstc=5909356.bc36eceb44175eebbb2b372b7bd252b1.1754883571704.1754883571704.1754910222298.2&__hssc=5909356.2.1754910222298&__hsfp=2562008206&_gl=1*iime50*_gcl_au*NTc3MzUwMjY4LjE3NTQ5MTAyMDM.*_ga*MTg3MzIxNzgyNS4xNzU0ODgzNTUz*_ga_47WX3HKKY2*czE3NTQ5MDgwMDgkbzQkZzEkdDE3NTQ5MTAyNDkkajYwJGwwJGgw).\n",
    "\n",
    "LangChain implements a standard interface for large language models and related technologies, such as embedding models and vector stores, and integrates with hundreds of providers. See the integrations page for more.\n",
    "\n",
    "<img src=\"https://python.langchain.com/svg/langchain_stack_112024_dark.svg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77ad0c0",
   "metadata": {},
   "source": [
    "LangChain uses LLMs to build applications for various use cases. Created by Harrison Chase, it was first released as an open-source project in October 2022. To date (11-8-25), it has accumulated `113K` stars on [GitHub](https://github.com/langchain-ai/langchain) and has over `3.7k+` contributors.\n",
    "\n",
    "<img src=\"https://atsailabstorage.blob.core.windows.net/atsusecases/langchain-starhistory.png\" width=\"800\">\n",
    "Source: [LangChain Star History](https://www.star-history.com/#langchain-ai/langchain&Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ceed41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25d24499",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0c5cc-b3c4-4ccc-b2f1-df018237dbea",
   "metadata": {},
   "source": [
    "## Chat Models\n",
    "https://js.langchain.com/docs/integrations/chat/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ec89812-d456-4351-8eed-3d0b5614594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"openai/gpt-oss-120b\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58a8d153-0991-4dd5-a59f-c9c63b185178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2\n",
    "from langchain.chat_models import init_chat_model\n",
    "LLM = init_chat_model(model=\"groq:openai/gpt-oss-120b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da5bfff9-472c-4453-ac46-4e8df6f2e552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I help you today?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = LLM.invoke(\"Hi!\")\n",
    "r.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4c1ce-8afd-497d-8a5a-91573678b9c5",
   "metadata": {},
   "source": [
    "## Prompts Templates\n",
    "A prompt is the text we give to the LLM (or ChatModel). <br>\n",
    "LangChain provides special classes like PromptTemplate and ChatPromptTemplate to make prompts dynamic and reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8330e5a6-7622-467c-9183-4edb332e194b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Explain Large Language Language in simple Term'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single Input Variable\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template= \"Explain {topic} in simple Term\"\n",
    ")\n",
    "\n",
    "prompt = template.format(topic = \"Large Language Language\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dd359ba-73f1-4093-94d9-5e874fd71f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Large Language Model (LLM) ‚Äì Simple Explanation**\\n\\nImagine you have a huge library of books, articles, chats, and all kinds of written text. A **large language model** is a computer program that has read (or been trained on) that massive collection of words and sentences. By ‚Äúreading‚Äù all that text, the model learns how language works‚Äîhow words fit together, what ideas usually follow each other, and how to answer questions or write new text that sounds natural.\\n\\n### How It Works (in everyday language)\\n\\n| Step | What Happens | Simple Analogy |\\n|------|-------------|--------------|\\n| **1. Learning from lots of text** | The model reads billions of words. | Like a child listening to many conversations and stories. |\\n| **2. Spotting patterns** | It notices which words often appear together and how sentences are built. | Like noticing that ‚Äúpeanut butter‚Äù often goes with ‚Äújelly‚Äù. |\\n| **3. Building a ‚Äúbrain‚Äù** | It creates a huge network of tiny math units (called *parameters*) that store those patterns. | Like building a huge map of all the word connections you‚Äôve ever heard. |\\n| **4. Generating text** | When you ask it something, it uses the map to predict the next word, then the next, and so on, creating a full answer. | Like playing a game of ‚Äúwhat comes next?‚Äù with a very well‚Äëread friend. |\\n\\n### Key Points in Plain Language\\n\\n- **‚ÄúLarge‚Äù**: The model has *a lot* of parameters (often billions) and has been trained on *a lot* of text. The more data and parameters, the better it can capture subtle language nuances.\\n- **‚ÄúLanguage‚Äù**: It deals with human language‚ÄîEnglish, Spanish, Chinese, etc.‚Äîand learns the rules, slang, facts, and even some reasoning.\\n- **‚ÄúModel‚Äù**: This is the mathematical ‚Äúbrain‚Äù that stores what it learned and can be used to answer questions, write stories, translate languages, summarize text, and more.\\n\\n### Everyday Example\\n\\nThink of a **large language model** like a super‚Äësmart autocomplete on your phone, but instead of just finishing a single sentence, it can:\\n\\n- Write a whole paragraph for you.\\n- Explain a concept (like this answer).\\n- Translate a sentence from one language to another.\\n- Answer questions about history, science, or your own notes.\\n\\nAll of this happens because the model has ‚Äúread‚Äù a massive amount of text and learned the patterns that make language make sense.\\n\\n### Bottom Line\\n\\nA **large language model** is a computer program that has learned how humans write and speak by studying huge amounts of text. It uses that knowledge to generate new, human‚Äëlike text when you ask it something. Think of it as a very well‚Äëread, very fast, and always‚Äëready writing assistant.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa80eb04-99a0-478e-bdfd-155cbadb87d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: You are helpfull assistent\\nHuman: Summarize the concept of Large Language Model in 3 bullet points.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ChatPromptTemplate \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpfull assistent\"),\n",
    "    (\"user\", \"Summarize the concept of {concept} in 3 bullet points.\")\n",
    "])\n",
    "\n",
    "message = template.format(concept = \"Large Language Model\")\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43492224-24ed-49a0-8ffe-8d1b0736f783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- **Statistical pattern learners:** Large Language Models (LLMs) are neural networks trained on massive text corpora to predict the next word (or token) given its context, learning statistical patterns of language rather than explicit rules.\\n\\n- **Scale and capability:** By scaling up the number of parameters (often billions) and the amount of training data, LLMs acquire broad knowledge and can perform diverse tasks‚Äîtranslation, summarization, reasoning‚Äîthrough simple prompting.\\n\\n- **Probabilistic generation:** LLMs output a probability distribution over possible next tokens, allowing them to generate coherent, context‚Äëaware text, but they lack true understanding or consciousness; their outputs reflect learned patterns and may contain errors or biases.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(message)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eb8bcc-e967-43db-b0ba-7836d98592e7",
   "metadata": {},
   "source": [
    "- SystemMessage -- for content which should be passed to direct the conversation\n",
    "- HumanMessage -- for content in the input from the user.\n",
    "- AIMessage -- for content in the response from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "991ceee5-3faf-4f08-9d1e-94638ef97bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a wise and slightly humorous AI mentor.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Can you explain LangChain in one sentence?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='LangChain is a Python framework that helps you build apps powered by large language models.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Great! Can you also give me one real-world use case?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Message Types\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import AIMessage, SystemMessage, HumanMessage\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a wise and slightly humorous AI mentor.\"),\n",
    "    HumanMessage(content=\"Can you explain LangChain in one sentence?\"),\n",
    "    AIMessage(content=\"LangChain is a Python framework that helps you build apps powered by large language models.\"),\n",
    "    HumanMessage(content=\"Great! Can you also give me one real-world use case?\")\n",
    "])\n",
    "\n",
    "message = template.format_messages()\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7fece46-fce1-47c0-b125-dabe5a710286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Use‚Äëcase example:**\\u202fA SaaS company builds a **customer‚Äësupport chatbot** with LangChain that pulls the latest product documentation, ticket history, and FAQ articles into a single LLM‚Äëdriven assistant‚Äîso when a user asks ‚ÄúHow do I reset my API key?‚Äù the bot can instantly retrieve the relevant policy, walk the user through the steps, and even log the interaction for future analytics, all without writing a custom retrieval‚Äëaugmented‚Äëgeneration pipeline from scratch.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(message).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8a5ce-580f-4171-9868-d5d399d5d6aa",
   "metadata": {},
   "source": [
    "## Chain in LangChain\n",
    "A Chain is basically a pipeline that connects multiple steps together, where the output of one step becomes the input of the next.\n",
    "https://python.langchain.com/api_reference/langchain/chains.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524a8d6d-066f-45be-8574-1c8a77126c42",
   "metadata": {},
   "source": [
    "### 1. Single-Step Chain (LLMChain)\n",
    "This is the simplest form ‚Äî Prompt ‚Üí LLM ‚Üí Output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a4e1da99-5657-4ada-bd87-84706a271ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Quantum Computing',\n",
       " 'text': '**Quantum Computing in a nutshell**\\n\\n1. **Bits vs. Qubits**  \\n   - *Classical computers* use **bits** that are either **0** or **1**.  \\n   - *Quantum computers* use **qubits** that can be **0, 1, or both at the same time** (thanks to a property called **super‚Äëposition**).\\n\\n2. **Super‚Äëposition**  \\n   Imagine a spinning coin. While it‚Äôs spinning, you can‚Äôt say it‚Äôs heads *or* tails‚Äîit‚Äôs effectively both until you look. A qubit works similarly: it holds many possibilities at once, letting a quantum computer explore many solutions in parallel.\\n\\n3. **Entanglement**  \\n   When two qubits become **entangled**, the state of one instantly influences the other, no matter how far apart they are. This creates a kind of ‚Äúinstant‚Äëcommunication‚Äù link that lets quantum computers coordinate many calculations at once.\\n\\n4. **Why it matters**  \\n   Because a quantum computer can process many possibilities simultaneously, it can solve certain problems (like factoring huge numbers, simulating molecules, or optimizing complex systems) far faster than any classical computer can.\\n\\n5. **A simple analogy**  \\n   - **Classical computer:** A single‚Äëlane road where cars (bits) can only go forward one at a time.  \\n   - **Quantum computer:** A multi‚Äëlane highway where each car can be in many lanes at once, and some cars are magically linked (entangled) so they move together in perfect sync. This lets the ‚Äútraffic‚Äù solve a puzzle much quicker.\\n\\n---\\n\\n### üéâ Fun Fact\\n\\nThe term **‚Äúquantum‚Äù** comes from the Latin word *quantum* meaning ‚Äúhow much.‚Äù In the 1900s, physicists discovered that energy isn‚Äôt continuous‚Äîit comes in tiny packets called **quanta**. That same idea‚Äîtiny, discrete units‚Äîunderlies the qubits that power quantum computers!'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain {topic} in simple terms and add one fun fact.\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=model, prompt=prompt)\n",
    "\n",
    "result = chain.invoke(\"Quantum Computing\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6810e60b-ef38-4aab-81c9-6d2ef7d0957d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**What is cloud computing?**  \\nThink of the cloud as a giant, always‚Äëavailable ‚Äúdigital library‚Äù that lives on the internet instead of on your own computer or phone. Instead of storing files, running programs, or doing calculations on a single device, you ‚Äúborrow‚Äù the power and storage of big, remote computers (called servers) that are kept in special data‚Äëcenter buildings. You can access your data and apps from any device that‚Äôs connected to the internet‚Äîjust like you can read a book from a library no matter where you are, as long as you have a library card.\\n\\n**Simple analogy:**  \\nImagine you‚Äôre a chef who wants to bake a cake. Instead of buying a huge oven, all the ingredients, and a kitchen for every single cake you make, you go to a shared kitchen (the cloud). You pay only for the time you use the oven, the space you need, and the tools you need. When you‚Äôre done, you leave the kitchen clean and ready for the next chef. The cloud works the same way for software, storage, and even heavy‚Äëduty computing tasks.\\n\\n**Key points in plain language**\\n\\n| What you want to do | Where it happens in the cloud | Why it‚Äôs handy |\\n|--------------------|----------------------------|--------------|\\n| **Store photos, videos, documents** | On remote servers (the ‚Äúcloud‚Äù) | Access them from any phone, tablet, or computer‚Äîno need to carry a hard drive |\\n| **Run an app (like email, video chat, or games)** | The app runs on powerful computers far away | Your device can be small and cheap, yet still run powerful software |\\n| **Do heavy calculations (e.g., AI, big data)** | Big data centers with lots of processors | You get super‚Äëfast performance without buying expensive hardware |\\n\\n**One fun fact:**  \\nThe term ‚Äúcloud‚Äù comes from the little cloud symbol you see in diagrams of computer networks. It started as a simple way to show ‚Äúthe internet‚Äù in a picture, but the name stuck‚Äîso now we literally call it ‚Äúthe cloud‚Äù! üå•Ô∏è'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Runnable chain\n",
    "# Use pipe | operator to make a chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "res = chain.invoke(\"Cloud Computing\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820f6c6a-b33f-45b4-9228-62210cad1a3e",
   "metadata": {},
   "source": [
    "### 2. Multi-Step Chain (SimpleSequentialChain)\n",
    "This combines two LLM steps, first create an outline, then expand it into a blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2021750-3e0c-433a-b156-1fb573d6b4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'The Future of Renewable Energy', 'output': '**The Future of Renewable Energy**\\n\\nThe renewable‚Äëenergy landscape is undergoing a rapid transformation, driven by breakthroughs that were once science‚Äëfiction. **Advanced solar solutions** such as perovskite cells promise efficiencies above 30\\u202f%, while bifacial panels harvest light from both sides, boosting output without extra land. Floating solar farms now dot reservoirs and coastal lagoons, turning water surfaces into power generators. In the wind sector, **offshore floating turbines** can be anchored in deep water where winds are strongest, and high‚Äëaltitude kite or airborne wind systems capture jet‚Äëstream energy that traditional turbines can‚Äôt reach. Meanwhile, **solid‚Äëstate batteries**, hydrogen‚Äëbased storage, and AI‚Äëdriven grid optimization promise to smooth the intermittent nature of sun and wind, turning intermittent power into reliable, dispatchable energy.\\n\\nEconomic and policy forces are accelerating this shift. Carbon‚Äëpricing mechanisms, renewable‚Äëportfolio standards, and global climate accords create a regulatory backbone that rewards clean power. The Levelized Cost of Energy (LCOE) for solar and wind has fallen by more than 80\\u202f% in a decade, and economies of scale in manufacturing keep driving prices down. Green bonds, ESG‚Äëfocused capital, and public‚Äëprivate partnerships are unlocking trillions of dollars in financing, making large‚Äëscale projects financially viable.\\n\\nBeyond technology, the transition reshapes society. Decentralized micro‚Äëgrids and off‚Äëgrid solutions bring electricity to remote communities, while new jobs in manufacturing, installation, and maintenance fuel a green economy. Yet challenges remain: grid stability, storage scalability, and coordination across transport, industry, and housing must be addressed. By 2030‚Äë2050, renewables are projected to supply over 70\\u202f% of global electricity, a milestone that hinges on consumer choices, corporate commitments, and continued policy support. The future is bright, and it‚Äôs already here.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "outline_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Create a 3-point outline for a blog post about {topic}.\"\n",
    ")\n",
    "\n",
    "outline_chain = LLMChain(llm=model, prompt=outline_prompt)\n",
    "\n",
    "# Step 2: Content generator\n",
    "content_prompt = PromptTemplate(\n",
    "    input_variables=[\"outline\"],\n",
    "    template=\"Expand this outline into a detailed 200-word blog post:\\n{outline}\"\n",
    ")\n",
    "content_chain = LLMChain(llm=model, prompt=content_prompt)\n",
    "\n",
    "# Combine into one chain\n",
    "blog_chain = SimpleSequentialChain(chains=[outline_chain, content_chain])\n",
    "\n",
    "# Run the full pipeline\n",
    "final_result = blog_chain.invoke(\"The Future of Renewable Energy\")\n",
    "print(final_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b90e2e7-5991-4ddf-9752-e268c99d28fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**The Future of Renewable Energy ‚Äì A Quick‚ÄëTake Overview**\n",
      "\n",
      "---\n",
      "\n",
      "### 1.‚ÄØKey Technological Trends\n",
      "\n",
      "| **Sector** | **Breakthroughs** | **Why It Matters** |\n",
      "|-----------|------------------|------------------|\n",
      "| **Solar** | ‚Ä¢ **Perovskite cells** ‚Äì >‚ÄØ30‚ÄØ% lab efficiencies, low‚Äëcost roll‚Äëto‚Äëroll production.<br>‚Ä¢ **Bifacial panels** ‚Äì capture light on both sides ‚Üí‚ÄØ+‚ÄØ20‚Äë30‚ÄØ% output.<br>‚Ä¢ **Solar‚Äëglass & building‚Äëintegrated PV** ‚Äì windows, fa√ßades become power generators. | Drives higher energy yield per‚Äëm¬≤ and turns existing surfaces into generators. |\n",
      "| **Wind** | ‚Ä¢ **Floating offshore turbines** ‚Äì access high‚Äëspeed, stable winds far from coast.<br>‚Ä¢ **Bladeless (vortex‚Äëinduced) turbines** ‚Äì low noise, minimal maintenance.<br>‚Ä¢ **AI‚Äëoptimized farm layouts** ‚Äì maximize kW per hectare, cut CAPEX. | Expands viable sites (deep water, constrained land) and reduces O&M costs. |\n",
      "| **Storage** | ‚Ä¢ **Solid‚Äëstate batteries** ‚Äì higher energy density, safer chemistry.<br>‚Ä¢ **Power‚Äëto‚Äëhydrogen** ‚Äì converts surplus electricity into transport‚Äëready fuel.<br>‚Ä¢ **Grid‚Äëscale thermal storage** (molten salt, concrete) ‚Äì multi‚Äëday heat storage to smooth intermittency. | Provides the ‚Äúmissing link‚Äù for long‚Äëduration, high‚Äëcapacity storage needed for 100‚ÄØ% renewables. |\n",
      "\n",
      "---\n",
      "\n",
      "### 2.‚ÄØEconomic & Policy Drivers\n",
      "\n",
      "| **Driver** | **Impact** |\n",
      "|-----------|-----------|\n",
      "| **LCOE drop** ‚Äì Solar & wind now cheaper than new coal in many markets. | Makes clean power the default economic choice. |\n",
      "| **Carbon pricing & RPS** ‚Äì Clear price signals for low‚Äëcarbon generation. | Encourages investment, reduces financing risk. |\n",
      "| **Green‚Äëbond & ESG financing** | Channels capital directly into clean‚Äëtech pipelines. |\n",
      "| **Corporate 100‚ÄØ% renewable procurement** | Fuels growth of micro‚Äëgrids, distributed generation, and demand‚Äëside flexibility. |\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Societal Impact & Roadmap\n",
      "\n",
      "| **Impact** | **Details** |\n",
      "|-----------|------------|\n",
      "| **Energy access** | Off‚Äëgrid solar + storage can power remote villages at <‚ÄØ$0.10/kWh, closing the energy‚Äëpoverty gap. |\n",
      "| **Jobs** | >‚ÄØ10‚ÄØM jobs globally (installation, manufacturing, O&M) with strong up‚Äëskilling pathways for former fossil‚Äëfuel workers. |\n",
      "| **Decarbonization targets** | **2030:** 30‚ÄØ% renewable electricity.<br>**2040:** 60‚ÄØ%.<br>**2050:** Fully decarbonized grid. |\n",
      "| **Key enablers** | ‚Ä¢ **Policy coordination** (national targets, grid codes).<br>‚Ä¢ **Public‚Äëprivate partnerships** (R&D, financing, deployment).<br>‚Ä¢ **Research institutions** (tech scaling, standards). |\n",
      "\n",
      "---\n",
      "\n",
      "## What‚Äôs Needed to Hit the 2050 Goal\n",
      "\n",
      "1. **Scale‚Äëup manufacturing** for perovskite and solid‚Äëstate batteries to drive down costs and secure supply chains (e.g., tin, lead‚Äëfree perovskites, lithium‚Äëfree solid electrolytes).  \n",
      "2. **Expand grid‚Äëinterconnection** for offshore wind and offshore hydrogen hubs‚Äîlinking offshore generation to inland demand centers.  \n",
      "3. **Deploy multi‚Äëday storage** (thermal, hydrogen, flow batteries) at the 100‚ÄëGW‚Äëhour scale to cover seasonal variability.  \n",
      "4. **Policy certainty**‚Äîlong‚Äëterm renewable‚Äëportfolio standards, stable carbon‚Äëprice trajectories, and streamlined permitting for offshore and floating installations.  \n",
      "5. **Workforce transition**‚Äîtargeted reskilling programs, apprenticeship pipelines, and community‚Äëbased job creation in underserved regions.  \n",
      "\n",
      "---\n",
      "\n",
      "### Bottom Line\n",
      "\n",
      "The convergence of **high‚Äëefficiency solar**, **next‚Äëgen wind**, and **robust, low‚Äëcost storage** is turning clean power from a niche option into the **default** energy source. With continued cost declines, supportive policy frameworks, and a focus on equitable job creation, the 30‚Äë60‚Äë100‚ÄØ% renewable pathway to 2050 is technically feasible and economically attractive. The next decade will be decisive: the faster we align technology, finance, and policy, the sooner the world can enjoy a resilient, affordable, and climate‚Äësafe power grid.\n"
     ]
    }
   ],
   "source": [
    "# Runnable chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "outline_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Create a 3-point outline for a blog post about {topic}.\"\n",
    ")\n",
    "\n",
    "outline_chain = outline_prompt | model | StrOutputParser()\n",
    "\n",
    "content_prompt = PromptTemplate(\n",
    "    input_variables=[\"outline\"],\n",
    "    template=\"Expand this outline into a detailed 200-word blog post:\\n{outline}\"\n",
    ")\n",
    "content_chain = content_prompt | model | StrOutputParser()\n",
    "\n",
    "blog_chain = outline_chain | content_chain | model | StrOutputParser()\n",
    "\n",
    "# Run the full pipeline\n",
    "final_result = blog_chain.invoke({\"The Future of Renewable Energy\"})\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b4470-1070-4a2c-a355-11fe4b2c15d2",
   "metadata": {},
   "source": [
    "##  Memory in LangChain\n",
    "By default, LangChain calls are stateless, the model only sees the current input and forgets everything else.\n",
    "https://python.langchain.com/api_reference/langchain/memory.html <br>\n",
    "https://python.langchain.com/docs/versions/migrating_memory/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d06d50ef-9650-40dc-9d4d-d60a5bb0bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"\n",
    "    You are a friendly chatbot.\n",
    "    User: {question}\n",
    "    Assistant:\n",
    "    \"\"\")\n",
    "\n",
    "# Create chain\n",
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74913969-11d3-4fd1-b41c-36515be6fb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Saad! üëã Great to meet you. How‚Äôs your day going so far?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\"Hi! I am Saad\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0a00152-7309-4578-bf80-9fe13cf09b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I‚Äôm afraid I don‚Äôt have your name yet‚Äîcould you remind me? üòä Once you tell me, I‚Äôll keep it in mind for the rest of our chat!'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\"Can you remeber my name?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed31d4cc-9b3c-475c-8b76-6579f22e60c8",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory\n",
    "- Stores: Full conversation history.\n",
    "- Use: Small chats where cost & token limits aren‚Äôt an issue.\n",
    "- Pro: Simple and easy.\n",
    "- Con: Can grow too large for long sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e96f423-9db1-42cf-9949-41ed49bf8ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create Memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"question\"],\n",
    "    template=\"\"\"\n",
    "    You are an AI Assistant for **Advanced Telecom Services (ATS)**.\n",
    "    Conversation so far:\n",
    "    {chat_history}\n",
    "    User: {question}\n",
    "    Assistant:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create chain\n",
    "chain = LLMChain(llm=model, prompt=prompt, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d55600ff-ee09-43ce-8b11-d26ce02e7c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Saad! üëã  \\n\\nI‚Äôm here to make your experience with **Advanced Telecom Services (ATS)** as smooth and enjoyable as possible. Whether you‚Äôre looking for:\\n\\n- **Mobile or internet plan recommendations**  \\n- **Bill details or payment assistance**  \\n- **Troubleshooting a service issue** (e.g., no signal, slow internet, billing errors)  \\n- **Information on new offers, devices, or upgrades**  \\n\\njust let me know what you need, and I‚Äôll guide you through it.  \\n\\nWhat would you like help with today? (Feel free to be as specific as you like‚Äîe.g., ‚ÄúI want a new data‚Äëplan for my family,‚Äù or ‚ÄúMy internet is down, what should I do?‚Äù)'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.run(\"Hi!, My name is Saad, How can you help me?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33b03e99-e30a-4b17-aa5c-59ef14b36b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is **Saad**. How can I assist you further today?'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.run(\"Can you remeber, What's my name?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a96d91b",
   "metadata": {},
   "source": [
    "# LLM-Powered Chatbot with Conversation History\n",
    "\n",
    "We'll go over an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a **conversation** and **remember previous interactions** with a chat model.\n",
    "\n",
    "Source: [LangChain Docs](https://python.langchain.com/docs/tutorials/chatbot/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fd3a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"openai/gpt-oss-120b\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99f0e58",
   "metadata": {},
   "source": [
    "`ChatModels` in LangChain work like special objects called \"Runnables\", which means they have a common way you can use them.\n",
    "If you just want to run the model, you give it a list of messages and use the `.invoke` method ‚Äî that‚Äôs how you ‚Äútalk‚Äù to it and get a response.\n",
    "\n",
    "It‚Äôs like: ‚ÄúPut your messages in a box, hand it to the .invoke button, and the model will answer.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ac8f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Ahmed! üëã Nice to meet you. How can I assist you today?', additional_kwargs={'reasoning_content': 'We need to respond. The user says \"Hi! I am Ahmed.\" We should greet them, ask how we can help. Follow guidelines: be friendly, no disallowed content.'}, response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 77, 'total_tokens': 141, 'completion_time': 0.120820524, 'prompt_time': 0.004475829, 'queue_time': 0.043859801, 'total_time': 0.125296353}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_085e8f07e2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--c3abdf58-20a1-4bd1-9a64-bdf1993ed035-0', usage_metadata={'input_tokens': 77, 'output_tokens': 64, 'total_tokens': 141})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I am Ahmed.\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a3c58e",
   "metadata": {},
   "source": [
    "The model on its own does not have any concept of state. For example, if you ask a followup question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a122b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I don‚Äôt have any information about your name. Could you let me know what you‚Äôd like me to call you?', additional_kwargs={'reasoning_content': 'The user asks \"What\\'s my name?\" We have no prior conversation. The user hasn\\'t provided their name. According to policy, we cannot guess or fabricate personal info. We can ask for clarification. So we should respond asking for their name or stating we don\\'t have that info.'}, response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 75, 'total_tokens': 164, 'completion_time': 0.160853452, 'prompt_time': 0.004359095, 'queue_time': 0.050482512, 'total_time': 0.165212547}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_4a19b1544c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--03c8295e-9a28-4f76-8821-7c871b485d5a-0', usage_metadata={'input_tokens': 75, 'output_tokens': 89, 'total_tokens': 164})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a71975",
   "metadata": {},
   "source": [
    "We can see that it doesn't take the previous conversation turn into context, and cannot answer the question. This makes for a terrible chatbot experience!\n",
    "\n",
    "To get around this, we need to pass the entire [conversation history](https://python.langchain.com/docs/concepts/chat_history/) into the model. Let's see what happens when we do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41882f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You introduced yourself as **Ahmed**. If there‚Äôs anything specific you‚Äôd like to share or discuss‚Äîwhether it‚Äôs a question, a topic you‚Äôre interested in, or something you need help with‚Äîjust let me know! üòä', additional_kwargs={'reasoning_content': 'User asks \"Who am I?\" They introduced themselves as Ahmed. We can respond with a friendly answer, maybe ask for more details. Should be brief.'}, response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 108, 'total_tokens': 196, 'completion_time': 0.158821381, 'prompt_time': 0.005547022, 'queue_time': 0.001862829, 'total_time': 0.164368403}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_45f51928b5', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--9a1e0998-3784-486d-9f82-d205a284d320-0', usage_metadata={'input_tokens': 108, 'output_tokens': 88, 'total_tokens': 196})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I am Ahmed.\"),\n",
    "        AIMessage(\n",
    "            content=\"Hello Ahmed! üëã Nice to meet you. How can I assist you today?\"\n",
    "        ),\n",
    "        HumanMessage(content=\"Who am I?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31162d5b",
   "metadata": {},
   "source": [
    "And now we can see that we get a good response!\n",
    "\n",
    "This is the basic idea underpinning a chatbot's ability to interact conversationally. So how do we best implement this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cad5b3",
   "metadata": {},
   "source": [
    "# Message persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d52e2",
   "metadata": {},
   "source": [
    "LangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\n",
    "\n",
    "Wrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.\n",
    "\n",
    "LangGraph comes with a simple in-memory checkpointer, which we use below. See its [documentation](https://langchain-ai.github.io/langgraph/concepts/persistence/?_gl=1*fi5jbl*_gcl_au*MTE0NDQ3MDcyNC4xNzU0OTExODMx*_ga*MTU2Mzg1ODA3NS4xNzQ5NjMyNjk5*_ga_47WX3HKKY2*czE3NTQ5MTE4MjYkbzckZzEkdDE3NTQ5MTE4MzckajQ5JGwwJGgw) for more detail, including how to use different persistence backends (e.g., SQLite or Postgres)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfe580a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x0000013C1BBEEFD0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modern usage with LangGraph: Production-friendly way to store chat state.\n",
    "from langgraph.checkpoint.memory import MemorySaver \n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fbd2af",
   "metadata": {},
   "source": [
    "We now need to create a `config` that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a `thread_id`. This should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0653126",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4576d23f",
   "metadata": {},
   "source": [
    "This enables us to support multiple conversation threads with a single application, a common requirement when your application has multiple users.\n",
    "\n",
    "We can then invoke the application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78abe6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi! I am Ahmed.', additional_kwargs={}, response_metadata={}, id='84e8f04b-9fc0-495f-b3ae-12b8feaf24a3'),\n",
       "  AIMessage(content='Hello Ahmed! üëã Nice to meet you. How can I assist you today?', additional_kwargs={'reasoning_content': 'We need to respond. The user says \"Hi! I am Ahmed.\" We should greet them, ask how we can help. Follow guidelines: be friendly, no disallowed content.'}, response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 77, 'total_tokens': 141, 'completion_time': 0.115650252, 'prompt_time': 0.004500526, 'queue_time': 0.001688329, 'total_time': 0.120150778}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_ce855234b5', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--d01c1df4-c72c-455b-bc07-80a42675dd85-0', usage_metadata={'input_tokens': 77, 'output_tokens': 64, 'total_tokens': 141}),\n",
       "  HumanMessage(content='Do you know my name?', additional_kwargs={}, response_metadata={}, id='5ca4a8f4-b7b1-4d82-9dd1-6191051655bf'),\n",
       "  AIMessage(content='Yes, you introduced yourself as **Ahmed**. How can I help you today, Ahmed?', additional_kwargs={'reasoning_content': 'The user says \"Do you know my name?\" The user earlier said \"Hi! I am Ahmed.\" So we know their name is Ahmed. So answer accordingly.'}, response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 110, 'total_tokens': 172, 'completion_time': 0.112257748, 'prompt_time': 0.00560125, 'queue_time': 0.00177436, 'total_time': 0.117858998}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_82669fd91d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--e26755ae-8b7d-4b84-9515-1c8a8d2c8842-0', usage_metadata={'input_tokens': 110, 'output_tokens': 62, 'total_tokens': 172}),\n",
       "  HumanMessage(content='Do you know my name?', additional_kwargs={}, response_metadata={}, id='7427c083-6875-4d24-8440-3690ba1920ab'),\n",
       "  AIMessage(content='Yes, you told me your name is **Ahmed**. üòä Is there anything specific you‚Äôd like to talk about or need help with today?', additional_kwargs={'reasoning_content': 'The user asks \"Do you know my name?\" They previously said \"Hi! I am Ahmed.\" The assistant responded confirming the name. The user asks again \"Do you know my name?\" Probably they want confirmation. The assistant should answer yes, and ask if there\\'s anything else. There\\'s no violation. So respond.'}, response_metadata={'token_usage': {'completion_tokens': 102, 'prompt_tokens': 145, 'total_tokens': 247, 'completion_time': 0.183894645, 'prompt_time': 0.006946819, 'queue_time': 0.088112347, 'total_time': 0.190841464}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_8db49de948', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--1a3d3ce9-2ab9-47ae-b7de-a075882df138-0', usage_metadata={'input_tokens': 145, 'output_tokens': 102, 'total_tokens': 247}),\n",
       "  HumanMessage(content='Hi! I am Ahmed.', additional_kwargs={}, response_metadata={}, id='1a392a63-aa8c-40ee-b900-98e4143be387'),\n",
       "  AIMessage(content='Nice to meet you again, Ahmed! üëã What would you like to chat about or what can I help you with today?', additional_kwargs={'reasoning_content': 'The user says \"Hi! I am Ahmed.\" The assistant has already responded. The user might be testing. They may want to continue conversation. The user hasn\\'t asked a question. We should respond politely, ask how can help.'}, response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 190, 'total_tokens': 272, 'completion_time': 0.148219421, 'prompt_time': 0.008550799, 'queue_time': 0.001936619, 'total_time': 0.15677022}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_213abb2467', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--1f5e0e79-572b-4512-aa48-fdf172998144-0', usage_metadata={'input_tokens': 190, 'output_tokens': 82, 'total_tokens': 272}),\n",
       "  HumanMessage(content='Hi! I am Ahmed.', additional_kwargs={}, response_metadata={}, id='9de989b5-a17c-4e39-b9bd-8be7371954a6'),\n",
       "  AIMessage(content=\"Hey there, Ahmed! üëã Great to see you again. What‚Äôs on your mind today? Feel free to ask anything or just chat‚ÄîI'm here to help!\", additional_kwargs={'reasoning_content': 'The user repeats greeting. Possibly they want conversation. We can respond politely.'}, response_metadata={'token_usage': {'completion_tokens': 59, 'prompt_tokens': 232, 'total_tokens': 291, 'completion_time': 0.106653306, 'prompt_time': 0.009968063, 'queue_time': 0.001946907, 'total_time': 0.116621369}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_45f51928b5', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--26415d92-f87d-4ff1-a5c8-1777e7c58c1d-0', usage_metadata={'input_tokens': 232, 'output_tokens': 59, 'total_tokens': 291}),\n",
       "  HumanMessage(content='Hi! I am Ahmed.', additional_kwargs={}, response_metadata={}, id='036215b1-1bde-4247-9c59-adaef7c3d194'),\n",
       "  AIMessage(content='Hey Ahmed! üëã Great to see you again. Is there something specific you‚Äôd like to talk about or a question I can help you with today?', additional_kwargs={'reasoning_content': 'The user repeats \"Hi! I am Ahmed.\" The assistant should respond politely, maybe ask if there\\'s any question. Since they keep repeating, maybe ask if they need anything. Should not be repetitive. Provide a friendly response.'}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 282, 'total_tokens': 368, 'completion_time': 0.155637594, 'prompt_time': 0.011625076, 'queue_time': 0.00198158, 'total_time': 0.16726267}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_82669fd91d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--f1921467-ad13-4315-acde-ffc7366d3697-0', usage_metadata={'input_tokens': 282, 'output_tokens': 86, 'total_tokens': 368})]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Hi! I am Ahmed.\"\n",
    "\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d49358e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, you told me your name is **Ahmed**. üòä Is there anything specific you‚Äôd like to talk about or need help with today?\n"
     ]
    }
   ],
   "source": [
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31b9372d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, you introduced yourself as **Ahmed**. How can I help you today, Ahmed?\n"
     ]
    }
   ],
   "source": [
    "query = \"Do you know my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c50f32",
   "metadata": {},
   "source": [
    "Great! Our chatbot now remembers things about us. If we change the config to reference a different `thread_id`, we can see that it starts the conversation fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed1d0894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don‚Äôt have any information about your name unless you‚Äôve shared it with me in this conversation. If you‚Äôd like me to address you by name, just let me know what you‚Äôd like to be called!\n"
     ]
    }
   ],
   "source": [
    "new_config = {\"configurable\": {\"thread_id\": \"xyz890\"}}\n",
    "\n",
    "query = \"Do you know my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config=new_config)\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4d6633",
   "metadata": {},
   "source": [
    "However, we can always go back to the original conversation (since we are persisting it in a database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1598a580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, you told me your name is **Ahmed**. üòä Is there anything specific you‚Äôd like to talk about or need help with today?\n"
     ]
    }
   ],
   "source": [
    "query = \"Do you know my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898c30dc",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "This is how we can support a chatbot having conversations with many users!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
