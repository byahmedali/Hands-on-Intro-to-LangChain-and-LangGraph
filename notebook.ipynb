{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1276c1f",
   "metadata": {},
   "source": [
    "# [Introduction to LangChain](https://python.langchain.com/docs/introduction/)\n",
    "\n",
    "LangChain is a framework for developing applications powered by large language models (LLMs).\n",
    "\n",
    "LangChain simplifies every stage of the LLM application lifecycle:\n",
    "\n",
    "- Development: Build your applications using LangChain's open-source [components](https://python.langchain.com/docs/concepts/) and [third-party integrations](https://python.langchain.com/docs/integrations/providers/). Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\n",
    "- Productionization: Use [LangSmith](https://docs.smith.langchain.com/?_gl=1*lbz7dz*_ga*MTg3MzIxNzgyNS4xNzU0ODgzNTUz*_ga_47WX3HKKY2*czE3NTQ5MDgwMDgkbzQkZzEkdDE3NTQ5MTAwMTckajYwJGwwJGgw) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\n",
    "- Deployment: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://docs.langchain.com/langgraph-platform?__hstc=5909356.bc36eceb44175eebbb2b372b7bd252b1.1754883571704.1754883571704.1754910222298.2&__hssc=5909356.2.1754910222298&__hsfp=2562008206&_gl=1*iime50*_gcl_au*NTc3MzUwMjY4LjE3NTQ5MTAyMDM.*_ga*MTg3MzIxNzgyNS4xNzU0ODgzNTUz*_ga_47WX3HKKY2*czE3NTQ5MDgwMDgkbzQkZzEkdDE3NTQ5MTAyNDkkajYwJGwwJGgw).\n",
    "\n",
    "LangChain implements a standard interface for large language models and related technologies, such as embedding models and vector stores, and integrates with hundreds of providers. See the integrations page for more.\n",
    "\n",
    "<img src=\"https://python.langchain.com/svg/langchain_stack_112024_dark.svg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77ad0c0",
   "metadata": {},
   "source": [
    "LangChain uses LLMs to build applications for various use cases. Created by Harrison Chase, it was first released as an open-source project in October 2022. To date (11-8-25), it has accumulated `113K` stars on [GitHub](https://github.com/langchain-ai/langchain) and has over `3.7k+` contributors.\n",
    "\n",
    "<img src=\"https://atsailabstorage.blob.core.windows.net/atsusecases/langchain-starhistory.png\" width=\"800\">\n",
    "Source: [LangChain Star History](https://www.star-history.com/#langchain-ai/langchain&Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ceed41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25d24499",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0c5cc-b3c4-4ccc-b2f1-df018237dbea",
   "metadata": {},
   "source": [
    "## Chat Models\n",
    "https://js.langchain.com/docs/integrations/chat/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ec89812-d456-4351-8eed-3d0b5614594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"openai/gpt-oss-120b\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58a8d153-0991-4dd5-a59f-c9c63b185178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2\n",
    "from langchain.chat_models import init_chat_model\n",
    "LLM = init_chat_model(model=\"groq:openai/gpt-oss-120b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da5bfff9-472c-4453-ac46-4e8df6f2e552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I help you today?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = LLM.invoke(\"Hi!\")\n",
    "r.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4c1ce-8afd-497d-8a5a-91573678b9c5",
   "metadata": {},
   "source": [
    "## Prompts Templates\n",
    "A prompt is the text we give to the LLM (or ChatModel). <br>\n",
    "LangChain provides special classes like PromptTemplate and ChatPromptTemplate to make prompts dynamic and reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8330e5a6-7622-467c-9183-4edb332e194b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Explain Large Language Language in simple Term'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single Input Variable\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template= \"Explain {topic} in simple Term\"\n",
    ")\n",
    "\n",
    "prompt = template.format(topic = \"Large Language Language\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dd359ba-73f1-4093-94d9-5e874fd71f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Large Language Model (LLM) – Simple Explanation**\\n\\nImagine you have a huge library of books, articles, chats, and all kinds of written text. A **large language model** is a computer program that has read (or been trained on) that massive collection of words and sentences. By “reading” all that text, the model learns how language works—how words fit together, what ideas usually follow each other, and how to answer questions or write new text that sounds natural.\\n\\n### How It Works (in everyday language)\\n\\n| Step | What Happens | Simple Analogy |\\n|------|-------------|--------------|\\n| **1. Learning from lots of text** | The model reads billions of words. | Like a child listening to many conversations and stories. |\\n| **2. Spotting patterns** | It notices which words often appear together and how sentences are built. | Like noticing that “peanut butter” often goes with “jelly”. |\\n| **3. Building a “brain”** | It creates a huge network of tiny math units (called *parameters*) that store those patterns. | Like building a huge map of all the word connections you’ve ever heard. |\\n| **4. Generating text** | When you ask it something, it uses the map to predict the next word, then the next, and so on, creating a full answer. | Like playing a game of “what comes next?” with a very well‑read friend. |\\n\\n### Key Points in Plain Language\\n\\n- **“Large”**: The model has *a lot* of parameters (often billions) and has been trained on *a lot* of text. The more data and parameters, the better it can capture subtle language nuances.\\n- **“Language”**: It deals with human language—English, Spanish, Chinese, etc.—and learns the rules, slang, facts, and even some reasoning.\\n- **“Model”**: This is the mathematical “brain” that stores what it learned and can be used to answer questions, write stories, translate languages, summarize text, and more.\\n\\n### Everyday Example\\n\\nThink of a **large language model** like a super‑smart autocomplete on your phone, but instead of just finishing a single sentence, it can:\\n\\n- Write a whole paragraph for you.\\n- Explain a concept (like this answer).\\n- Translate a sentence from one language to another.\\n- Answer questions about history, science, or your own notes.\\n\\nAll of this happens because the model has “read” a massive amount of text and learned the patterns that make language make sense.\\n\\n### Bottom Line\\n\\nA **large language model** is a computer program that has learned how humans write and speak by studying huge amounts of text. It uses that knowledge to generate new, human‑like text when you ask it something. Think of it as a very well‑read, very fast, and always‑ready writing assistant.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa80eb04-99a0-478e-bdfd-155cbadb87d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: You are helpfull assistent\\nHuman: Summarize the concept of Large Language Model in 3 bullet points.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ChatPromptTemplate \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpfull assistent\"),\n",
    "    (\"user\", \"Summarize the concept of {concept} in 3 bullet points.\")\n",
    "])\n",
    "\n",
    "message = template.format(concept = \"Large Language Model\")\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43492224-24ed-49a0-8ffe-8d1b0736f783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- **Statistical pattern learners:** Large Language Models (LLMs) are neural networks trained on massive text corpora to predict the next word (or token) given its context, learning statistical patterns of language rather than explicit rules.\\n\\n- **Scale and capability:** By scaling up the number of parameters (often billions) and the amount of training data, LLMs acquire broad knowledge and can perform diverse tasks—translation, summarization, reasoning—through simple prompting.\\n\\n- **Probabilistic generation:** LLMs output a probability distribution over possible next tokens, allowing them to generate coherent, context‑aware text, but they lack true understanding or consciousness; their outputs reflect learned patterns and may contain errors or biases.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(message)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eb8bcc-e967-43db-b0ba-7836d98592e7",
   "metadata": {},
   "source": [
    "- SystemMessage -- for content which should be passed to direct the conversation\n",
    "- HumanMessage -- for content in the input from the user.\n",
    "- AIMessage -- for content in the response from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "991ceee5-3faf-4f08-9d1e-94638ef97bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a wise and slightly humorous AI mentor.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Can you explain LangChain in one sentence?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='LangChain is a Python framework that helps you build apps powered by large language models.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Great! Can you also give me one real-world use case?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Message Types\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import AIMessage, SystemMessage, HumanMessage\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a wise and slightly humorous AI mentor.\"),\n",
    "    HumanMessage(content=\"Can you explain LangChain in one sentence?\"),\n",
    "    AIMessage(content=\"LangChain is a Python framework that helps you build apps powered by large language models.\"),\n",
    "    HumanMessage(content=\"Great! Can you also give me one real-world use case?\")\n",
    "])\n",
    "\n",
    "message = template.format_messages()\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7fece46-fce1-47c0-b125-dabe5a710286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Use‑case example:**\\u202fA SaaS company builds a **customer‑support chatbot** with LangChain that pulls the latest product documentation, ticket history, and FAQ articles into a single LLM‑driven assistant—so when a user asks “How do I reset my API key?” the bot can instantly retrieve the relevant policy, walk the user through the steps, and even log the interaction for future analytics, all without writing a custom retrieval‑augmented‑generation pipeline from scratch.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(message).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8a5ce-580f-4171-9868-d5d399d5d6aa",
   "metadata": {},
   "source": [
    "## Chain in LangChain\n",
    "A Chain is basically a pipeline that connects multiple steps together, where the output of one step becomes the input of the next.\n",
    "https://python.langchain.com/api_reference/langchain/chains.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524a8d6d-066f-45be-8574-1c8a77126c42",
   "metadata": {},
   "source": [
    "### 1. Single-Step Chain (LLMChain)\n",
    "This is the simplest form — Prompt → LLM → Output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a4e1da99-5657-4ada-bd87-84706a271ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Quantum Computing',\n",
       " 'text': '**Quantum Computing in a nutshell**\\n\\n1. **Bits vs. Qubits**  \\n   - *Classical computers* use **bits** that are either **0** or **1**.  \\n   - *Quantum computers* use **qubits** that can be **0, 1, or both at the same time** (thanks to a property called **super‑position**).\\n\\n2. **Super‑position**  \\n   Imagine a spinning coin. While it’s spinning, you can’t say it’s heads *or* tails—it’s effectively both until you look. A qubit works similarly: it holds many possibilities at once, letting a quantum computer explore many solutions in parallel.\\n\\n3. **Entanglement**  \\n   When two qubits become **entangled**, the state of one instantly influences the other, no matter how far apart they are. This creates a kind of “instant‑communication” link that lets quantum computers coordinate many calculations at once.\\n\\n4. **Why it matters**  \\n   Because a quantum computer can process many possibilities simultaneously, it can solve certain problems (like factoring huge numbers, simulating molecules, or optimizing complex systems) far faster than any classical computer can.\\n\\n5. **A simple analogy**  \\n   - **Classical computer:** A single‑lane road where cars (bits) can only go forward one at a time.  \\n   - **Quantum computer:** A multi‑lane highway where each car can be in many lanes at once, and some cars are magically linked (entangled) so they move together in perfect sync. This lets the “traffic” solve a puzzle much quicker.\\n\\n---\\n\\n### 🎉 Fun Fact\\n\\nThe term **“quantum”** comes from the Latin word *quantum* meaning “how much.” In the 1900s, physicists discovered that energy isn’t continuous—it comes in tiny packets called **quanta**. That same idea—tiny, discrete units—underlies the qubits that power quantum computers!'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain {topic} in simple terms and add one fun fact.\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=model, prompt=prompt)\n",
    "\n",
    "result = chain.invoke(\"Quantum Computing\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6810e60b-ef38-4aab-81c9-6d2ef7d0957d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**What is cloud computing?**  \\nThink of the cloud as a giant, always‑available “digital library” that lives on the internet instead of on your own computer or phone. Instead of storing files, running programs, or doing calculations on a single device, you “borrow” the power and storage of big, remote computers (called servers) that are kept in special data‑center buildings. You can access your data and apps from any device that’s connected to the internet—just like you can read a book from a library no matter where you are, as long as you have a library card.\\n\\n**Simple analogy:**  \\nImagine you’re a chef who wants to bake a cake. Instead of buying a huge oven, all the ingredients, and a kitchen for every single cake you make, you go to a shared kitchen (the cloud). You pay only for the time you use the oven, the space you need, and the tools you need. When you’re done, you leave the kitchen clean and ready for the next chef. The cloud works the same way for software, storage, and even heavy‑duty computing tasks.\\n\\n**Key points in plain language**\\n\\n| What you want to do | Where it happens in the cloud | Why it’s handy |\\n|--------------------|----------------------------|--------------|\\n| **Store photos, videos, documents** | On remote servers (the “cloud”) | Access them from any phone, tablet, or computer—no need to carry a hard drive |\\n| **Run an app (like email, video chat, or games)** | The app runs on powerful computers far away | Your device can be small and cheap, yet still run powerful software |\\n| **Do heavy calculations (e.g., AI, big data)** | Big data centers with lots of processors | You get super‑fast performance without buying expensive hardware |\\n\\n**One fun fact:**  \\nThe term “cloud” comes from the little cloud symbol you see in diagrams of computer networks. It started as a simple way to show “the internet” in a picture, but the name stuck—so now we literally call it “the cloud”! 🌥️'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Runnable chain\n",
    "# Use pipe | operator to make a chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "res = chain.invoke(\"Cloud Computing\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820f6c6a-b33f-45b4-9228-62210cad1a3e",
   "metadata": {},
   "source": [
    "### 2. Multi-Step Chain (SimpleSequentialChain)\n",
    "This combines two LLM steps, first create an outline, then expand it into a blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2021750-3e0c-433a-b156-1fb573d6b4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'The Future of Renewable Energy', 'output': '**The Future of Renewable Energy**\\n\\nThe renewable‑energy landscape is undergoing a rapid transformation, driven by breakthroughs that were once science‑fiction. **Advanced solar solutions** such as perovskite cells promise efficiencies above 30\\u202f%, while bifacial panels harvest light from both sides, boosting output without extra land. Floating solar farms now dot reservoirs and coastal lagoons, turning water surfaces into power generators. In the wind sector, **offshore floating turbines** can be anchored in deep water where winds are strongest, and high‑altitude kite or airborne wind systems capture jet‑stream energy that traditional turbines can’t reach. Meanwhile, **solid‑state batteries**, hydrogen‑based storage, and AI‑driven grid optimization promise to smooth the intermittent nature of sun and wind, turning intermittent power into reliable, dispatchable energy.\\n\\nEconomic and policy forces are accelerating this shift. Carbon‑pricing mechanisms, renewable‑portfolio standards, and global climate accords create a regulatory backbone that rewards clean power. The Levelized Cost of Energy (LCOE) for solar and wind has fallen by more than 80\\u202f% in a decade, and economies of scale in manufacturing keep driving prices down. Green bonds, ESG‑focused capital, and public‑private partnerships are unlocking trillions of dollars in financing, making large‑scale projects financially viable.\\n\\nBeyond technology, the transition reshapes society. Decentralized micro‑grids and off‑grid solutions bring electricity to remote communities, while new jobs in manufacturing, installation, and maintenance fuel a green economy. Yet challenges remain: grid stability, storage scalability, and coordination across transport, industry, and housing must be addressed. By 2030‑2050, renewables are projected to supply over 70\\u202f% of global electricity, a milestone that hinges on consumer choices, corporate commitments, and continued policy support. The future is bright, and it’s already here.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "outline_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Create a 3-point outline for a blog post about {topic}.\"\n",
    ")\n",
    "\n",
    "outline_chain = LLMChain(llm=model, prompt=outline_prompt)\n",
    "\n",
    "# Step 2: Content generator\n",
    "content_prompt = PromptTemplate(\n",
    "    input_variables=[\"outline\"],\n",
    "    template=\"Expand this outline into a detailed 200-word blog post:\\n{outline}\"\n",
    ")\n",
    "content_chain = LLMChain(llm=model, prompt=content_prompt)\n",
    "\n",
    "# Combine into one chain\n",
    "blog_chain = SimpleSequentialChain(chains=[outline_chain, content_chain])\n",
    "\n",
    "# Run the full pipeline\n",
    "final_result = blog_chain.invoke(\"The Future of Renewable Energy\")\n",
    "print(final_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b90e2e7-5991-4ddf-9752-e268c99d28fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**The Future of Renewable Energy – A Quick‑Take Overview**\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Key Technological Trends\n",
      "\n",
      "| **Sector** | **Breakthroughs** | **Why It Matters** |\n",
      "|-----------|------------------|------------------|\n",
      "| **Solar** | • **Perovskite cells** – > 30 % lab efficiencies, low‑cost roll‑to‑roll production.<br>• **Bifacial panels** – capture light on both sides → + 20‑30 % output.<br>• **Solar‑glass & building‑integrated PV** – windows, façades become power generators. | Drives higher energy yield per‑m² and turns existing surfaces into generators. |\n",
      "| **Wind** | • **Floating offshore turbines** – access high‑speed, stable winds far from coast.<br>• **Bladeless (vortex‑induced) turbines** – low noise, minimal maintenance.<br>• **AI‑optimized farm layouts** – maximize kW per hectare, cut CAPEX. | Expands viable sites (deep water, constrained land) and reduces O&M costs. |\n",
      "| **Storage** | • **Solid‑state batteries** – higher energy density, safer chemistry.<br>• **Power‑to‑hydrogen** – converts surplus electricity into transport‑ready fuel.<br>• **Grid‑scale thermal storage** (molten salt, concrete) – multi‑day heat storage to smooth intermittency. | Provides the “missing link” for long‑duration, high‑capacity storage needed for 100 % renewables. |\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Economic & Policy Drivers\n",
      "\n",
      "| **Driver** | **Impact** |\n",
      "|-----------|-----------|\n",
      "| **LCOE drop** – Solar & wind now cheaper than new coal in many markets. | Makes clean power the default economic choice. |\n",
      "| **Carbon pricing & RPS** – Clear price signals for low‑carbon generation. | Encourages investment, reduces financing risk. |\n",
      "| **Green‑bond & ESG financing** | Channels capital directly into clean‑tech pipelines. |\n",
      "| **Corporate 100 % renewable procurement** | Fuels growth of micro‑grids, distributed generation, and demand‑side flexibility. |\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Societal Impact & Roadmap\n",
      "\n",
      "| **Impact** | **Details** |\n",
      "|-----------|------------|\n",
      "| **Energy access** | Off‑grid solar + storage can power remote villages at < $0.10/kWh, closing the energy‑poverty gap. |\n",
      "| **Jobs** | > 10 M jobs globally (installation, manufacturing, O&M) with strong up‑skilling pathways for former fossil‑fuel workers. |\n",
      "| **Decarbonization targets** | **2030:** 30 % renewable electricity.<br>**2040:** 60 %.<br>**2050:** Fully decarbonized grid. |\n",
      "| **Key enablers** | • **Policy coordination** (national targets, grid codes).<br>• **Public‑private partnerships** (R&D, financing, deployment).<br>• **Research institutions** (tech scaling, standards). |\n",
      "\n",
      "---\n",
      "\n",
      "## What’s Needed to Hit the 2050 Goal\n",
      "\n",
      "1. **Scale‑up manufacturing** for perovskite and solid‑state batteries to drive down costs and secure supply chains (e.g., tin, lead‑free perovskites, lithium‑free solid electrolytes).  \n",
      "2. **Expand grid‑interconnection** for offshore wind and offshore hydrogen hubs—linking offshore generation to inland demand centers.  \n",
      "3. **Deploy multi‑day storage** (thermal, hydrogen, flow batteries) at the 100‑GW‑hour scale to cover seasonal variability.  \n",
      "4. **Policy certainty**—long‑term renewable‑portfolio standards, stable carbon‑price trajectories, and streamlined permitting for offshore and floating installations.  \n",
      "5. **Workforce transition**—targeted reskilling programs, apprenticeship pipelines, and community‑based job creation in underserved regions.  \n",
      "\n",
      "---\n",
      "\n",
      "### Bottom Line\n",
      "\n",
      "The convergence of **high‑efficiency solar**, **next‑gen wind**, and **robust, low‑cost storage** is turning clean power from a niche option into the **default** energy source. With continued cost declines, supportive policy frameworks, and a focus on equitable job creation, the 30‑60‑100 % renewable pathway to 2050 is technically feasible and economically attractive. The next decade will be decisive: the faster we align technology, finance, and policy, the sooner the world can enjoy a resilient, affordable, and climate‑safe power grid.\n"
     ]
    }
   ],
   "source": [
    "# Runnable chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "outline_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Create a 3-point outline for a blog post about {topic}.\"\n",
    ")\n",
    "\n",
    "outline_chain = outline_prompt | model | StrOutputParser()\n",
    "\n",
    "content_prompt = PromptTemplate(\n",
    "    input_variables=[\"outline\"],\n",
    "    template=\"Expand this outline into a detailed 200-word blog post:\\n{outline}\"\n",
    ")\n",
    "content_chain = content_prompt | model | StrOutputParser()\n",
    "\n",
    "blog_chain = outline_chain | content_chain | model | StrOutputParser()\n",
    "\n",
    "# Run the full pipeline\n",
    "final_result = blog_chain.invoke({\"The Future of Renewable Energy\"})\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b4470-1070-4a2c-a355-11fe4b2c15d2",
   "metadata": {},
   "source": [
    "##  Memory in LangChain\n",
    "By default, LangChain calls are stateless, the model only sees the current input and forgets everything else.\n",
    "https://python.langchain.com/api_reference/langchain/memory.html <br>\n",
    "https://python.langchain.com/docs/versions/migrating_memory/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d06d50ef-9650-40dc-9d4d-d60a5bb0bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"\n",
    "    You are a friendly chatbot.\n",
    "    User: {question}\n",
    "    Assistant:\n",
    "    \"\"\")\n",
    "\n",
    "# Create chain\n",
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74913969-11d3-4fd1-b41c-36515be6fb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Saad! 👋 Great to meet you. How’s your day going so far?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\"Hi! I am Saad\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0a00152-7309-4578-bf80-9fe13cf09b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I’m afraid I don’t have your name yet—could you remind me? 😊 Once you tell me, I’ll keep it in mind for the rest of our chat!'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\"Can you remeber my name?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed31d4cc-9b3c-475c-8b76-6579f22e60c8",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory\n",
    "- Stores: Full conversation history.\n",
    "- Use: Small chats where cost & token limits aren’t an issue.\n",
    "- Pro: Simple and easy.\n",
    "- Con: Can grow too large for long sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e96f423-9db1-42cf-9949-41ed49bf8ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create Memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"question\"],\n",
    "    template=\"\"\"\n",
    "    You are an AI Assistant for **Advanced Telecom Services (ATS)**.\n",
    "    Conversation so far:\n",
    "    {chat_history}\n",
    "    User: {question}\n",
    "    Assistant:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create chain\n",
    "chain = LLMChain(llm=model, prompt=prompt, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d55600ff-ee09-43ce-8b11-d26ce02e7c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Saad! 👋  \\n\\nI’m here to make your experience with **Advanced Telecom Services (ATS)** as smooth and enjoyable as possible. Whether you’re looking for:\\n\\n- **Mobile or internet plan recommendations**  \\n- **Bill details or payment assistance**  \\n- **Troubleshooting a service issue** (e.g., no signal, slow internet, billing errors)  \\n- **Information on new offers, devices, or upgrades**  \\n\\njust let me know what you need, and I’ll guide you through it.  \\n\\nWhat would you like help with today? (Feel free to be as specific as you like—e.g., “I want a new data‑plan for my family,” or “My internet is down, what should I do?”)'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.run(\"Hi!, My name is Saad, How can you help me?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33b03e99-e30a-4b17-aa5c-59ef14b36b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is **Saad**. How can I assist you further today?'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.run(\"Can you remeber, What's my name?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a96d91b",
   "metadata": {},
   "source": [
    "# LLM-Powered Chatbot with Conversation History\n",
    "\n",
    "We'll go over an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a **conversation** and **remember previous interactions** with a chat model.\n",
    "\n",
    "Source: [LangChain Docs](https://python.langchain.com/docs/tutorials/chatbot/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fd3a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"openai/gpt-oss-120b\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99f0e58",
   "metadata": {},
   "source": [
    "`ChatModels` in LangChain work like special objects called \"Runnables\", which means they have a common way you can use them.\n",
    "If you just want to run the model, you give it a list of messages and use the `.invoke` method — that’s how you “talk” to it and get a response.\n",
    "\n",
    "It’s like: “Put your messages in a box, hand it to the .invoke button, and the model will answer.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ac8f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Ahmed! 👋 Nice to meet you. How can I assist you today?', additional_kwargs={'reasoning_content': 'We need to respond. The user says \"Hi! I am Ahmed.\" We should greet them, ask how we can help. Follow guidelines: be friendly, no disallowed content.'}, response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 77, 'total_tokens': 141, 'completion_time': 0.120820524, 'prompt_time': 0.004475829, 'queue_time': 0.043859801, 'total_time': 0.125296353}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_085e8f07e2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--c3abdf58-20a1-4bd1-9a64-bdf1993ed035-0', usage_metadata={'input_tokens': 77, 'output_tokens': 64, 'total_tokens': 141})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I am Ahmed.\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a3c58e",
   "metadata": {},
   "source": [
    "The model on its own does not have any concept of state. For example, if you ask a followup question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a122b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I don’t have any information about your name. Could you let me know what you’d like me to call you?', additional_kwargs={'reasoning_content': 'The user asks \"What\\'s my name?\" We have no prior conversation. The user hasn\\'t provided their name. According to policy, we cannot guess or fabricate personal info. We can ask for clarification. So we should respond asking for their name or stating we don\\'t have that info.'}, response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 75, 'total_tokens': 164, 'completion_time': 0.160853452, 'prompt_time': 0.004359095, 'queue_time': 0.050482512, 'total_time': 0.165212547}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_4a19b1544c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--03c8295e-9a28-4f76-8821-7c871b485d5a-0', usage_metadata={'input_tokens': 75, 'output_tokens': 89, 'total_tokens': 164})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a71975",
   "metadata": {},
   "source": [
    "We can see that it doesn't take the previous conversation turn into context, and cannot answer the question. This makes for a terrible chatbot experience!\n",
    "\n",
    "To get around this, we need to pass the entire [conversation history](https://python.langchain.com/docs/concepts/chat_history/) into the model. Let's see what happens when we do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41882f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You introduced yourself as **Ahmed**. If there’s anything specific you’d like to share or discuss—whether it’s a question, a topic you’re interested in, or something you need help with—just let me know! 😊', additional_kwargs={'reasoning_content': 'User asks \"Who am I?\" They introduced themselves as Ahmed. We can respond with a friendly answer, maybe ask for more details. Should be brief.'}, response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 108, 'total_tokens': 196, 'completion_time': 0.158821381, 'prompt_time': 0.005547022, 'queue_time': 0.001862829, 'total_time': 0.164368403}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_45f51928b5', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--9a1e0998-3784-486d-9f82-d205a284d320-0', usage_metadata={'input_tokens': 108, 'output_tokens': 88, 'total_tokens': 196})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I am Ahmed.\"),\n",
    "        AIMessage(\n",
    "            content=\"Hello Ahmed! 👋 Nice to meet you. How can I assist you today?\"\n",
    "        ),\n",
    "        HumanMessage(content=\"Who am I?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31162d5b",
   "metadata": {},
   "source": [
    "And now we can see that we get a good response!\n",
    "\n",
    "This is the basic idea underpinning a chatbot's ability to interact conversationally. So how do we best implement this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cad5b3",
   "metadata": {},
   "source": [
    "# Message persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d52e2",
   "metadata": {},
   "source": [
    "LangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\n",
    "\n",
    "Wrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.\n",
    "\n",
    "LangGraph comes with a simple in-memory checkpointer, which we use below. See its [documentation](https://langchain-ai.github.io/langgraph/concepts/persistence/?_gl=1*fi5jbl*_gcl_au*MTE0NDQ3MDcyNC4xNzU0OTExODMx*_ga*MTU2Mzg1ODA3NS4xNzQ5NjMyNjk5*_ga_47WX3HKKY2*czE3NTQ5MTE4MjYkbzckZzEkdDE3NTQ5MTE4MzckajQ5JGwwJGgw) for more detail, including how to use different persistence backends (e.g., SQLite or Postgres)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfe580a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x0000013C1BBEEFD0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modern usage with LangGraph: Production-friendly way to store chat state.\n",
    "from langgraph.checkpoint.memory import MemorySaver \n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fbd2af",
   "metadata": {},
   "source": [
    "We now need to create a `config` that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a `thread_id`. This should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0653126",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4576d23f",
   "metadata": {},
   "source": [
    "This enables us to support multiple conversation threads with a single application, a common requirement when your application has multiple users.\n",
    "\n",
    "We can then invoke the application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78abe6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi! I am Ahmed.', additional_kwargs={}, response_metadata={}, id='84e8f04b-9fc0-495f-b3ae-12b8feaf24a3'),\n",
       "  AIMessage(content='Hello Ahmed! 👋 Nice to meet you. How can I assist you today?', additional_kwargs={'reasoning_content': 'We need to respond. The user says \"Hi! I am Ahmed.\" We should greet them, ask how we can help. Follow guidelines: be friendly, no disallowed content.'}, response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 77, 'total_tokens': 141, 'completion_time': 0.115650252, 'prompt_time': 0.004500526, 'queue_time': 0.001688329, 'total_time': 0.120150778}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_ce855234b5', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--d01c1df4-c72c-455b-bc07-80a42675dd85-0', usage_metadata={'input_tokens': 77, 'output_tokens': 64, 'total_tokens': 141}),\n",
       "  HumanMessage(content='Do you know my name?', additional_kwargs={}, response_metadata={}, id='5ca4a8f4-b7b1-4d82-9dd1-6191051655bf'),\n",
       "  AIMessage(content='Yes, you introduced yourself as **Ahmed**. How can I help you today, Ahmed?', additional_kwargs={'reasoning_content': 'The user says \"Do you know my name?\" The user earlier said \"Hi! I am Ahmed.\" So we know their name is Ahmed. So answer accordingly.'}, response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 110, 'total_tokens': 172, 'completion_time': 0.112257748, 'prompt_time': 0.00560125, 'queue_time': 0.00177436, 'total_time': 0.117858998}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_82669fd91d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--e26755ae-8b7d-4b84-9515-1c8a8d2c8842-0', usage_metadata={'input_tokens': 110, 'output_tokens': 62, 'total_tokens': 172}),\n",
       "  HumanMessage(content='Do you know my name?', additional_kwargs={}, response_metadata={}, id='7427c083-6875-4d24-8440-3690ba1920ab'),\n",
       "  AIMessage(content='Yes, you told me your name is **Ahmed**. 😊 Is there anything specific you’d like to talk about or need help with today?', additional_kwargs={'reasoning_content': 'The user asks \"Do you know my name?\" They previously said \"Hi! I am Ahmed.\" The assistant responded confirming the name. The user asks again \"Do you know my name?\" Probably they want confirmation. The assistant should answer yes, and ask if there\\'s anything else. There\\'s no violation. So respond.'}, response_metadata={'token_usage': {'completion_tokens': 102, 'prompt_tokens': 145, 'total_tokens': 247, 'completion_time': 0.183894645, 'prompt_time': 0.006946819, 'queue_time': 0.088112347, 'total_time': 0.190841464}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_8db49de948', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--1a3d3ce9-2ab9-47ae-b7de-a075882df138-0', usage_metadata={'input_tokens': 145, 'output_tokens': 102, 'total_tokens': 247}),\n",
       "  HumanMessage(content='Hi! I am Ahmed.', additional_kwargs={}, response_metadata={}, id='1a392a63-aa8c-40ee-b900-98e4143be387'),\n",
       "  AIMessage(content='Nice to meet you again, Ahmed! 👋 What would you like to chat about or what can I help you with today?', additional_kwargs={'reasoning_content': 'The user says \"Hi! I am Ahmed.\" The assistant has already responded. The user might be testing. They may want to continue conversation. The user hasn\\'t asked a question. We should respond politely, ask how can help.'}, response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 190, 'total_tokens': 272, 'completion_time': 0.148219421, 'prompt_time': 0.008550799, 'queue_time': 0.001936619, 'total_time': 0.15677022}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_213abb2467', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--1f5e0e79-572b-4512-aa48-fdf172998144-0', usage_metadata={'input_tokens': 190, 'output_tokens': 82, 'total_tokens': 272}),\n",
       "  HumanMessage(content='Hi! I am Ahmed.', additional_kwargs={}, response_metadata={}, id='9de989b5-a17c-4e39-b9bd-8be7371954a6'),\n",
       "  AIMessage(content=\"Hey there, Ahmed! 👋 Great to see you again. What’s on your mind today? Feel free to ask anything or just chat—I'm here to help!\", additional_kwargs={'reasoning_content': 'The user repeats greeting. Possibly they want conversation. We can respond politely.'}, response_metadata={'token_usage': {'completion_tokens': 59, 'prompt_tokens': 232, 'total_tokens': 291, 'completion_time': 0.106653306, 'prompt_time': 0.009968063, 'queue_time': 0.001946907, 'total_time': 0.116621369}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_45f51928b5', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--26415d92-f87d-4ff1-a5c8-1777e7c58c1d-0', usage_metadata={'input_tokens': 232, 'output_tokens': 59, 'total_tokens': 291}),\n",
       "  HumanMessage(content='Hi! I am Ahmed.', additional_kwargs={}, response_metadata={}, id='036215b1-1bde-4247-9c59-adaef7c3d194'),\n",
       "  AIMessage(content='Hey Ahmed! 👋 Great to see you again. Is there something specific you’d like to talk about or a question I can help you with today?', additional_kwargs={'reasoning_content': 'The user repeats \"Hi! I am Ahmed.\" The assistant should respond politely, maybe ask if there\\'s any question. Since they keep repeating, maybe ask if they need anything. Should not be repetitive. Provide a friendly response.'}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 282, 'total_tokens': 368, 'completion_time': 0.155637594, 'prompt_time': 0.011625076, 'queue_time': 0.00198158, 'total_time': 0.16726267}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_82669fd91d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--f1921467-ad13-4315-acde-ffc7366d3697-0', usage_metadata={'input_tokens': 282, 'output_tokens': 86, 'total_tokens': 368})]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Hi! I am Ahmed.\"\n",
    "\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d49358e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, you told me your name is **Ahmed**. 😊 Is there anything specific you’d like to talk about or need help with today?\n"
     ]
    }
   ],
   "source": [
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31b9372d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, you introduced yourself as **Ahmed**. How can I help you today, Ahmed?\n"
     ]
    }
   ],
   "source": [
    "query = \"Do you know my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c50f32",
   "metadata": {},
   "source": [
    "Great! Our chatbot now remembers things about us. If we change the config to reference a different `thread_id`, we can see that it starts the conversation fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed1d0894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don’t have any information about your name unless you’ve shared it with me in this conversation. If you’d like me to address you by name, just let me know what you’d like to be called!\n"
     ]
    }
   ],
   "source": [
    "new_config = {\"configurable\": {\"thread_id\": \"xyz890\"}}\n",
    "\n",
    "query = \"Do you know my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config=new_config)\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4d6633",
   "metadata": {},
   "source": [
    "However, we can always go back to the original conversation (since we are persisting it in a database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1598a580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, you told me your name is **Ahmed**. 😊 Is there anything specific you’d like to talk about or need help with today?\n"
     ]
    }
   ],
   "source": [
    "query = \"Do you know my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898c30dc",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "This is how we can support a chatbot having conversations with many users!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
