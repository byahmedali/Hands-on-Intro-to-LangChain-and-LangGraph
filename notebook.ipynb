{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1276c1f",
   "metadata": {},
   "source": [
    "# [Introduction to LangChain](https://python.langchain.com/docs/introduction/)\n",
    "\n",
    "LangChain is a framework for developing applications powered by large language models (LLMs).\n",
    "\n",
    "LangChain simplifies every stage of the LLM application lifecycle:\n",
    "\n",
    "- Development: Build your applications using LangChain's open-source [components](https://python.langchain.com/docs/concepts/) and [third-party integrations](https://python.langchain.com/docs/integrations/providers/). Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\n",
    "- Productionization: Use [LangSmith](https://docs.smith.langchain.com/?_gl=1*lbz7dz*_ga*MTg3MzIxNzgyNS4xNzU0ODgzNTUz*_ga_47WX3HKKY2*czE3NTQ5MDgwMDgkbzQkZzEkdDE3NTQ5MTAwMTckajYwJGwwJGgw) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\n",
    "- Deployment: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://docs.langchain.com/langgraph-platform?__hstc=5909356.bc36eceb44175eebbb2b372b7bd252b1.1754883571704.1754883571704.1754910222298.2&__hssc=5909356.2.1754910222298&__hsfp=2562008206&_gl=1*iime50*_gcl_au*NTc3MzUwMjY4LjE3NTQ5MTAyMDM.*_ga*MTg3MzIxNzgyNS4xNzU0ODgzNTUz*_ga_47WX3HKKY2*czE3NTQ5MDgwMDgkbzQkZzEkdDE3NTQ5MTAyNDkkajYwJGwwJGgw).\n",
    "\n",
    "LangChain implements a standard interface for large language models and related technologies, such as embedding models and vector stores, and integrates with hundreds of providers. See the integrations page for more.\n",
    "\n",
    "<img src=\"https://python.langchain.com/svg/langchain_stack_112024_dark.svg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77ad0c0",
   "metadata": {},
   "source": [
    "LangChain uses LLMs to build applications for various use cases. Created by Harrison Chase, it was first released as an open-source project in October 2022. To date (11-8-25), it has accumulated `113K` stars on [GitHub](https://github.com/langchain-ai/langchain) and has over `3.7k+` contributors.\n",
    "\n",
    "<img src=\"https://atsailabstorage.blob.core.windows.net/atsusecases/langchain-starhistory.png\" width=\"800\">\n",
    "Source: [LangChain Star History](https://www.star-history.com/#langchain-ai/langchain&Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ceed41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import HTML, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25d24499",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0c5cc-b3c4-4ccc-b2f1-df018237dbea",
   "metadata": {},
   "source": [
    "## Chat Models\n",
    "https://js.langchain.com/docs/integrations/chat/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ec89812-d456-4351-8eed-3d0b5614594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"openai/gpt-oss-120b\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50dc2ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I help you today?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(\"hi\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58a8d153-0991-4dd5-a59f-c9c63b185178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "LLM = init_chat_model(model=\"groq:openai/gpt-oss-120b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da5bfff9-472c-4453-ac46-4e8df6f2e552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I help you today?'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = LLM.invoke(\"Hi!\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4c1ce-8afd-497d-8a5a-91573678b9c5",
   "metadata": {},
   "source": [
    "## Prompts Templates\n",
    "A prompt is the text we give to the LLM (or ChatModel). <br>\n",
    "LangChain provides special classes like PromptTemplate and ChatPromptTemplate to make prompts dynamic and reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8330e5a6-7622-467c-9183-4edb332e194b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  Explain Large Language Language in simple Term\n"
     ]
    }
   ],
   "source": [
    "# Single Input Variable\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate(template=\"Explain {topic} in simple Term\")\n",
    "\n",
    "prompt = template.format(topic=\"Large Language Language\")\n",
    "\n",
    "print(\"Prompt: \", prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8dd359ba-73f1-4093-94d9-5e874fd71f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Large Language Model (LLM) – Simple Explanation**\n",
       "\n",
       "Imagine you have a huge library of books, articles, chats, and all kinds of written text. A **large language model** is a computer program that has read (or been trained on) that massive collection of words and sentences. By “reading” all that text, the model learns how language works—how words fit together, what ideas usually follow each other, and how to answer questions or write new text that sounds natural.\n",
       "\n",
       "### How It Works (in everyday language)\n",
       "\n",
       "| Step | What Happens | Simple Analogy |\n",
       "|------|-------------|--------------|\n",
       "| **1. Learning from lots of text** | The model reads billions of words. | Like a child listening to many conversations and stories. |\n",
       "| **2. Spotting patterns** | It notices which words often appear together and how sentences are built. | Like noticing that “peanut butter” often goes with “jelly”. |\n",
       "| **3. Building a “brain”** | It creates a huge network of tiny math units (called *parameters*) that store those patterns. | Like building a huge map of all the word connections you’ve ever heard. |\n",
       "| **4. Generating text** | When you ask it something, it uses the map to predict the next word, then the next, and so on, creating a full answer. | Like playing a game of “what comes next?” with a very well‑read friend. |\n",
       "\n",
       "### Key Points in Plain Language\n",
       "\n",
       "- **“Large”**: The model has *a lot* of parameters (often billions) and has been trained on *a lot* of text. The more data and parameters, the better it can capture subtle language nuances.\n",
       "- **“Language”**: It deals with human language—English, Spanish, Chinese, etc.—and learns the rules, slang, facts, and even some reasoning.\n",
       "- **“Model”**: This is the mathematical “brain” that stores what it learned and can be used to answer questions, write stories, translate languages, summarize text, and more.\n",
       "\n",
       "### Everyday Example\n",
       "\n",
       "Think of a **large language model** like a super‑smart autocomplete on your phone, but instead of just finishing a single sentence, it can:\n",
       "\n",
       "- Write a whole paragraph for you.\n",
       "- Explain a concept (like this answer).\n",
       "- Translate a sentence from one language to another.\n",
       "- Answer questions about history, science, or your own notes.\n",
       "\n",
       "All of this happens because the model has “read” a massive amount of text and learned the patterns that make language make sense.\n",
       "\n",
       "### Bottom Line\n",
       "\n",
       "A **large language model** is a computer program that has learned how humans write and speak by studying huge amounts of text. It uses that knowledge to generate new, human‑like text when you ask it something. Think of it as a very well‑read, very fast, and always‑ready writing assistant."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa80eb04-99a0-478e-bdfd-155cbadb87d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: You are helpfull assistent\\nHuman: Summarize the concept of Large Language Model in 3 bullet points.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ChatPromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are helpfull assistent\"),\n",
    "        (\"user\", \"Summarize the concept of {concept} in 3 bullet points.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "message = template.format(concept=\"Large Language Model\")\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43492224-24ed-49a0-8ffe-8d1b0736f783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- **Statistical pattern learners:** Large Language Models (LLMs) are neural networks trained on massive text corpora to predict the next word (or token) given its context, learning statistical patterns of language rather than explicit rules.\n",
       "\n",
       "- **Scale and capability:** By scaling up the number of parameters (often billions) and the amount of training data, LLMs acquire broad knowledge and can perform diverse tasks—translation, summarization, reasoning—through simple prompting.\n",
       "\n",
       "- **Probabilistic generation:** LLMs output a probability distribution over possible next tokens, allowing them to generate coherent, context‑aware text, but they lack true understanding or consciousness; their outputs reflect learned patterns and may contain errors or biases."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(message)\n",
    "\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eb8bcc-e967-43db-b0ba-7836d98592e7",
   "metadata": {},
   "source": [
    "- SystemMessage -- for content which should be passed to direct the conversation\n",
    "- HumanMessage -- for content in the input from the user.\n",
    "- AIMessage -- for content in the response from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "991ceee5-3faf-4f08-9d1e-94638ef97bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a wise and slightly humorous AI mentor.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Can you explain LangChain in one sentence?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='LangChain is a Python framework that helps you build apps powered by large language models.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Great! Can you also give me one real-world use case?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Message Types\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import AIMessage, SystemMessage, HumanMessage\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a wise and slightly humorous AI mentor.\"),\n",
    "        HumanMessage(content=\"Can you explain LangChain in one sentence?\"),\n",
    "        AIMessage(\n",
    "            content=\"LangChain is a Python framework that helps you build apps powered by large language models.\"\n",
    "        ),\n",
    "        HumanMessage(content=\"Great! Can you also give me one real-world use case?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "message = template.format_messages()\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7fece46-fce1-47c0-b125-dabe5a710286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Use‑case example:** A SaaS company builds a **customer‑support chatbot** with LangChain that pulls the latest product documentation, ticket history, and FAQ articles into a single LLM‑driven assistant—so when a user asks “How do I reset my API key?” the bot can instantly retrieve the relevant policy, walk the user through the steps, and even log the interaction for future analytics, all without writing a custom retrieval‑augmented‑generation pipeline from scratch."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(message).content\n",
    "\n",
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8a5ce-580f-4171-9868-d5d399d5d6aa",
   "metadata": {},
   "source": [
    "## Chain in LangChain\n",
    "A Chain is basically a pipeline that connects multiple steps together, where the output of one step becomes the input of the next.\n",
    "https://python.langchain.com/api_reference/langchain/chains.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524a8d6d-066f-45be-8574-1c8a77126c42",
   "metadata": {},
   "source": [
    "### 1. Single-Step Chain (LLMChain)\n",
    "This is the simplest form — Prompt → LLM → Output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4e1da99-5657-4ada-bd87-84706a271ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Quantum Computing',\n",
       " 'text': '**Quantum Computing in a nutshell**\\n\\n1. **Bits vs. Qubits**  \\n   - *Classical computers* use **bits** that are either **0** or **1**.  \\n   - *Quantum computers* use **qubits** that can be **0, 1, or both at the same time** (thanks to a property called **super‑position**).\\n\\n2. **Super‑position**  \\n   Imagine a spinning coin. While it’s spinning, you can’t say it’s heads *or* tails—it’s effectively both until you look. A qubit works similarly: it holds many possibilities at once, letting a quantum computer explore many solutions in parallel.\\n\\n3. **Entanglement**  \\n   When two qubits become **entangled**, the state of one instantly influences the other, no matter how far apart they are. This creates a kind of “instant‑communication” link that lets quantum computers coordinate many calculations at once.\\n\\n4. **Why it matters**  \\n   Because a quantum computer can process many possibilities simultaneously, it can solve certain problems (like factoring huge numbers, simulating molecules, or optimizing complex systems) far faster than any classical computer can.\\n\\n5. **A simple analogy**  \\n   - **Classical computer:** A single‑lane road where cars (bits) can only go forward one at a time.  \\n   - **Quantum computer:** A multi‑lane highway where each car can be in many lanes at once, and some cars are magically linked (entangled) so they move together in perfect sync. This lets the “traffic” solve a puzzle much quicker.\\n\\n---\\n\\n### 🎉 Fun Fact\\n\\nThe term **“quantum”** comes from the Latin word *quantum* meaning “how much.” In the 1900s, physicists discovered that energy isn’t continuous—it comes in tiny packets called **quanta**. That same idea—tiny, discrete units—underlies the qubits that power quantum computers!'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain {topic} in simple terms and add one fun fact.\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=model, prompt=prompt)\n",
    "\n",
    "result = chain.invoke(\"Quantum Computing\")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6810e60b-ef38-4aab-81c9-6d2ef7d0957d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**What is cloud computing?**  \\nThink of the cloud as a giant, always‑available “digital library” that lives on the internet instead of on your own computer or phone. Instead of storing files, running programs, or doing calculations on a single device, you “borrow” the power and storage of big, remote computers (called servers) that are kept in special data‑center buildings. You can access your data and apps from any device that’s connected to the internet—just like you can read a book from a library no matter where you are, as long as you have a library card.\\n\\n**Simple analogy:**  \\nImagine you’re a chef who wants to bake a cake. Instead of buying a huge oven, all the ingredients, and a kitchen for every single cake you make, you go to a shared kitchen (the cloud). You pay only for the time you use the oven, the space you need, and the tools you need. When you’re done, you leave the kitchen clean and ready for the next chef. The cloud works the same way for software, storage, and even heavy‑duty computing tasks.\\n\\n**Key points in plain language**\\n\\n| What you want to do | Where it happens in the cloud | Why it’s handy |\\n|--------------------|----------------------------|--------------|\\n| **Store photos, videos, documents** | On remote servers (the “cloud”) | Access them from any phone, tablet, or computer—no need to carry a hard drive |\\n| **Run an app (like email, video chat, or games)** | The app runs on powerful computers far away | Your device can be small and cheap, yet still run powerful software |\\n| **Do heavy calculations (e.g., AI, big data)** | Big data centers with lots of processors | You get super‑fast performance without buying expensive hardware |\\n\\n**One fun fact:**  \\nThe term “cloud” comes from the little cloud symbol you see in diagrams of computer networks. It started as a simple way to show “the internet” in a picture, but the name stuck—so now we literally call it “the cloud”! 🌥️'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Runnable chain\n",
    "# Use pipe | operator to make a chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "res = chain.invoke(\"Cloud Computing\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820f6c6a-b33f-45b4-9228-62210cad1a3e",
   "metadata": {},
   "source": [
    "### 2. Multi-Step Chain (SimpleSequentialChain)\n",
    "This combines two LLM steps, first create an outline, then expand it into a blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2021750-3e0c-433a-b156-1fb573d6b4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'The Future of Renewable Energy', 'output': '**The Future of Renewable Energy**\\n\\nThe renewable‑energy landscape is undergoing a rapid transformation, driven by breakthroughs that were once science‑fiction. **Advanced solar solutions** such as perovskite cells promise efficiencies above 30\\u202f%, while bifacial panels harvest light from both sides, boosting output without extra land. Floating solar farms now dot reservoirs and coastal lagoons, turning water surfaces into power generators. In the wind sector, **offshore floating turbines** can be anchored in deep water where winds are strongest, and high‑altitude kite or airborne wind systems capture jet‑stream energy that traditional turbines can’t reach. Meanwhile, **solid‑state batteries**, hydrogen‑based storage, and AI‑driven grid optimization promise to smooth the intermittent nature of sun and wind, turning intermittent power into reliable, dispatchable energy.\\n\\nEconomic and policy forces are accelerating this shift. Carbon‑pricing mechanisms, renewable‑portfolio standards, and global climate accords create a regulatory backbone that rewards clean power. The Levelized Cost of Energy (LCOE) for solar and wind has fallen by more than 80\\u202f% in a decade, and economies of scale in manufacturing keep driving prices down. Green bonds, ESG‑focused capital, and public‑private partnerships are unlocking trillions of dollars in financing, making large‑scale projects financially viable.\\n\\nBeyond technology, the transition reshapes society. Decentralized micro‑grids and off‑grid solutions bring electricity to remote communities, while new jobs in manufacturing, installation, and maintenance fuel a green economy. Yet challenges remain: grid stability, storage scalability, and coordination across transport, industry, and housing must be addressed. By 2030‑2050, renewables are projected to supply over 70\\u202f% of global electricity, a milestone that hinges on consumer choices, corporate commitments, and continued policy support. The future is bright, and it’s already here.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "outline_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Create a 3-point outline for a blog post about {topic}.\",\n",
    ")\n",
    "\n",
    "outline_chain = LLMChain(llm=model, prompt=outline_prompt)\n",
    "\n",
    "# Step 2: Content generator\n",
    "content_prompt = PromptTemplate(\n",
    "    input_variables=[\"outline\"],\n",
    "    template=\"Expand this outline into a detailed 200-word blog post:\\n{outline}\",\n",
    ")\n",
    "content_chain = LLMChain(llm=model, prompt=content_prompt)\n",
    "\n",
    "# Combine into one chain\n",
    "blog_chain = SimpleSequentialChain(chains=[outline_chain, content_chain])\n",
    "\n",
    "# Run the full pipeline\n",
    "final_result = blog_chain.invoke(\"The Future of Renewable Energy\")\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b90e2e7-5991-4ddf-9752-e268c99d28fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Great overview!**  \n",
      "Your piece captures the excitement and urgency surrounding the renewable‑energy transition. Below is a concise summary of the three main sections, followed by a few suggestions that could help tighten the narrative and add depth.\n",
      "\n",
      "---\n",
      "\n",
      "## 📋 Quick Summary\n",
      "\n",
      "| **Section** | **Key Take‑aways** |\n",
      "|------------|-------------------|\n",
      "| **1. Emerging Technologies** | • **Solar:** Perovskite cells, bifacial panels, solar‑glass integration.<br>• **Wind:** Floating offshore turbines, bladeless vortex generators, AI‑optimized layouts.<br>• **Storage:** Solid‑state batteries, hydrogen‑based power‑to‑fuel, thermal‑storage (molten salt, concrete). |\n",
      "| **2. Economic & Policy Drivers** | • LCOE for solar/wind now < LCOE for new coal in many markets.<br>• Carbon‑pricing, renewable‑portfolio standards, green‑bond financing drive demand.<br>• Corporate 100 % renewable procurement fuels micro‑grids and green‑fund growth. |\n",
      "| **3. Societal Impact & Path Forward** | • Renewable rollout can close energy‑access gaps and create millions of jobs.<br>• Reskilling pathways for former fossil‑fuel workers.<br>• Roadmap: 40 % renewables by 2030 → 70 % by 2040 → fully decarbonized grid by 2050, requiring coordinated policy, industry scaling, and research breakthroughs. |\n",
      "\n",
      "---\n",
      "\n",
      "## 🔧 Suggestions for Strengthening the Piece\n",
      "\n",
      "| **Area** | **What to Add / Refine** |\n",
      "|----------|------------------------|\n",
      "| **Data & Sources** | Cite a few recent, credible sources (e.g., IEA 2024 report, BloombergNEF, IPCC) to back up claims like “LCOE below coal” and “perovskite efficiencies > 25 %”. |\n",
      "| **Quantify Impact** | Add concrete numbers: e.g., “perovskite‑silicon tandem cells have reached 30 % efficiency in lab tests” or “floating offshore turbines can generate 10–15 % more energy than fixed‑bottom units”. |\n",
      "| **Policy Examples** | Highlight specific policies that have proven effective: e.g., EU’s Renewable Energy Directive, California’s SB 100, China’s “dual carbon” targets, or the UK’s Contracts for Difference (CfD) scheme. |\n",
      "| **Economic Lens** | Briefly discuss the **levelized cost of storage (LCOS)** and how it’s falling, which is crucial for integrating intermittent renewables. |\n",
      "| **Social Angle** | Add a short anecdote or case study (e.g., a solar‑glass project in a low‑income urban district) to illustrate the “energy‑access” point. |\n",
      "| **Roadmap Detail** | Break the 2050 roadmap into **milestones** (e.g., 2025‑2029: scaling manufacturing; 2030‑2034: grid‑integration pilots; 2035‑2040: large‑scale storage). This makes the roadmap feel more actionable. |\n",
      "| **Visual Elements** | If this will be a report or blog post, consider adding: <br>• **Infographic** of the technology stack (solar → wind → storage).<br>• **Timeline** of policy milestones.<br>• **Map** of emerging offshore wind sites. |\n",
      "| **Call‑to‑Action** | End with a concrete “what can readers do?” – e.g., “support local renewable projects, invest in green bonds, or advocate for carbon‑pricing legislation in your community.” |\n",
      "\n",
      "---\n",
      "\n",
      "## 📈 What’s Next? (Ideas for a Follow‑Up Piece)\n",
      "\n",
      "1. **Deep‑Dive on Perovskite** – Technical challenges (stability, lead‑free alternatives) and the roadmap to commercial deployment.  \n",
      "2. **AI in Renewable Planning** – Case studies of AI‑optimized wind farms (e.g., Google’s DeepMind for grid balancing).  \n",
      "3. **Hydrogen Economy** – How green hydrogen can complement renewables in heavy‑industry and transport.  \n",
      "4. **Equity & Just Transition** – Policies and programs that have successfully reskilled fossil‑fuel workers (e.g., Germany’s “Energiewende” training programs).  \n",
      "\n",
      "---\n",
      "\n",
      "### Final Thought\n",
      "\n",
      "Your article already paints a compelling picture of a clean‑energy future that’s already in motion. By weaving in a few concrete data points, policy examples, and a clear call‑to‑action, you’ll turn a strong overview into a compelling, actionable roadmap that resonates with policymakers, investors, and the general public alike. Keep the momentum—this is exactly the kind of narrative that can help turn the “inevitable” into reality. 🌍⚡\n",
      "\n",
      "--- \n",
      "\n",
      "*Feel free to let me know if you’d like a deeper dive into any of the technologies, a draft of an infographic, or help fleshing out a specific policy case study.*\n"
     ]
    }
   ],
   "source": [
    "# Runnable chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "outline_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Create a 3-point outline for a blog post about {topic}.\",\n",
    ")\n",
    "\n",
    "outline_chain = outline_prompt | model | StrOutputParser()\n",
    "\n",
    "content_prompt = PromptTemplate(\n",
    "    input_variables=[\"outline\"],\n",
    "    template=\"Expand this outline into a detailed 200-word blog post:\\n{outline}\",\n",
    ")\n",
    "content_chain = content_prompt | model | StrOutputParser()\n",
    "\n",
    "blog_chain = outline_chain | content_chain | model | StrOutputParser()\n",
    "\n",
    "# Run the full pipeline\n",
    "final_result = blog_chain.invoke({\"The Future of Renewable Energy\"})\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b4470-1070-4a2c-a355-11fe4b2c15d2",
   "metadata": {},
   "source": [
    "##  Memory in LangChain\n",
    "By default, LangChain calls are stateless, the model only sees the current input and forgets everything else.\n",
    "https://python.langchain.com/api_reference/langchain/memory.html <br>\n",
    "https://python.langchain.com/docs/versions/migrating_memory/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d06d50ef-9650-40dc-9d4d-d60a5bb0bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"\n",
    "    You are a friendly chatbot.\n",
    "    User: {question}\n",
    "    Assistant:\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# Create chain\n",
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "74913969-11d3-4fd1-b41c-36515be6fb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Saad! 👋 Great to meet you. How’s your day going so far?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\"Hi! I am Saad\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0a00152-7309-4578-bf80-9fe13cf09b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I’m afraid I don’t have your name yet—could you remind me? 😊 Once you tell me, I’ll keep it in mind for the rest of our chat!'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\"Can you remeber my name?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed31d4cc-9b3c-475c-8b76-6579f22e60c8",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory\n",
    "- Stores: Full conversation history.\n",
    "- Use: Small chats where cost & token limits aren’t an issue.\n",
    "- Pro: Simple and easy.\n",
    "- Con: Can grow too large for long sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e96f423-9db1-42cf-9949-41ed49bf8ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aa304\\AppData\\Local\\Temp\\ipykernel_21716\\1956154715.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create Memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"question\"],\n",
    "    template=\"\"\"\n",
    "    You are an AI Assistant for **Advanced Telecom Services (ATS)**.\n",
    "    Conversation so far:\n",
    "    {chat_history}\n",
    "    User: {question}\n",
    "    Assistant:\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# Create chain\n",
    "chain = LLMChain(llm=model, prompt=prompt, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d55600ff-ee09-43ce-8b11-d26ce02e7c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aa304\\AppData\\Local\\Temp\\ipykernel_21716\\2420808583.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain.run(\"Hi!, My name is Saad, How can you help me?\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Saad! 👋 I'm your virtual assistant for **Advanced Telecom Services (ATS)**. I’m here to help you with anything from choosing the right mobile or internet plan, checking your bill, troubleshooting a service issue, or learning about our latest offers and features.\\n\\nWhat can I assist you with today? (e.g.,\\u202fplan options, billing, technical support, new services, etc.)\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.run(\"Hi!, My name is Saad, How can you help me?\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33b03e99-e30a-4b17-aa5c-59ef14b36b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is **Saad**. How can I assist you further today?'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.run(\"Can you remeber, What's my name?\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a96d91b",
   "metadata": {},
   "source": [
    "# LLM-Powered Chatbot with Conversation History\n",
    "\n",
    "We'll go over an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a **conversation** and **remember previous interactions** with a chat model.\n",
    "\n",
    "Source: [LangChain Docs](https://python.langchain.com/docs/tutorials/chatbot/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5fd3a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"openai/gpt-oss-120b\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99f0e58",
   "metadata": {},
   "source": [
    "`ChatModels` in LangChain work like special objects called \"Runnables\", which means they have a common way you can use them.\n",
    "If you just want to run the model, you give it a list of messages and use the `.invoke` method — that’s how you “talk” to it and get a response.\n",
    "\n",
    "It’s like: “Put your messages in a box, hand it to the .invoke button, and the model will answer.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "62ac8f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Ahmed! 👋 Nice to meet you. How can I assist you today?', additional_kwargs={'reasoning_content': 'The user says \"Hi! I am Ahmed.\" It\\'s a greeting and introduction. We should respond politely, greeting back, maybe ask how we can help. No special instructions. Just respond friendly.'}, response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 77, 'total_tokens': 143, 'completion_time': 0.119421001, 'prompt_time': 0.004487928, 'queue_time': 0.001880367, 'total_time': 0.123908929}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_b9ee615ece', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--4b61e73b-a9d1-4d99-8825-fa9c2e87466b-0', usage_metadata={'input_tokens': 77, 'output_tokens': 66, 'total_tokens': 143})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I am Ahmed.\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a3c58e",
   "metadata": {},
   "source": [
    "The model on its own does not have any concept of state. For example, if you ask a followup question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a122b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I don’t have any information about your name. If you’d like me to address you by name, feel free to let me know!', additional_kwargs={'reasoning_content': 'The user asks \"What\\'s my name?\" We have no prior conversation. The user hasn\\'t provided their name. According to policy, we cannot guess or fabricate personal info. We can ask for clarification. So we should respond asking for their name or say we don\\'t have that info.'}, response_metadata={'token_usage': {'completion_tokens': 93, 'prompt_tokens': 75, 'total_tokens': 168, 'completion_time': 0.169508298, 'prompt_time': 0.004467525, 'queue_time': 0.001955639, 'total_time': 0.173975823}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_4a19b1544c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--5baf73b6-4f4e-42ae-8083-d7513295d587-0', usage_metadata={'input_tokens': 75, 'output_tokens': 93, 'total_tokens': 168})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a71975",
   "metadata": {},
   "source": [
    "We can see that it doesn't take the previous conversation turn into context, and cannot answer the question. This makes for a terrible chatbot experience!\n",
    "\n",
    "To get around this, we need to pass the entire [conversation history](https://python.langchain.com/docs/concepts/chat_history/) into the model. Let's see what happens when we do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "41882f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You introduced yourself as **Ahmed**. If there’s anything specific you’d like to talk about or any question you have, just let me know!', additional_kwargs={'reasoning_content': 'User asks \"Who am I?\" They previously said \"Hi! I am Ahmed.\" So answer: Ahmed. Could ask for more context? Probably respond that you are Ahmed.'}, response_metadata={'token_usage': {'completion_tokens': 75, 'prompt_tokens': 108, 'total_tokens': 183, 'completion_time': 0.136154885, 'prompt_time': 0.005520813, 'queue_time': 0.001786645, 'total_time': 0.141675698}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_8db49de948', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--2ab8a726-f560-4807-8c7b-ae8724cdeecf-0', usage_metadata={'input_tokens': 108, 'output_tokens': 75, 'total_tokens': 183})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I am Ahmed.\"),\n",
    "        AIMessage(\n",
    "            content=\"Hello Ahmed! 👋 Nice to meet you. How can I assist you today?\"\n",
    "        ),\n",
    "        HumanMessage(content=\"Who am I?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31162d5b",
   "metadata": {},
   "source": [
    "And now we can see that we get a good response!\n",
    "\n",
    "This is the basic idea underpinning a chatbot's ability to interact conversationally. So how do we best implement this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cad5b3",
   "metadata": {},
   "source": [
    "# Message persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d52e2",
   "metadata": {},
   "source": [
    "LangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\n",
    "\n",
    "Wrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.\n",
    "\n",
    "LangGraph comes with a simple in-memory checkpointer, which we use below. See its [documentation](https://langchain-ai.github.io/langgraph/concepts/persistence/?_gl=1*fi5jbl*_gcl_au*MTE0NDQ3MDcyNC4xNzU0OTExODMx*_ga*MTU2Mzg1ODA3NS4xNzQ5NjMyNjk5*_ga_47WX3HKKY2*czE3NTQ5MTE4MjYkbzckZzEkdDE3NTQ5MTE4MzckajQ5JGwwJGgw) for more detail, including how to use different persistence backends (e.g., SQLite or Postgres)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bfe580a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x000001F1578902F0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modern usage with LangGraph: Production-friendly way to store chat state.\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fbd2af",
   "metadata": {},
   "source": [
    "We now need to create a `config` that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a `thread_id`. This should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f0653126",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4576d23f",
   "metadata": {},
   "source": [
    "This enables us to support multiple conversation threads with a single application, a common requirement when your application has multiple users.\n",
    "\n",
    "We can then invoke the application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "78abe6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi! I am Ahmed.', additional_kwargs={}, response_metadata={}, id='544b4147-86e8-43bd-b51f-62292b72e703'),\n",
       "  AIMessage(content='Hello Ahmed! 👋 Nice to meet you. How can I assist you today?', additional_kwargs={'reasoning_content': 'The user says \"Hi! I am Ahmed.\" It\\'s a greeting and introduction. We should respond politely, greeting back, maybe ask how we can help. No special instructions. Just respond friendly.'}, response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 77, 'total_tokens': 143, 'completion_time': 0.119635286, 'prompt_time': 0.004475704, 'queue_time': 0.001564642, 'total_time': 0.12411099}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_8db49de948', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--f5f02d3b-6bb0-4ba7-9e3c-855fcf518f98-0', usage_metadata={'input_tokens': 77, 'output_tokens': 66, 'total_tokens': 143})]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Hi! I am Ahmed.\"\n",
    "\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d49358e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Ahmed! 👋 Nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "31b9372d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, you introduced yourself as **Ahmed**. 😊 How can I help you today, Ahmed?\n"
     ]
    }
   ],
   "source": [
    "query = \"Do you know my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c50f32",
   "metadata": {},
   "source": [
    "Great! Our chatbot now remembers things about us. If we change the config to reference a different `thread_id`, we can see that it starts the conversation fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ed1d0894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don’t know your name—unless you’ve already mentioned it earlier in the conversation (which I don’t see). If you’d like me to address you by name, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "new_config = {\"configurable\": {\"thread_id\": \"xyz890\"}}\n",
    "\n",
    "query = \"Do you know my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config=new_config)\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4d6633",
   "metadata": {},
   "source": [
    "However, we can always go back to the original conversation (since we are persisting it in a database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1598a580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes—you told me your name is **Ahmed**. How can I assist you further?\n"
     ]
    }
   ],
   "source": [
    "query = \"Do you know my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898c30dc",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "This is how we can support a chatbot having conversations with many users!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
