{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1276c1f",
   "metadata": {},
   "source": [
    "# Introduction to LangChain and LangGraph\n",
    "\n",
    "## What is LangChain?\n",
    "\n",
    "LangChain is a powerful framework for building applications powered by Large Language Models (LLMs). Think of it as a toolkit that helps you create AI-powered applications more easily by providing:\n",
    "\n",
    "1. [**Components**](https://python.langchain.com/docs/integrations/components/): Ready-to-use building blocks for working with LLMs\n",
    "2. [**Chains**](https://python.langchain.com/api_reference/langchain/chains.html): Ways to combine these components into useful applications\n",
    "3. [**Memory**](https://python.langchain.com/api_reference/langchain/memory.html): Tools to help LLMs [remember conversation history\n",
    "4. [**Agents**](https://python.langchain.com/api_reference/langchain/agents.html): Autonomous AI assistants that can use tools and make decisions\n",
    "\n",
    "## What is LangGraph?\n",
    "\n",
    "LangGraph is an extension of LangChain that helps you build stateful, multi-step applications. It's particularly useful for:\n",
    "- Building conversational agents that remember context\n",
    "- Creating workflows where each step depends on previous steps\n",
    "- Managing state and persistence in your applications\n",
    "\n",
    "## The LangChain Ecosystem\n",
    "\n",
    "LangChain offers a complete ecosystem for LLM application development:\n",
    "\n",
    "<img src=\"https://python.langchain.com/svg/langchain_stack_112024_dark.svg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77ad0c0",
   "metadata": {},
   "source": [
    "## Key Features of LangChain\n",
    "\n",
    "1. **Prompts Management**: Easy creation and reuse of prompts\n",
    "2. **Model Integration**: Support for multiple LLM providers\n",
    "3. **Chain Building**: Combine multiple steps into pipelines\n",
    "4. **Memory Systems**: Add memory to your LLM applications\n",
    "5. **Agents & Tools**: Create AI assistants that can use tools\n",
    "\n",
    "LangChain was created by Harrison Chase and first released in October 2022. It has since become one of the most popular frameworks for building LLM applications with over [113K GitHub stars](https://github.com/langchain-ai/langchain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ceed41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import HTML, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d24499",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0c5cc-b3c4-4ccc-b2f1-df018237dbea",
   "metadata": {},
   "source": [
    "## [Chat Models](https://python.langchain.com/docs/integrations/chat/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ec89812-d456-4351-8eed-3d0b5614594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"openai/gpt-oss-120b\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50dc2ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I help you today?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(\"hi\")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4c1ce-8afd-497d-8a5a-91573678b9c5",
   "metadata": {},
   "source": [
    "## [Working with Prompts Templates](https://python.langchain.com/docs/concepts/prompt_templates/)\n",
    "\n",
    "LangChain provides powerful tools for working with prompts:\n",
    "\n",
    "1. **PromptTemplate**: For creating reusable text prompts with variables\n",
    "2. **ChatPromptTemplate**: For creating structured chat prompts with different message types:\n",
    "   - `SystemMessage`: Instructions or context for the AI\n",
    "   - `HumanMessage`: User inputs\n",
    "   - `AIMessage`: AI responses\n",
    "\n",
    "Prompt templates help you:\n",
    "- Keep your prompts consistent\n",
    "- Make prompts reusable with variables\n",
    "- Structure multi-turn conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8330e5a6-7622-467c-9183-4edb332e194b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  Explain Large Language Language in simple Term\n"
     ]
    }
   ],
   "source": [
    "# Single Input Variable\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate(template=\"Explain {topic} in simple Term\")\n",
    "\n",
    "prompt = template.format(topic=\"Large Language Language\")\n",
    "\n",
    "print(\"Prompt: \", prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dd359ba-73f1-4093-94d9-5e874fd71f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Large Language Model (LLM) – a simple, everyday‑style explanation**\n",
       "\n",
       "---\n",
       "\n",
       "### 1. What it is, in one sentence  \n",
       "A **large language model** is a computer program that has read a huge amount of text and learns how to predict the next word in a sentence, so it can write, answer questions, translate, and do many other language‑related tasks.\n",
       "\n",
       "---\n",
       "\n",
       "### 2. How it works – the “story” version  \n",
       "\n",
       "| Step | What happens | Everyday analogy |\n",
       "|------|--------------|----------------|\n",
       "| **1. Reading a huge library** | The model is fed billions of words from books, articles, websites, etc. | **Like a kid who reads thousands of books** |\n",
       "| **2. Learning patterns** | It looks for patterns: “If the sentence says ‘The cat is …’, the next word is often ‘sleeping’ or ‘on the roof’.” | **Like learning that “peanut butter and …” is usually followed by “jelly”.** |\n",
       "| **3. Guessing the next word** | When you give it a prompt, the model predicts the most likely next word, then the next, and so on, until it forms a full sentence. | **Like playing a game of “fill‑in‑the‑blank”** |\n",
       "| **4. Using the guess** | The string of predicted words becomes the answer, story, translation, etc. | **Like a storyteller finishing a story you started.** |\n",
       "\n",
       "---\n",
       "\n",
       "### 3. Why “large” matters  \n",
       "\n",
       "- **Lots of data**: The more text it reads, the better it gets at spotting subtle patterns.\n",
       "- **Big brain (parameters)**: Think of “parameters” as tiny knobs that the model adjusts while learning. A “large” model has billions of these knobs, giving it a finer‑grained sense of language.\n",
       "\n",
       "---\n",
       "\n",
       "### 4. What it can do (in simple terms)\n",
       "\n",
       "| Task | Example |\n",
       "|------|--------|\n",
       "| **Answer questions** | “What’s the capital of France?” → “Paris.” |\n",
       "| **Write text** | Write a short story about a dragon. |\n",
       "| **Translate** | Turn English into Spanish. |\n",
       "| **Summarize** | Turn a long article into a short paragraph. |\n",
       "| **Chat** | Have a conversation like we’re doing now. |\n",
       "\n",
       "---\n",
       "\n",
       "### 5. What it can’t do (or does poorly)\n",
       "\n",
       "- **Understand the world** the way humans do. It only knows what it has seen in text.\n",
       "- **Feel emotions** or have personal experiences.\n",
       "- **Guarantee truth** – it can confidently say something that’s wrong if the pattern it learned suggests that answer.\n",
       "\n",
       "---\n",
       "\n",
       "### 6. Quick analogy: **The Predictive Text on Your Phone**\n",
       "\n",
       "- When you type “I’m going to the …”, your phone suggests “store”, “park”, “gym”, etc.  \n",
       "- A large language model does the same thing, but on a **much larger scale**: it can generate whole paragraphs, not just a single word, and it has learned from **all** the text on the internet, not just your personal messages.\n",
       "\n",
       "---\n",
       "\n",
       "### 7. TL;DR (the ultra‑short version)\n",
       "\n",
       "A **large language model** is a computer program that reads a massive amount of text, learns how words usually follow each other, and then uses that knowledge to generate or understand language—much like a very well‑read, super‑fast “autocomplete” that can write essays, answer questions, translate languages, and more. \n",
       "\n",
       "--- \n",
       "\n",
       "*Hope that makes the idea clear! If you want a deeper dive into any part (like how the “tiny knobs” work, or how training happens), just let me know.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa80eb04-99a0-478e-bdfd-155cbadb87d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: You are helpfull assistent\\nHuman: Summarize the concept of Large Language Model in 3 bullet points.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ChatPromptTemplate\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are helpfull assistent\"),\n",
    "        (\"user\", \"Summarize the concept of {concept} in 3 bullet points.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "message = template.format(concept=\"Large Language Model\")\n",
    "\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43492224-24ed-49a0-8ffe-8d1b0736f783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- **Statistical pattern learners:** Large Language Models (LLMs) are neural networks trained on massive text corpora to predict the next word (or token) in a sequence, learning statistical patterns, grammar, facts, and reasoning abilities from the data.\n",
       "\n",
       "- **Scale and architecture:** They typically use transformer architectures with billions (or more) of parameters, enabling them to capture long‑range dependencies and generate coherent, context‑aware text across many domains.\n",
       "\n",
       "- **Versatile applications:** Once trained, LLMs can be prompted or fine‑tuned for tasks such as translation, summarization, coding, and conversational agents, often achieving performance comparable to or surpassing specialized models."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(message)\n",
    "\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eb8bcc-e967-43db-b0ba-7836d98592e7",
   "metadata": {},
   "source": [
    "### Using Message Types\n",
    "- SystemMessage -- for content which should be passed to direct the conversation\n",
    "- HumanMessage -- for content in the input from the user.\n",
    "- AIMessage -- for content in the response from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "991ceee5-3faf-4f08-9d1e-94638ef97bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a wise and slightly humorous AI mentor.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Can you explain LangChain in one sentence?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='LangChain is a Python framework that helps you build apps powered by large language models.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Great! Can you also give me one real-world use case?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Message Types\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a wise and slightly humorous AI mentor.\"),\n",
    "        HumanMessage(content=\"Can you explain LangChain in one sentence?\"),\n",
    "        AIMessage(content=\"LangChain is a Python framework that helps you build apps powered by large language models.\"),\n",
    "        HumanMessage(content=\"Great! Can you also give me one real-world use case?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "message = template.format_messages()\n",
    "\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7fece46-fce1-47c0-b125-dabe5a710286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Use‑case: AI‑powered customer‑support chatbot that pulls in‑depth product knowledge from a company’s internal docs, FAQs, and support tickets, then uses a language model (via LangChain) to understand a user’s question, retrieve the most relevant sections, and generate a helpful, context‑aware response in real time.** \n",
       "\n",
       "*Why it shines:*  \n",
       "- **Document retrieval** (vector‑store search) pulls the exact policy or troubleshooting step the user needs.  \n",
       "- **Chain orchestration** lets you combine a *retriever* → *LLM* → *post‑processing* (e.g., tone‑adjustment, escalation check) all in one reusable pipeline.  \n",
       "- **Scalable**: you can add new knowledge bases (manuals, release notes, chat logs) without rewriting the bot—just re‑index and the same chain handles the rest.  \n",
       "\n",
       "In short, LangChain lets you stitch together “search‑the‑knowledge‑base” + “generate‑the‑answer” + “apply‑business‑rules” into a single, maintainable chatbot. 🚀"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(message).content\n",
    "\n",
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8a5ce-580f-4171-9868-d5d399d5d6aa",
   "metadata": {},
   "source": [
    "## Chains in LangChain\n",
    "\n",
    "A Chain in LangChain is a sequence of steps that process inputs to produce outputs. There are two ways to create chains:\n",
    "\n",
    "1. **Classic Chains**: Using classes like `LLMChain`\n",
    "2. **LCEL (LangChain Expression Language)**: Using the modern pipe operator `|`\n",
    "\n",
    "LCEL is the recommended way to build chains as it's:\n",
    "- More flexible\n",
    "- Easier to debug\n",
    "- Better for streaming\n",
    "- More performant\n",
    "\n",
    "Let's look at examples of both approaches:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524a8d6d-066f-45be-8574-1c8a77126c42",
   "metadata": {},
   "source": [
    "### 1. Single-Step Chain (LLMChain)\n",
    "This is the simplest form — Prompt → LLM → Output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4e1da99-5657-4ada-bd87-84706a271ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aa304\\AppData\\Local\\Temp\\ipykernel_17232\\1870853064.py:9: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=model, prompt=prompt)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'topic': 'Quantum Computing',\n",
       " 'text': '**Quantum Computing in a nutshell**\\n\\n1. **Bits vs. Qubits**  \\n   - *Classical computer*: uses **bits** that are either **0** or **1**.  \\n   - *Quantum computer*: uses **qubits** that can be **0, 1, or both at the same time** (thanks to a property called **super‑position**).\\n\\n2. **Super‑position**  \\n   Think of a spinning coin. While it’s spinning, you can’t say it’s heads *or* tails—it’s in a blend of both. A qubit can hold a blend of 0 and 1 simultaneously, letting a quantum computer explore many possibilities at once.\\n\\n3. **Entanglement**  \\n   When two qubits become **entangled**, the state of one instantly influences the other, no matter how far apart they are. This lets a quantum computer link qubits together in a way that lets them “talk” instantly, giving the machine extra “co‑ordination power”.\\n\\n4. **Why it matters**  \\n   Because a quantum computer can try many solutions at the same time, it can solve certain problems (like factoring huge numbers, simulating molecules, or optimizing complex systems) far faster than any classical computer can.\\n\\n5. **A simple analogy**  \\n   - **Classical**: Searching for a name in a phone book by reading each entry one by one.  \\n   - **Quantum**: Having a magical “super‑search” that looks at *all* pages at once and instantly tells you where the name is (but only for certain types of problems).\\n\\n---\\n\\n### 🎉 Fun Fact\\n\\nThe term **“quantum”** comes from the Latin word *quantum* meaning “how much”. The first quantum computer was built in 1998, and it was so tiny that it fit inside a refrigerator—because the delicate quantum states need to be kept **extremely cold**, often just a few thousandths of a degree above absolute zero! 🚀'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain {topic} in simple terms and add one fun fact.\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=model, prompt=prompt)\n",
    "\n",
    "result = chain.invoke(\"Quantum Computing\")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7790487e",
   "metadata": {},
   "source": [
    "**LCEL (LangChain Expression Language)**: Using the modern pipe operator `|`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6810e60b-ef38-4aab-81c9-6d2ef7d0957d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**What is cloud computing?**  \n",
       "Think of the cloud as a giant, shared computer that lives on the internet instead of on your own desk. Instead of buying and maintaining your own hardware (like a big server or a bunch of hard drives), you “rent” space and power from this remote computer. You can store files, run apps, and process data on the cloud, and you access everything through the internet—just like you open a website or use an app on your phone. The cloud does the heavy lifting, so your own device can stay small, cheap, and always up‑to‑date.\n",
       "\n",
       "**Fun fact:** The term “cloud” comes from the old practice of drawing a cloud shape on diagrams to represent the internet. The “cloud” we use today is literally the same symbol—just now it’s a massive, real‑world network of data centers spread across the globe! 🚀"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Runnable chain\n",
    "# Use pipe | operator to make a chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "response = chain.invoke(\"Cloud Computing\")\n",
    "\n",
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820f6c6a-b33f-45b4-9228-62210cad1a3e",
   "metadata": {},
   "source": [
    "### 2. Multi-Step Chain (SimpleSequentialChain)\n",
    "This combines two LLM steps, first create an outline, then expand it into a blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2021750-3e0c-433a-b156-1fb573d6b4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'The Future of Renewable Energy',\n",
       " 'output': '**The Future of Renewable Energy**\\n\\nThe renewable‑energy landscape is undergoing a rapid transformation, driven by breakthroughs that were once science‑fiction. **Advanced solar solutions**—from perovskite cells that promise 30\\u202f% higher efficiency to bifacial panels that harvest light from both sides—are now being deployed on floating platforms that turn reservoirs into power plants. Meanwhile, **next‑generation wind** is taking to the seas and skies: floating offshore turbines tap stronger, steadier winds, while high‑altitude kite and airborne systems capture energy at altitudes where wind speeds are three‑to‑four times higher than on land. The real game‑changer, however, is **storage and grid intelligence**. Solid‑state batteries deliver longer life and faster charging, hydrogen‑based systems store excess power for weeks, and AI‑driven grid optimization balances supply and demand in real time, making intermittent sources reliable.\\n\\nPolicy and economics are the twin engines accelerating this shift. Carbon‑pricing mechanisms, renewable‑portfolio standards, and the Paris Agreement’s targets create a predictable regulatory environment. Simultaneously, the Levelized Cost of Energy (LCOE) for solar and wind has fallen below $0.05\\u202fkWh in many markets, and economies of scale are driving prices even lower. Green bonds, ESG‑focused funds, and public‑private partnerships are funneling trillions of dollars into clean‑tech pipelines.\\n\\nBeyond technology, the **societal impact** is profound. Decentralized micro‑grids bring electricity to remote villages, while new jobs in manufacturing, installation, and maintenance are reshaping local economies. Yet challenges remain: grid stability, scalable storage, and coordination across transport, industry, and housing sectors must be addressed. Looking ahead to 2030‑2050, renewables could supply 70\\u202f% of global electricity, cutting CO₂ emissions by half. Achieving this vision will hinge on consumer behavior, digital innovation, and a shared commitment to a cleaner, more equitable energy future.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "outline_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Create a 3-point outline for a blog post about {topic}.\",\n",
    ")\n",
    "\n",
    "outline_chain = LLMChain(llm=model, prompt=outline_prompt)\n",
    "\n",
    "# Step 2: Content generator\n",
    "content_prompt = PromptTemplate(\n",
    "    input_variables=[\"outline\"],\n",
    "    template=\"Expand this outline into a detailed 200-word blog post:\\n{outline}\",\n",
    ")\n",
    "content_chain = LLMChain(llm=model, prompt=content_prompt)\n",
    "\n",
    "# Combine into one chain\n",
    "blog_chain = SimpleSequentialChain(chains=[outline_chain, content_chain])\n",
    "\n",
    "# Run the full pipeline\n",
    "final_result = blog_chain.invoke(\"The Future of Renewable Energy\")\n",
    "\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b90e2e7-5991-4ddf-9752-e268c99d28fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Summary – The Future of Renewable Energy**\n",
       "\n",
       "| **Theme** | **Key Points** | **Implications** |\n",
       "|----------|---------------|----------------|\n",
       "| **1. Emerging Technologies** | • **Perovskite & bifacial solar** – higher efficiency, lower cost, integration into buildings.<br>• **Floating offshore & bladeless wind** – access to deep‑water resources, reduced noise/maintenance, AI‑optimized layouts.<br>• **Advanced storage** – solid‑state batteries, hydrogen, thermal‑storage systems close the intermittency gap. | Enables higher‑density, ubiquitous generation and reliable, on‑demand renewable power. |\n",
       "| **2. Economic & Policy Drivers** | • **LCOE decline** – economies of scale, manufacturing efficiencies.<br>• **Policy tools** – carbon pricing, renewable‑portfolio standards, green‑bond financing.<br>• **Corporate procurement & micro‑grids** – direct clean‑power purchases, community‑level resilience.<br>• **Green‑investment funds** – capital directed to projects meeting financial & environmental criteria. | Creates a predictable, investment‑friendly market that accelerates deployment. |\n",
       "| **3. Societal Impact & Path Forward** | • **Energy equity** – affordable, reliable power for underserved communities and developing nations.<br>• **Smart‑grid & digital‑twin** – real‑time optimization, demand‑response, resilient grid operations.<br>• **Roadmaps (2030‑2050)** – government, industry, academia set milestones; success hinges on collaboration, research, and public‑private partnerships. | Moves the transition from a technology‑only story to a holistic, inclusive, and resilient energy system. |\n",
       "\n",
       "---\n",
       "\n",
       "### Why These Trends Matter\n",
       "\n",
       "1. **Technology‑driven cost reductions** make renewables competitive with fossil fuels without subsidies.\n",
       "2. **Policy certainty** (e.g., carbon pricing) de‑riskes investments, attracting both institutional and corporate capital.\n",
       "3. **Storage breakthroughs** eliminate the “intermittency” barrier, enabling renewables to serve baseload demand.\n",
       "4. **Equity focus** ensures that the transition benefits all socioeconomic groups, reducing the global energy divide.\n",
       "5. **Digital integration** (smart grids, digital twins) maximizes efficiency, reduces waste, and improves reliability.\n",
       "\n",
       "---\n",
       "\n",
       "### Actionable Takeaways\n",
       "\n",
       "| **For Policymakers** | **For Investors & Corporates** | **For Researchers & Innovators** |\n",
       "|----------------------|------------------------------|--------------------------------|\n",
       "| • Strengthen carbon‑pricing mechanisms and expand renewable‑portfolio standards.<br>• Incentivize community‑owned micro‑grids and energy‑justice programs. | • Allocate capital to projects with clear ESG metrics and proven technology readiness.<br>• Use corporate power purchase agreements (PPAs) to lock in clean‑energy supply. | • Accelerate scaling of perovskite and solid‑state battery production.<br>• Develop AI‑driven grid‑optimization tools and digital‑twin platforms. |\n",
       "| • Support financing tools (green bonds, climate‑finance) for early‑stage projects. | • Diversify procurement: combine utility‑scale, distributed, and storage assets. | • Focus on low‑cost, high‑durability materials for offshore and floating wind. |\n",
       "| • Promote energy‑equity programs (subsidies, community‑ownership models). | • Partner with local utilities to integrate micro‑grids and demand‑response programs. | • Collaborate with policy makers to align research roadmaps with 2030‑2050 targets. |\n",
       "\n",
       "---\n",
       "\n",
       "### Outlook to 2050\n",
       "\n",
       "- **2030:** >50 % of global electricity from renewables; widespread adoption of perovskite solar and floating offshore wind; solid‑state batteries in commercial use.\n",
       "- **2040:** Grid‑scale storage (hydrogen, thermal) provides >30 % of system flexibility; smart‑grid and digital‑twin platforms become standard.\n",
       "- **2050:** Near‑zero‑carbon electricity grid in most developed nations; energy‑equity metrics integrated into national energy plans; global renewable share >80 % with residual fossil use limited to hard‑to‑decarbonize sectors.\n",
       "\n",
       "---\n",
       "\n",
       "**Bottom line:** The convergence of cutting‑edge technology, supportive policy frameworks, and a focus on equity is turning clean energy from a hopeful vision into an inevitable reality. The next decade will be decisive—those who invest, innovate, and legislate now will shape a resilient, inclusive, and sustainable energy landscape for generations to come."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Runnable chain\n",
    "\n",
    "outline_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Create a 3-point outline for a blog post about {topic}.\",\n",
    ")\n",
    "\n",
    "outline_chain = outline_prompt | model | StrOutputParser()\n",
    "\n",
    "content_prompt = PromptTemplate(\n",
    "    input_variables=[\"outline\"],\n",
    "    template=\"Expand this outline into a detailed 200-word blog post:\\n{outline}\",\n",
    ")\n",
    "content_chain = content_prompt | model | StrOutputParser()\n",
    "\n",
    "blog_chain = outline_chain | content_chain | model | StrOutputParser()\n",
    "\n",
    "# Run the full pipeline\n",
    "final_result = blog_chain.invoke({\"The Future of Renewable Energy\"})\n",
    "\n",
    "Markdown(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b4470-1070-4a2c-a355-11fe4b2c15d2",
   "metadata": {},
   "source": [
    "# Memory in LangChain and LangGraph\n",
    "\n",
    "LLMs are stateless by default - they don't remember previous conversations. LangChain provides two ways to add memory:\n",
    "\n",
    "1. **LangChain Memory**: Traditional memory systems like `ConversationBufferMemory` (Deprecated)\n",
    "   - Simple to use\n",
    "   - Good for basic chatbots\n",
    "   - Limited scalability\n",
    "\n",
    "2. **LangGraph State Management**: Modern approach using `StateGraph` & `MemorySaver`\n",
    "   - Better for production use\n",
    "   - Supports multiple users/threads\n",
    "   - More scalable\n",
    "   - Better persistence options\n",
    "\n",
    "> Let's use the second approach provided by LangGraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a96d91b",
   "metadata": {},
   "source": [
    "# Conversation History with LangGraph MemorySaver\n",
    "\n",
    "We'll go over an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a **conversation** and **remember previous interactions** with a chat model.\n",
    "\n",
    "Source: [LangChain Docs](https://python.langchain.com/docs/tutorials/chatbot/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99f0e58",
   "metadata": {},
   "source": [
    "`ChatModels` in LangChain work like special objects called \"Runnables\", which means they have a common way you can use them.\n",
    "If you just want to run the model, you give it a list of messages and use the `.invoke` method — that’s how you “talk” to it and get a response.\n",
    "\n",
    "It’s like: “Put your messages in a box, hand it to the .invoke button, and the model will answer.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54b7c3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello Saad! 👋 Nice to meet you. How can I help you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke([HumanMessage(content=\"Hi! I am Saad.\")])\n",
    "\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a3c58e",
   "metadata": {},
   "source": [
    "The model on its own does not have any concept of state. For example, if you ask a followup question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d69fca8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I’m sorry, but I don’t have any information about your name. If you’d like to share it, I’ll be happy to address you by that name in our conversation!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke([HumanMessage(content=\"What's my name?\")])\n",
    "\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a71975",
   "metadata": {},
   "source": [
    "We can see that it doesn't take the previous conversation turn into context, and cannot answer the question. This makes for a terrible chatbot experience!\n",
    "\n",
    "To get around this, we need to pass the entire [conversation history](https://python.langchain.com/docs/concepts/chat_history/) into the model. Let's see what happens when we do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41882f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You introduced yourself as **Saad**. 😊 If there’s anything specific you’d like to talk about or any question you have, just let me know!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I am Saad.\"),\n",
    "        AIMessage(content=\"Hello Saad! 👋 Nice to meet you. How can I assist you today?\"),\n",
    "        HumanMessage(content=\"Who am I?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31162d5b",
   "metadata": {},
   "source": [
    "And now we can see that we get a good response!\n",
    "\n",
    "This is the basic idea underpinning a chatbot's ability to interact conversationally. So how do we best implement this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cad5b3",
   "metadata": {},
   "source": [
    "# Message persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d52e2",
   "metadata": {},
   "source": [
    "\n",
    "LangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\n",
    "\n",
    "Wrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.\n",
    "\n",
    "LangGraph comes with a simple in-memory checkpointer, which we use below. See its [documentation](https://langchain-ai.github.io/langgraph/concepts/persistence/?_gl=1*fi5jbl*_gcl_au*MTE0NDQ3MDcyNC4xNzU0OTExODMx*_ga*MTU2Mzg1ODA3NS4xNzQ5NjMyNjk5*_ga_47WX3HKKY2*czE3NTQ5MTE4MjYkbzckZzEkdDE3NTQ5MTE4MzckajQ5JGwwJGgw) for more detail, including how to use different persistence backends (e.g., SQLite or Postgres)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfe580a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGoAAADqCAIAAADF80cYAAAAAXNSR0IArs4c6QAAFX1JREFUeJztnXl8FEW+wKun574zyYQck8lJkIQE4gSCYJQjkrBE2CDLraKyLODiQx/LuvrEg+fxWXEF3V1MvNbVqKw8EQkB1JVdAgIJkHAFEpKQ+5xJJnPPdPf0+2PYmMU5U9Nkwtb3L+iq6v7lO9Xd1VXdVRhN0wAxUlijHcDYBumDAumDAumDAumDAumDgg1ZvrvZZjZQNjNls1AUMTbaQDgH4wtxvggXy/Bx8XyYXWEja/ddv2RuumRuvGCSyNlSBYcvwvkiFoc7Nuoy4XDazE6rmTLoCPMgmTxZnDRJlJAuGsGuAtbX22b/xxe9hN05IVuaMkUsV3JGcNTQQd9HXKs21p0x8gSsWb+IVKp4ARUPQB9F0Me+7Gu5askpUEzMkY4o2tDl8klD5WFdUob43iVK/0v5q89qog6UdI6L59/7QAB7H1tQBH1sX5+2w174yxiBGPeniF/6dF2Or9/pmDIrLGu2PBhxhjRnvxu4cHxw0foYRRTXZ2bf+syD5Oc72nKLIlLvlAQvyJCm7ozxhzLt0qfUIqmPOujjXkk6nF8Xd2bmyv5z3AEAJmRL0u+SHSjpoEgfdcuHvtOH++VKztR5iqCGNwaYlq8Qy9mVR/q9Z/Omb1BLXK0y5q2KCnZsY4N5q6OuVBqMA6SXPN70Hf9KO3WegsPFGIhtDMDls+6cHVbxVZ+XPB71DWoJbZc9Y6aMmdjGBpm58p4Wu5cK6FHftWpTxkwZNjYew5iChYOMmbJr1UaPGTwlNJw3xk8cyWMgDLNmzeru7g601Oeff/7SSy8xExGInyhsqDF5SnWvz6QnrUYqPNp3uzGItLe3m0weA/VCbW0tA+HcQKniGfpJT+ev+w6rrmZboA/P/kPTdGlpaXl5eUtLS3Jy8vTp09evX3/27NkNGzYAAAoLC2fNmrVjx46Ghoa9e/dWVVV1d3cnJyc/8MADixYtAgDU19evXLly165dL774YmRkpEAgqK6uBgB8/fXXn376aWpqatADjlTxetvskjA3rtzrs5spgQS2K9ATpaWlH3300Zo1a5KTkzs7O//0pz/JZLJVq1a9+eabTz75ZFlZWVRUFADgjTfe6Onp+d3vfodhWGNj4/bt29VqdVZWFpfLBQC89957jzzyyOTJk9PS0h566KGUlJRt27YxFLBAgtstlNskD/qsTqF/z8wjoKamZtKkSatWrXL9Nzs72+Fw/DTba6+9ZrFYoqOjXXn27dt34sSJrKwsV+qMGTNWrFjBUIQ3IRDjdqvTbZJ7fU4njXOYau5lZGTs3r17+/btGo0mNzdXrVZ7iMFZWlr6ww8/tLa2urakpaUNpU6cOJGh8H4Kh8vy9PTmXp9AhGu73NSIoLB69WqJRHL06NFt27ax2ez58+c/8cQTYWFhw/NQFLVp0yaapjdt2jRt2jSRSLR69WpXEoZhAAA+H6qTPSAsRjIyzv3h3OsTStiWegtD0eA4vnjx4sWLFzc2NlZWVhYXF9tstldffXV4ntra2qtXrxYXF2s0GteWoZvyrX+rxGKghBL3lzIPtU+CW43uL5bwlJWVpaenJyYmJicnJycn63S67777bqhauTAajQAApfJG12xdXV17e/vQhe8mhhdkArORFErdi3Lf7lPG8rQddifFyO9cVla2devWiooKg8FQUVFx7NixzMxMAIBKpQIAfPPNN5cvX05KSsIwrLS01GQyNTU17dq1Kycnp6ury+0OY2NjL126dObMmYGBgaBHSxK0vpfw2ASmPbB/d0fjBZOnVBi6urqeeuopjUaj0Wjy8/NLSkqsVqsr6dlnn83JyVm/fj1N04cPH16yZIlGo1m8eHFtbe23336r0WhWrFhx/fp1jUZTVVU1tMOqqqqioqJp06ZVVlYGPdqGGuOBkg5PqR57my+dGOxsss17cFzQf8+xxZG/dselCtOmux8a8/jMm6qRtNVbvPd23fYYB8j2a9bxnnvavY11nD+m72yyzV/jvru0o6NjqOl7EywWy+l0385cunTpxo0b/Yh8JGzevLmmpsZtklwu1+v1bpNefvnlmTNnuk0q/6BLNV6Ymeux186bPicFPnmleeYiZXKmm64Xp9NpNpvdFrTZbJ7aZRwOh7kmm8VioSj3DQaCIDgc9yP6AoGAzXZzY60/azxZrnvo2QRvvXbeL5y9bbaSZxr7ux1BvySHONpOe8kzjb1tNu/ZfHSHKlW8eaujDr7f6bC5PxlvSxw258H3OuevifbZ7eTXMHndWWPNP/SFa2NEMqb6EUIHk548+H5X1my5P2Oz/r6k0dFoPbqnd97qqEg1U/2AoUBvq/3Ix915K8dFJ/p1gQ7gFSFDP3mgpCMxXTwtX8G+7YbfCAd9+pCurc6yYG2MVOFvX2dgL6hRBF172lB31jhphiw5U8zh3Q4SCbuz4bzp8klDWo7UU/PYEyN8PbLpkvn6RbNJT4RH88RyNl+E80X4WBkRJhy0zUzZzJRJT2q77JIwTlKGKPHWvB55E13Xbf3djkEtoe9z2CxBvjvrdDoAQHh4eHB3yxex5BFcmZITHsWNShiNl3NvDcXFxRiGrVu3brQD8ch/9jA4NEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFEgfFKH4WcyCBQsoiqJp2mq1AgBEIhFFURwO5+DBg6Md2s0wNU0aDNHR0dXV1UOT27g+sc/Ozh7tuNwQiifv8uXL5fJ/m548PDx8aA6rkCIU9eXl5aWkpAzfkpCQcO+9945eRB4JRX2u+UpkshvTf8jl8pUrV452RO4JUX1z585NSEhw/Ts+Pn7OnDmjHZF7QlQfAGDZsmUikUgkEi1btmy0Y/EI1J3XYXNqO+wMtXzSk3InJszEcTw9KbejwcrEITAMRMTyuPyR16ERtvva6iwnDujsVkokZQMwNr7BdwdtNpB8IT5zYYRqvGAE5UdS+04f6r9WbZy7KlYsD8VmY6AYB4i/f9J5xzTp1HlhfmT/NwKuty1XLJdPDRY8Fnd7uAMASMI4BWvjLh7Xt9YFfIkIWN/x/X3TF0TyIK4XIQhfwJq+IPKE18UR3BKYBZKgDf2kKvVWz2V/C1BNEOl1BBngSn2B6dP3OmQRXIZnWh0dMAzIIjj6PiKgUoHpczoB63Z05wIDGO1ksvYhbgLpgwLpgwLpgwLpgwLpgwLpgwLpgwLpgwLpgwLpg2Ls6evp6Z49N/vkyQrv2bY9/5vfPr2J6WDGnr6QAumDgvEO9+df2MrlcrM103f84X85HE7axIwXnv/9nr/99ZPSD8LCFPMLFv5y7a9dOVtbm9/c+Wr9tSscDjc+PvGxRzZmZt5Y2+m7vx/+8MPdZot5xl33FBUtG768U/mh/QfKvmxubkxKGj93TsHiols6qsl47eNwOOcvnKu7dmXvF0f++NaHNefPPrH5MT5fUF5WsXXLtk8/+8vFizUAAJ1O+/iv18TFxb//7p63dr4nkUi3v/yM3W4HADQ1Nbzy6nMLFhR9/Nd9c+bkv/3H14d2/u235a/v2J6WlvFZ6YFH1qz/9LMPi0veYvovGg7j+jAMczqdG9c/KZPKkpJS4uMTuRzuqpWPCASC6dPv5vP59deuAgD+9sUnAqFw8389HRUVrVYn/GbLNr1+oPzQfgDAvq/2REfHrlyxRiKWZGtyfjZ/0dDODxz8MmtK9qbHt8jlYdmanDUP/2rv/306aBhk+o8agnF9NE3HxKiGlmMRCkXxCUlDqSKR2Gw2AQCamxtTUyeyWDfikUllKpX6ytVLAIDOzvaEYUUmpN5YapGiqCtXLk2detdQ0pQp2SRJXqm9yPQfNQTj1z6apoekuMDcDavr+rXx6sThWwQCoc1qBQAYjQa5/McRWC6P59qtw+EgSbLk3bdL3n17eMEBvY/l7INIqIzVCgRCm902fIvValEowgEAYrFkeJLLKYZhAoFAKBTm59+fe/fs4QVVse7XDGWCUNE3ITXt+6NHSJJ0neaDg/r29tafL1oKAIgcF3XmzCmapl1329OVJ4ZWqkxMTDGbTVlTbrx4arfb+/p6lMrIWxZ2qLT7Fi1cotcPvLnz1f5+XVNTwyuvbROJxPnzCgEAs+7J0+m07xTvAgCcPVdZVvblUMNl7aOPHz9+9MiRMoqiamrOvvDSb7ds3UgQgQ02whAq+uLi4l984ff19Vce+EX+f/9mA47jb+18z7Uk2fTpd6/75aaKiu9nz83esWP7b7e+4LpvAACmTNG88+ePq8+fWbxk3tPPPOGw21/e/qanBcWYILA3rHrb7N9/3rtgXRyTIY0aZcVteSsjA1qUPVRq3xgF6YMC6YMC6YMC6YMC6YMC6YMC6YMC6YMC6YMC6YMC6YMC6YMiMH2s29w2jQX44UBgPmRKrl7rCDCmMcOglpArA+srDEwfh4sJxLi20x5gYGMAbYddJGOzOUzWPgDA1PsUx/Z22YO9kvHoYrdQx/Z2Tc1XBFpwJN/znjyou/SDYXqhMiFNHGjZEOT6ZVNleV/GTFnO/FuiDwDQXm89vr9PryXCY3hux22DgpOmAQAsxr6howGt67TLldy7F43wc2ioWYQY/RgfAHDgwAEAwP3338/Q/uE/xoca5+XyWTHJI/nR/AQTDmAYFpvC4CEguc0bckyD9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EGB9EERimuTFxYWdnZ2Ds13+K8JUGNCcG3yUKx9hYWFOI7jOM76F2w2e+HChaMdlxtCUd/SpUtVKtXwLWq1evny5aMXkUdCUZ9CoSgoKBg6czEMy8vLG1prO6QIRX0AgCVLlsTF3ZijUqVSrVixYrQjck+I6gsPD8/Ly8MwDMOwgoICuVw+2hG5J0T1udYmV6vVsbGxobw2eRAaLuZBsuG8aVBHWo2UzUzZ7UFrCfX19gEMKJXKYO2Qx8P4IlwowaXh7JTJYpEMdtrqkeujCPrcUX19tdGgI+TRIjaPg3NxNgfH2aFboynSSRIURVCkhdD3mKXh3IlTxZNz5XiAE2gMMUJ99edMFfv6OCJuWLRUEikc2bFHHUOvRd9lIMyO3CJl6p0jmdYiYH12q7Ps3e5BPRWVohCG8UdwyFDD3G/taRiQKfCF66I5vMCqYWD6DP3kvj92iJSSiIRQbIXB0Hddbx0w/3xDjFQRwAUxAH09rbbyD3qUqeHisNCdmwEGk87W26C9f22U/xOH+3uZtxiogx/0xKRH3q7uAADicH5MemTZ+91mA+VnEb/0kQS9788dkcnhPDEXLsJQhy/mKpPD97/TSZF+nZR+6TtV3i9UiMURt229G444XMCXCU8f9mvJGd/6zINUc60lLO52u1d4QaGWN16wmAdJnzl96/vnl32y2BB95GQOWYysYr/OZzYf+mxmZ3uDVaIM0YbxgL57y3M5tVePB33P0khRS63ZZvZxD/Ghr+G8UaoUBTWwMQIGpONETZdM3nP50HetxiyKCNGqxzRihbChxuI9j48Wdl+bLXlG0Do8bmLQ0Pf1oZ0tbRcJwn7H+Lvum702IlwFAKg4uedoxce/WvP2R58/3dvXHB01fvbdD945Od9V6tyFI0e+K7bZzWl35N6d8wvgmkiOAQRyXnOl1nseb7WPJGiSpBnqQaEo8p0PH29pu7j05/+zZdNnAoHkrZJHB/TdAAA2m2u1Gb4qf2NZ0f+8/tKp9Am5e/a9ZDT1AwC6eho+2/t8TvaipzfvzcqY91X5H5iIzQWbixOE0+l1llFvaga1hEDM1KpTTc3VfdqWFQ+8kJoyTSJW3F+wmccVVJzc4xrcIAh7wdz18XEZGIZppsynKLKjsw4AcPzUF4qw2Dn3PCwQSFJTpk27k6mZEV3whexBrbdly7zpM+lJNg9nICoAAGhuvcDl8JMT73T9F8fxBPXk5tbzQ0vYqVXpriQ+XwwAsNlNAABdf/u4yB/XYlTFTgSAsbk/AeAI2Ca9t9aft2sfm4sxN4Zus5sdhG3LcznDN4bJowEAgKaHr8DrwuXUajWKRT+uV8lh84aSmICiaNxr/fGmTyjGKbvvlvfIkIjD+TzRmpWvD9/I8h4sAHy+2EH8uF6lg7D+VHQQIe2UUOq1hnlJE0jYDpu/fQ+BEh2VYrObw+RR4YpY1xZtf7tUHOG9VJg8qr7h9ND7G1frf2C09hFWUijx9ot6u/bxhSw2l0XYGKmAE1JyUlNyvtj/in6wx2QeqDi5Z+fuh8+eP+S9VGb6XINRW3bkbQDAtcaqU2e+Aow1XBwWksPHvc+r66Pdp75DaOyzKOKkwY4NAADWPrjzZNWXH+95tqXtYqQyIUez6K6pRd6LpE2Y+bN5j5+q2vfPE6Vh8ujli7ft/mCD08nIKWLUWhIn+Xji8tHb3HjedPLwoCozKtixjQHaz3fPKJQneTXoo0msShUO9lodFqZuICGLw0oa+qxxqT4eWH2cvDwBa4JG2t00oJrk/tGNosjnX8t3m0SSDjbOddsqi41O3fDobu+HDojnXs6jgfvTyOmkWCw3l3+1Kn3dw2952mFvQ/+EqVIO18dV1fdQkdVEfbS9OSE7hu+hp75/oNPtdpvN5Grx/hQc58ikwXyU9hQDAMBB2LkcN0M/bDZXKnF/o7cZHS3nutY8n8AT+Dg7/Rppq/7HwLmjhsSpMSw8dN8gCBZO0nm9qnPqfbLMXN+dxH7pmHKPXBnDab/UF4Jv8gYXmqbbLvRExHAyZvo1OOGXPoyF/ezRaA5Oddf5NYAydum62s/l0gsei/Zz0SJ/T0Y2ByvaGANIe2tNj9O/QbyxhZOkW2t6MKejaGOs/0vuBPaSBkXSh/7S3dPqUGdFcfiwb3eFDoSNbDnXHZPEy39wHM4O4BlmJG9Ynflm4Mz3AxFqmUItY+HMdRfdCiiK7m/R61oN2feFZeeF+VHi3xjhC2oDPUT1P/XXL5mFcqFAzhOHC9hcpnoGmYC0UaYBq2XQbh2wJGWIsmbJA11izAXU26UkQTdfttTXmNuumGiA8cUcrpDD5oXoSU3TgHKQDgthMzswGqjTxOOzRCmZUOOIQfuqyKQn9X3EoJbwZ3B+dMCASMqWRXDkSo5YHpzfOBQ/yhpD3P5PEYyC9EGB9EGB9EGB9EGB9EHx/6Xr7EcJxlTxAAAAAElFTkSuQmCC",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x000001E41D138440>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modern usage with LangGraph: Production-friendly way to store chat state.\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fbd2af",
   "metadata": {},
   "source": [
    "We now need to create a `config` that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a `thread_id`. This should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0653126",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4576d23f",
   "metadata": {},
   "source": [
    "This enables us to support multiple conversation threads with a single application, a common requirement when your application has multiple users.\n",
    "\n",
    "We can then invoke the application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78abe6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi! I am Saad.', additional_kwargs={}, response_metadata={}, id='246012ff-1fef-4b93-bdf2-fc730d8118c7'),\n",
       "  AIMessage(content='Hello Saad! 👋 Nice to meet you. How can I help you today?', additional_kwargs={'reasoning_content': 'The user says \"Hi! I am Saad.\" Probably a greeting. We should respond friendly. No special instructions. So respond with greeting.'}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 78, 'total_tokens': 135, 'completion_time': 0.113675522, 'prompt_time': 0.004576473, 'queue_time': 0.001326876, 'total_time': 0.118251995}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_4a19b1544c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--4209ed58-3e49-403b-b0be-925e7a9972d5-0', usage_metadata={'input_tokens': 78, 'output_tokens': 57, 'total_tokens': 135})]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Hi! I am Saad.\"\n",
    "\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d49358e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Saad! 👋 Nice to meet you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31b9372d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, you told me your name is **Saad**. 😊 How can I assist you today, Saad?\n"
     ]
    }
   ],
   "source": [
    "query = \"Do you know my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c50f32",
   "metadata": {},
   "source": [
    "Great! Our chatbot now remembers things about us. If we change the config to reference a different `thread_id`, we can see that it starts the conversation fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed1d0894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don’t know your name unless you tell me. If you’d like to share it, feel free! Otherwise, I’m happy to help with any question you have.\n"
     ]
    }
   ],
   "source": [
    "new_config = {\"configurable\": {\"thread_id\": \"xyz890\"}}\n",
    "\n",
    "query = \"Do you know my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config=new_config)\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4d6633",
   "metadata": {},
   "source": [
    "However, we can always go back to the original conversation (since we are persisting it in a database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1598a580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, you introduced yourself as **Saad**. How can I help you today, Saad?\n"
     ]
    }
   ],
   "source": [
    "query = \"Do you know my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898c30dc",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "This is how we can support a chatbot having conversations with many users!\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you understand the basics of LangChain and LangGraph, you can:\n",
    "\n",
    "1. **Explore More Components**:\n",
    "   - Document loaders\n",
    "   - Vector stores\n",
    "   - Structured output\n",
    "   - Tools and agents\n",
    "\n",
    "2. **Learn Advanced Features**:\n",
    "   - Custom chains\n",
    "   - Advanced agents\n",
    "   - Async operations\n",
    "   - Streaming responses\n",
    "\n",
    "3. **Build Production Applications**:\n",
    "   - Use LangSmith for monitoring\n",
    "   - Deploy with LangServe\n",
    "   - Implement proper error handling\n",
    "   - Add security measures\n",
    "\n",
    "Check out the [LangChain documentation](https://python.langchain.com/docs/get_started/introduction) for more examples and tutorials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-intro-and-memory-management",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
