{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd3e866-c385-4171-9fa5-2feccbc25580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065657af-27cc-4af5-8d12-45342a0de302",
   "metadata": {},
   "source": [
    "## Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1697545-1bb6-4a96-88fe-e971d2fb0a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"openai/gpt-oss-120b\", temperature=0)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06af35e9-9fb0-4c7a-9441-0b21d7ebe4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meth 2\n",
    "from langchain.chat_models import init_chat_model #https://js.langchain.com/docs/integrations/chat/\n",
    "\n",
    "LLM = init_chat_model(model=\"groq:openai/gpt-oss-120b\")\n",
    "LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dac21b1-be35-4b80-b573-812dcce5285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM.invoke(\"Hi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c4f599-c769-4ccc-99e6-bb94037f40cb",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "A prompt is the text we give to the LLM (or ChatModel). <br>\n",
    "LangChain provides special classes like PromptTemplate and ChatPromptTemplate to make prompts dynamic and reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa031ce-6095-4095-ae5a-0f17468e70d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Input Variable\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template= \"Explain {topic} in simple Term\"\n",
    ")\n",
    "\n",
    "prompt = template.format(topic = \"Large Language Language\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440e0728-31c0-495e-8cf7-a3844c00a68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(prompt)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a7d21c-db79-44a4-85e2-97194179002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatPromptTemplate \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpfull assistent\"),\n",
    "    (\"user\", \"Summarize the concept of {concept} in 3 bullet points.\")\n",
    "])\n",
    "\n",
    "message = template.format(concept = \"Large Language Model\")\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09d9ca0-da0f-4c1e-946e-abe26f323e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(message)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b1d9a-c829-4273-a865-3a4d622e498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Message Types\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import AIMessage, SystemMessage, HumanMessage\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a wise and slightly humorous AI mentor.\"),\n",
    "    HumanMessage(content=\"Can you explain LangChain in one sentence?\"),\n",
    "    AIMessage(content=\"LangChain is a Python framework that helps you build apps powered by large language models.\"),\n",
    "    HumanMessage(content=\"Great! Can you also give me one real-world use case?\")\n",
    "])\n",
    "\n",
    "message = template.format_messages()\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e7cc90-da71-4f28-a357-6014e67bd6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke(message).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25837b5a-5212-4e7c-a847-1ca5f7c049aa",
   "metadata": {},
   "source": [
    "## Chain in LangChain\n",
    "A Chain is basically a pipeline that connects multiple steps together, where the output of one step becomes the input of the next.\n",
    "https://python.langchain.com/api_reference/langchain/chains.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd90420-702e-4ad7-9d7f-2b4bdd4f7dd9",
   "metadata": {},
   "source": [
    "### 1. Single-Step Chain (LLMChain)\n",
    "This is the simplest form â€” Prompt â†’ LLM â†’ Output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e02368d-2bd0-4e90-b7f0-3243532df8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain {topic} in simple terms and add one fun fact.\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=model, prompt=prompt)\n",
    "\n",
    "result = chain.invoke(\"Quantum Computing\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1fefe1-4244-4181-a036-2e2e9cf944e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runnable chain\n",
    "# Use pipe | operator to make a chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "res = chain.invoke(\"Cloud Computing\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659f7378-7252-4347-9adf-27b8496ec80e",
   "metadata": {},
   "source": [
    "### 2. Multi-Step Chain (SimpleSequentialChain)\n",
    "This combines two LLM steps, first create an outline, then expand it into a blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93064fb-7471-409f-9b2f-47ee1634915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "outline_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Create a 3-point outline for a blog post about {topic}.\"\n",
    ")\n",
    "\n",
    "outline_chain = LLMChain(llm=model, prompt=outline_prompt)\n",
    "\n",
    "# Step 2: Content generator\n",
    "content_prompt = PromptTemplate(\n",
    "    input_variables=[\"outline\"],\n",
    "    template=\"Expand this outline into a detailed 200-word blog post:\\n{outline}\"\n",
    ")\n",
    "content_chain = LLMChain(llm=model, prompt=content_prompt)\n",
    "\n",
    "# Combine into one chain\n",
    "blog_chain = SimpleSequentialChain(chains=[outline_chain, content_chain])\n",
    "\n",
    "# Run the full pipeline\n",
    "final_result = blog_chain.invoke(\"The Future of Renewable Energy\")\n",
    "print(final_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140bfee8-21bb-4079-b15c-e5fadc76cf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runnable chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "outline_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Create a 3-point outline for a blog post about {topic}.\"\n",
    ")\n",
    "\n",
    "outline_chain = outline_prompt | model | StrOutputParser()\n",
    "\n",
    "content_prompt = PromptTemplate(\n",
    "    input_variables=[\"outline\"],\n",
    "    template=\"Expand this outline into a detailed 200-word blog post:\\n{outline}\"\n",
    ")\n",
    "content_chain = content_prompt | model | StrOutputParser()\n",
    "\n",
    "blog_chain = outline_chain | content_chain | model | StrOutputParser()\n",
    "\n",
    "# Run the full pipeline\n",
    "final_result = blog_chain.invoke({\"The Future of Renewable Energy\"})\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13770a3-582b-41cc-a0ba-58da6673be36",
   "metadata": {},
   "source": [
    "##  Memory in LangChain\n",
    "By default, LangChain calls are stateless, the model only sees the current input and forgets everything else.\n",
    "https://python.langchain.com/api_reference/langchain/memory.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3899832d-de29-43e9-867e-97a4898fa56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def withoutMemory():\n",
    "    # Prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"\"\"You are a friendly chatbot.\n",
    "User: {question}\n",
    "Assistant:\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "    # Create chain\n",
    "    chain = LLMChain(llm=model, prompt=prompt)\n",
    "\n",
    "    print(\"ðŸ¤– Chatbot is ready! Type 'exit' to stop.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "        response = chain.run(user_input)\n",
    "        print(f\"Bot: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b56a2a5-d070-489f-a3ef-0a2f2212a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "withoutMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9236f9f3-dce4-453f-b734-6f83aa4f8fe7",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory\n",
    "- Stores: Full conversation history.\n",
    "- Use: Small chats where cost & token limits arenâ€™t an issue.\n",
    "- Pro: Simple and easy.\n",
    "- Con: Can grow too large for long sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61e7cfd-ac8b-47a8-a3b6-bfadba7d5e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompts = \"\"\"You are an AI Assistant for **Advanced Telecom Services (ATS)**. Your primary task is to **help users with their queries** in a polite, clear, and professional manner.\n",
    "Conversation so far:\n",
    "{chat_history}\n",
    "\n",
    "**About ATS:**\n",
    "**Founded:** 2011, Washington, USA\n",
    "**Focus:** Digital wireless (3G, LTE, 5G), testing, test automation, software development, cloud, and artificial intelligence.\n",
    "**Mission:** Innovate and achieve operational efficiency for the telecom and wireless industry.\n",
    "**Team:** Top minds delivering highâ€‘quality engineering and technology services, partnering with customers for superior service delivery and success.\n",
    "**Contact Information:**\n",
    "**Website:** https://www.atsailab.com/\n",
    "**Address:** 744 241st Ln SE, Sammamish, WA 98074.\n",
    "**Email:** info@atsailab.com\n",
    "**Phone:** +1 (425) 533-1351\n",
    "**LinkedIn:** https://www.linkedin.com/company/atsailab/\n",
    "**Your Role as an assistant:** Respond politely and helpfully to every user query.\n",
    "Only respond with the information provided in this message. Do not hallucinate information. \n",
    "Provide accurate information about ATS's services, expertise, and contact details.\n",
    "Offer guidance, answer technical questions, and assist with any related requests while maintaining a courteous tone.'\n",
    "\n",
    "User: {question}\n",
    "Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1c37b8-fd3b-4c5a-b3e8-bae0abef9199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def langchainMemory():\n",
    "    \n",
    "    # Create Memory\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "    # Prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"chat_history\", \"question\"],\n",
    "        template=system_prompts\n",
    "    )\n",
    "\n",
    "    # Create chain\n",
    "    chain = LLMChain(llm=model, prompt=prompt, memory=memory)\n",
    "\n",
    "    print(\"ðŸ¤– Chatbot is ready! Type 'exit' to stop.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "        response = chain.run(user_input)\n",
    "        print(f\"Bot: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154556f2-9718-42dd-a48c-0683d4742d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchainMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcef842-866f-4be8-88d4-5126838eb939",
   "metadata": {},
   "source": [
    "### ConversationBufferWindowMemory\n",
    "- Stores: Only the last N interactions (k).\n",
    "- Use: Long-running chats, avoid sending huge history.\n",
    "- Pro: Efficient, keeps relevant context.\n",
    "- Con: Loses older history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cb3619-dd06-46fb-8cfc-a88d4f5b3601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "def interactive_chat_with_window_memory(window_size=3):\n",
    "\n",
    "    # Create windowed memory\n",
    "    memory = ConversationBufferWindowMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        k=window_size  # Only remember the last `k` exchanges\n",
    "    )\n",
    "\n",
    "    # Prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"chat_history\", \"question\"],\n",
    "        template=\"\"\"\n",
    "You are a helpful chatbot that remembers only the last {window_size} exchanges.\n",
    "Conversation so far:\n",
    "{chat_history}\n",
    "\n",
    "User: {question}\n",
    "Assistant:\n",
    "\"\"\".replace(\"{window_size}\", str(window_size))\n",
    "    )\n",
    "\n",
    "    # Create chain\n",
    "    chain = LLMChain(llm=model, prompt=prompt, memory=memory)\n",
    "\n",
    "    print(f\"ðŸ¤– Chatbot ready! Remembering last {window_size} exchanges. Type 'exit' to stop.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "        response = chain.run(user_input)\n",
    "        print(f\"Bot: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc8309a-c3f7-4cab-9e4e-f814f9a2890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_chat_with_window_memory(window_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920c00c0-de5e-4c3d-93ba-9b333f029168",
   "metadata": {},
   "source": [
    "### LangGraph Memory Saver\n",
    "Production-friendly way to store chat state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1538d43f-b06e-4317-acba-17899501728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompts = \"\"\"You are an AI Assistant for **Advanced Telecom Services (ATS)**. Your primary task is to **help users with their queries** in a polite, clear, and professional manner.\n",
    "**About ATS:**\n",
    "**Founded:** 2011, Washington, USA\n",
    "**Focus:** Digital wireless (3G, LTE, 5G), testing, test automation, software development, cloud, and artificial intelligence.\n",
    "**Mission:** Innovate and achieve operational efficiency for the telecom and wireless industry.\n",
    "**Team:** Top minds delivering highâ€‘quality engineering and technology services, partnering with customers for superior service delivery and success.\n",
    "**Contact Information:**\n",
    "**Website:** https://www.atsailab.com/\n",
    "**Address:** 744 241st Ln SE, Sammamish, WA 98074.\n",
    "**Email:** info@atsailab.com\n",
    "**Phone:** +1 (425) 533-1351\n",
    "**LinkedIn:** https://www.linkedin.com/company/atsailab/\n",
    "**Your Role as an assistant:** Respond politely and helpfully to every user query.\n",
    "Only respond with the information provided in this message. Do not hallucinate information. \n",
    "Provide accurate information about ATS's services, expertise, and contact details.\n",
    "Offer guidance, answer technical questions, and assist with any related requests while maintaining a courteous tone.'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61a1ce3-bfb9-4895-ae0d-85815a155332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState, StateGraph\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Memory\n",
    "memory = MemorySaver()\n",
    "\n",
    "def chatbot_node(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": state[\"messages\"] + [response]}\n",
    "\n",
    "graph = StateGraph(MessagesState)\n",
    "graph.add_node(\"chatbot\", chatbot_node)\n",
    "graph.set_entry_point(\"chatbot\")\n",
    "\n",
    "# Compile with memory\n",
    "app = graph.compile(checkpointer=memory)\n",
    "# app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbd072a-eb08-49d2-93fa-8bc2373bf34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "\n",
    "    # Invoke graph with the session ID (for persistence)\n",
    "    result = app.invoke({\"messages\": [system_prompts, HumanMessage(content=user_input)]},\n",
    "                        config={\"configurable\": {\"thread_id\": \"1\"}})\n",
    "\n",
    "    # Get last assistant message\n",
    "    result[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
